This file is a merged representation of the entire codebase, combined into a single document by Repomix.

# File Summary

## Purpose
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)

# Directory Structure
```
backend/
  rgbd/
    WorldMotionEstimator.cc
    WorldPoseEstimator.cc
  BackendDefinitions.cc
  BackendModule.cc
  BackendParams.cc
  BackendPipeline.cc
  DynamicPointSymbol.cc
  FactorGraphTools.cc
  RGBDBackendModule.cc
common/
  byte_tracker/
    ByteTracker.cc
    Detection.cc
    KalmanFilter.cc
    Lapjv.cc
    Rect.cc
    Track.cc
  Algorithms.cc
  Camera.cc
  CameraParams.cc
  DynamicObjects.cc
  GroundTruthPacket.cc
  ImageContainer.cc
  ImageTypes.cc
  Map.cc
  MapNodes.cc
  ModuleFactory.cc
  PointCloudProcess.cc
  StereoCamera.cc
  StructuredContainers.cc
  Types.cc
  ZEDCamera.cc
dataprovider/
  ClusterSlamDataProvider.cc
  DataInterfacePipeline.cc
  DataProvider.cc
  DataProviderFactory.cc
  DataProviderUtils.cc
  DatasetLoader.cc
  DatasetProvider.cc
  OMDDataProvider.cc
  ProjectAriaDataProvider.cc
  VirtualKittidataProvider.cc
  ZEDDataProvider.cc
factors/
  LandmarkMotionPoseFactor.cc
  LandmarkMotionTernaryFactor.cc
  LandmarkPoseSmoothingFactor.cc
  ObjectKinematicFactor.cc
  Pose3FlowProjectionFactor.cc
frontend/
  anms/
    anms.cc
    NonMaximumSupression.cc
  imu/
    ThreadSafeImuBuffer.cc
  vision/
    Feature.cc
    FeatureDetector.cc
    FeatureTracker.cc
    FeatureTrackerBase.cc
    Frame.cc
    MotionSolver.cc
    ObjectTracker.cc
    ORBextractor.cc
    StaticFeatureTracker.cc
    StereoMatcher.cc
    TrackerParams.cc
    UndistortRectifier.cc
    VisionTools.cc
  Frontend-Definitions.cc
  FrontendModule.cc
  FrontendParams.cc
  FrontendPipeline.cc
  RGBDInstance-Definitions.cc
  RGBDInstanceFrontendModule.cc
logger/
  Logger.cc
pipeline/
  Pipeline-Definitions.cc
  PipelineBase.cc
  PipelineManager.cc
  PipelineParams.cc
  PipelineSpinner.cc
utils/
  CsvParser.cc
  GtsamUtils.cc
  Histogram.cc
  JsonUtils.cc
  Metrics.cc
  OpenCVUtils.cc
  Spinner.cc
  Statistics.cc
  TimingStats.cc
visualizer/
  ColourMap.cc
  OpenCVFrontendDisplay.cc
  Visualizer-Definitions.cc
  VisualizerPipelines.cc
dynosam_tests.md
```

# Files

## File: backend/rgbd/WorldMotionEstimator.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/backend/rgbd/WorldMotionEstimator.hpp"
#include "dynosam/backend/BackendDefinitions.hpp"

#include "dynosam/factors/LandmarkMotionTernaryFactor.hpp"


namespace dyno {

StateQuery<gtsam::Pose3> WorldMotionAccessor::getObjectMotion(FrameId frame_id, ObjectId object_id) const {
    const auto frame_node_k = map()->getFrame(frame_id);
    CHECK(frame_node_k);

    //from k-1 to k
    return this->query<gtsam::Pose3>(
        frame_node_k->makeObjectMotionKey(object_id)
    );


}
StateQuery<gtsam::Pose3> WorldMotionAccessor::getObjectPose(FrameId frame_id, ObjectId object_id) const {
    const auto object_poses = getObjectPoses(frame_id);
    if(object_poses.exists(object_id)) {
        return StateQuery<gtsam::Pose3>(
            ObjectPoseSymbol(object_id, frame_id),
            object_poses.at(object_id)
        );
    }
    return StateQuery<gtsam::Pose3>::InvalidMap();
}

EstimateMap<ObjectId, gtsam::Pose3> WorldMotionAccessor::getObjectPoses(FrameId frame_id) const {
    EstimateMap<ObjectId, gtsam::Pose3> object_poses;
    //for each object, go over the cached object poses and check if that object has a pose at the query frame
    for(const auto&[object_id, pose] : object_pose_cache_.collectByFrame(frame_id)) {
        object_poses.insert2(object_id,
            ReferenceFrameValue<gtsam::Pose3>(
                pose,
                ReferenceFrame::GLOBAL
            )
        );
    }
    return object_poses;
}

void WorldMotionAccessor::postUpdateCallback(const BackendMetaData& backend_info) {
    //this is pretty slow!!
    //update object_pose_cache_ with new values
    //this means we have to start again at the first frame and update all the poses!!
    ObjectPoseMap object_poses;
    const auto frames = map()->getFrames();
    auto frame_itr = frames.begin();

    const auto& gt_packet_map = backend_info.ground_truth_packets;

    //advance itr one so we're now at the second frame
    std::advance(frame_itr, 1);
    for(auto itr = frame_itr; itr != frames.end(); itr++) {
        auto prev_itr = itr;
        std::advance(prev_itr, -1);
        CHECK(prev_itr != frames.end());

        const auto[frame_id_k, frame_k_ptr] = *itr;
        const auto[frame_id_k_1, frame_k_1_ptr] = *prev_itr;
        CHECK_EQ(frame_id_k_1 + 1, frame_id_k);

        //collect all object centoids from the latest estimate
        gtsam::FastMap<ObjectId, gtsam::Point3> centroids_k = this->computeObjectCentroids(frame_id_k);
        gtsam::FastMap<ObjectId, gtsam::Point3> centroids_k_1 = this->computeObjectCentroids(frame_id_k_1);
        //collect motions from k-1 to k
        MotionEstimateMap motion_estimates = this->getObjectMotions(frame_id_k);

        //construct centroid vectors in object id order
        gtsam::Point3Vector object_centroids_k_1, object_centroids_k;
        //we may not have a pose for every motion, e.g. if there is a new object at frame k,
        //it wont have a motion yet!
        //if we have a motion we MUST have a pose at the previous frame!
        for(const auto& [object_id, _] : motion_estimates) {
            CHECK(centroids_k.exists(object_id));
            CHECK(centroids_k_1.exists(object_id));

            object_centroids_k.push_back(centroids_k.at(object_id));
            object_centroids_k_1.push_back(centroids_k_1.at(object_id));
        }

        PropogatePoseResult propogation_result;
        if(FLAGS_init_object_pose_from_gt) {
            if(!gt_packet_map) LOG(WARNING) << "FLAGS_init_object_pose_from_gt is true but gt_packet map not provided!";
            dyno::propogateObjectPoses(
                object_poses,
                motion_estimates,
                object_centroids_k_1,
                object_centroids_k,
                frame_id_k,
                gt_packet_map,
                &propogation_result);
        }
        else {
            dyno::propogateObjectPoses(
                object_poses,
                motion_estimates,
                object_centroids_k_1,
                object_centroids_k,
                frame_id_k,
                std::nullopt,
                &propogation_result);
        }

        if(VLOG_IS_ON(20)) {
            //report on how many were propogated with motions for this frame only
            const auto propogation_type = propogation_result.collectByFrame(frame_id_k);
            std::stringstream ss;
            ss << "Propogation result: " << propogation_type.size() << " objects for frame " << frame_id_k;

            size_t n_used_motion = 0;
            for(const auto&[_, type] : propogation_type) {
                if(type == PropogateType::Propogate) {
                    n_used_motion++;
                }
            }
            ss << " of which " << n_used_motion << " were motion propogated";
            VLOG(100) << ss.str();
        }

    }

    VLOG(50) << "Updated object pose cache";
    object_pose_cache_ = object_poses;
    LOG(INFO) << "ending MotionWorldAccessor::postUpdateCallback";
}


void WorldMotionFormulation::dynamicPointUpdateCallback(
        const PointUpdateContextType& context, UpdateObservationResult& result,
        gtsam::Values& new_values,
        gtsam::NonlinearFactorGraph& new_factors) {
    const auto lmk_node = context.lmk_node;
    const auto frame_node_k_1 = context.frame_node_k_1;
    const auto frame_node_k = context.frame_node_k;

    auto dynamic_point_noise = noise_models_.dynamic_point_noise;
    auto theta_accessor = this->accessorFromTheta();

    const gtsam::Key object_point_key_k_1 = lmk_node->makeDynamicKey(frame_node_k_1->frame_id);
    const gtsam::Key object_point_key_k = lmk_node->makeDynamicKey(frame_node_k->frame_id);

    // if first motion (i.e first time we have both k-1 and k), add both at k-1 and k
    if(context.is_starting_motion_frame) {
        CHECK(!theta_accessor->exists(object_point_key_k_1));

        new_factors.emplace_shared<PoseToPointFactor>(
            frame_node_k_1->makePoseKey(), //pose key at previous frames
            object_point_key_k_1,
            lmk_node->getMeasurement(frame_node_k_1).landmark,
            dynamic_point_noise
        );
        if(result.debug_info) result.debug_info->getObjectInfo(context.getObjectId()).num_dynamic_factors++;
        result.updateAffectedObject(frame_node_k_1->frame_id, context.getObjectId());

        //add landmark at previous frame
        const Landmark measured_k_1 = lmk_node->getMeasurement(frame_node_k_1->frame_id).landmark;
        Landmark lmk_world_k_1;
        getSafeQuery(
            lmk_world_k_1,
            theta_accessor->query<Landmark>(object_point_key_k_1),
            gtsam::Point3(context.X_k_1_measured * measured_k_1)
        );
        new_values.insert(object_point_key_k_1, lmk_world_k_1);
        if(result.debug_info) result.debug_info->getObjectInfo(context.getObjectId()).num_new_dynamic_points++;
    }


    // previous point must be added by the previous iteration
    CHECK(new_values.exists(object_point_key_k_1) || theta_accessor->exists(object_point_key_k_1));

    const Landmark measured_k = lmk_node->getMeasurement(frame_node_k).landmark;

    new_factors.emplace_shared<PoseToPointFactor>(
        frame_node_k->makePoseKey(), //pose key at this (in the iteration) frames
        object_point_key_k,
        measured_k,
        dynamic_point_noise
    );
    if(result.debug_info) result.debug_info->getObjectInfo(context.getObjectId()).num_dynamic_factors++;

    result.updateAffectedObject(frame_node_k->frame_id, context.getObjectId());

    Landmark lmk_world_k;
    getSafeQuery(
        lmk_world_k,
        theta_accessor->query<Landmark>(object_point_key_k),
        gtsam::Point3(context.X_k_measured * measured_k)
    );
    new_values.insert(object_point_key_k, lmk_world_k);
    if(result.debug_info) result.debug_info->getObjectInfo(context.getObjectId()).num_new_dynamic_points++;


    const gtsam::Key object_motion_key_k = frame_node_k->makeObjectMotionKey(context.getObjectId());


    auto landmark_motion_noise = noise_models_.landmark_motion_noise;
    new_factors.emplace_shared<LandmarkMotionTernaryFactor>(
        object_point_key_k_1,
        object_point_key_k,
        object_motion_key_k,
        landmark_motion_noise
    );
    result.updateAffectedObject(frame_node_k_1->frame_id, context.getObjectId());
    result.updateAffectedObject(frame_node_k->frame_id, context.getObjectId());
    if(result.debug_info) result.debug_info->getObjectInfo(context.getObjectId()).num_motion_factors++;


    //mark as now in map
    is_dynamic_tracklet_in_map_.insert2(context.getTrackletId(), true);

}

void WorldMotionFormulation::objectUpdateContext(
        const ObjectUpdateContextType& context, UpdateObservationResult& result,
        gtsam::Values& new_values, gtsam::NonlinearFactorGraph& new_factors)
{
    auto frame_node_k = context.frame_node_k;
    const gtsam::Key object_motion_key_k = frame_node_k->makeObjectMotionKey(context.getObjectId());

    auto theta_accessor = this->accessorFromTheta();

    //skip if no motion pair availble
    if(!context.has_motion_pair) {
        return;
    }

    const auto frame_id = context.getFrameId();
    const auto object_id = context.getObjectId();

    if(!is_other_values_in_map.exists(object_motion_key_k)) {

        //when we have an initial motion
        Motion3 initial_motion = Motion3::Identity();
        if(!FLAGS_init_H_with_identity) {
            map()->hasInitialObjectMotion(frame_id, object_id, &initial_motion);
            LOG(INFO) << "Using motion from frontend " << initial_motion;
            initial_motion = gtsam::Pose3(gtsam::Rot3::Identity(), initial_motion.translation());
        }

        new_values.insert(object_motion_key_k, initial_motion);
        is_other_values_in_map.insert2(object_motion_key_k, true);
    }

    if(frame_id < 2) return;

    auto frame_node_k_1 = map()->getFrame(frame_id - 1u);
    if (!frame_node_k_1) { return; }

    if(FLAGS_use_smoothing_factor && frame_node_k_1->objectObserved(object_id)) {
        //motion key at previous frame
        const gtsam::Symbol object_motion_key_k_1 = frame_node_k_1->makeObjectMotionKey(object_id);

        auto object_smoothing_noise = noise_models_.object_smoothing_noise;
        CHECK(object_smoothing_noise);
        CHECK_EQ(object_smoothing_noise->dim(), 6u);

        {
            ObjectId object_label_k_1, object_label_k;
            FrameId frame_id_k_1, frame_id_k;
            CHECK(reconstructMotionInfo(object_motion_key_k_1, object_label_k_1, frame_id_k_1));
            CHECK(reconstructMotionInfo(object_motion_key_k, object_label_k, frame_id_k));
            CHECK_EQ(object_label_k_1, object_label_k);
            CHECK_EQ(frame_id_k_1 + 1, frame_id_k); //assumes consequative frames
        }

        //if the motion key at k (motion from k-1 to k), and key at k-1 (motion from k-2 to k-1)
        //exists in the map or is about to exist via new values, add the smoothing factor
        if(is_other_values_in_map.exists(object_motion_key_k_1) && is_other_values_in_map.exists(object_motion_key_k)) {
            new_factors.emplace_shared<gtsam::BetweenFactor<gtsam::Pose3>>(
                object_motion_key_k_1,
                object_motion_key_k,
                gtsam::Pose3::Identity(),
                object_smoothing_noise
            );
            if(result.debug_info) result.debug_info->getObjectInfo(context.getObjectId()).smoothing_factor_added = true;

            // object_debug_info.smoothing_factor_added = true;
        }

        // if(smoothing_added) {
        //     //TODO: add back in
        //     // object_debug_info.smoothing_factor_added = true;
        // }

    }



}


} //dyno
````

## File: backend/rgbd/WorldPoseEstimator.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/backend/rgbd/WorldPoseEstimator.hpp"
#include "dynosam/backend/BackendDefinitions.hpp"
#include "dynosam/factors/LandmarkMotionPoseFactor.hpp"
#include "dynosam/factors/LandmarkPoseSmoothingFactor.hpp"

namespace dyno {

StateQuery<gtsam::Pose3> WorldPoseAccessor::getSensorPose(FrameId frame_id) const {
    const auto frame_node = map()->getFrame(frame_id);
    CHECK_NOTNULL(frame_node);
    return this->query<gtsam::Pose3>(
        frame_node->makePoseKey()
    );
}

StateQuery<gtsam::Pose3> WorldPoseAccessor::getObjectMotion(FrameId frame_id, ObjectId object_id) const {
    const auto frame_node_k = map()->getFrame(frame_id);
    CHECK_NOTNULL(frame_node_k);
    const auto object_motion_key = frame_node_k->makeObjectMotionKey(object_id);

    if(frame_id < 2) {
        //if the current frame is 1 then skip as we never get a frame that is 0
        //this is because the first frame (frame_id=0) is never sent to the backend
        //as at least two frames (0 and 1) are needed to track!
        return StateQuery<gtsam::Pose3>::NotInMap(object_motion_key);
    }

    const auto object_pose_k = this->getObjectPose(frame_id, object_id);
    const auto object_pose_k_1 = this->getObjectPose(frame_id - 1u, object_id);

    if(object_pose_k && object_pose_k_1) {
        // ^w_{k-1}H_k = ^wL_k \: ^wL_{k-1}^{-1}
        const gtsam::Pose3 motion = object_pose_k.get() * object_pose_k_1->inverse();
        return StateQuery<gtsam::Pose3>{object_motion_key, motion};
    }
    return StateQuery<gtsam::Pose3>::NotInMap(object_motion_key);
}
StateQuery<gtsam::Pose3> WorldPoseAccessor::getObjectPose(FrameId frame_id, ObjectId object_id) const {
    const auto frame_node = map()->getFrame(frame_id);
    if(!frame_node) {
        return StateQuery<gtsam::Pose3>::InvalidMap();
    }

    // CHECK(frame_node) << "Frame Id is null at frame " << frame_id;
    return this->query<gtsam::Pose3>(
        frame_node->makeObjectPoseKey(object_id)
    );
}

StateQuery<gtsam::Point3> WorldPoseAccessor::getDynamicLandmark(FrameId frame_id, TrackletId tracklet_id) const {
    const auto lmk = map()->getLandmark(tracklet_id);
    CHECK_NOTNULL(lmk);

    return this->query<gtsam::Point3>(
        lmk->makeDynamicKey(frame_id)
    );
}



void WorldPoseFormulation::dynamicPointUpdateCallback(
        const PointUpdateContextType& context, UpdateObservationResult& result,
        gtsam::Values& new_values,
        gtsam::NonlinearFactorGraph& new_factors) {
    const auto lmk_node = context.lmk_node;
    const auto frame_node_k_1 = context.frame_node_k_1;
    const auto frame_node_k = context.frame_node_k;

    auto dynamic_point_noise = noise_models_.dynamic_point_noise;

    auto theta_accessor = this->accessorFromTheta();

    const gtsam::Key object_point_key_k_1 = lmk_node->makeDynamicKey(frame_node_k_1->frame_id);
    const gtsam::Key object_point_key_k = lmk_node->makeDynamicKey(frame_node_k->frame_id);

    // if first motion (i.e first time we have both k-1 and k), add both at k-1 and k
    if(context.is_starting_motion_frame) {
        new_factors.emplace_shared<PoseToPointFactor>(
            frame_node_k_1->makePoseKey(), //pose key at previous frames
            object_point_key_k_1,
            lmk_node->getMeasurement(frame_node_k_1).landmark,
            dynamic_point_noise
        );
        // object_debug_info.num_dynamic_factors++;
        result.updateAffectedObject(frame_node_k_1->frame_id, context.getObjectId());
        if(result.debug_info) result.debug_info->getObjectInfo(context.getObjectId()).num_dynamic_factors++;

        //add landmark at previous frame
        const Landmark measured_k_1 = lmk_node->getMeasurement(frame_node_k_1->frame_id).landmark;
        Landmark lmk_world_k_1;
        getSafeQuery(
            lmk_world_k_1,
            theta_accessor->query<Landmark>(object_point_key_k_1),
            gtsam::Point3(context.X_k_1_measured * measured_k_1)
        );
        new_values.insert(object_point_key_k_1, lmk_world_k_1);
        if(result.debug_info) result.debug_info->getObjectInfo(context.getObjectId()).num_new_dynamic_points++;
    }


    // previous point must be added by the previous iteration
    CHECK(new_values.exists(object_point_key_k_1) || theta_accessor->exists(object_point_key_k_1));

    const Landmark measured_k = lmk_node->getMeasurement(frame_node_k).landmark;

    new_factors.emplace_shared<PoseToPointFactor>(
        frame_node_k->makePoseKey(), //pose key at this (in the iteration) frames
        object_point_key_k,
        measured_k,
        dynamic_point_noise
    );
    if(result.debug_info) result.debug_info->getObjectInfo(context.getObjectId()).num_dynamic_factors++;
    result.updateAffectedObject(frame_node_k->frame_id, context.getObjectId());

    Landmark lmk_world_k;
    getSafeQuery(
        lmk_world_k,
        theta_accessor->query<Landmark>(object_point_key_k),
        gtsam::Point3(context.X_k_measured * measured_k)
    );
    new_values.insert(object_point_key_k, lmk_world_k);
    if(result.debug_info) result.debug_info->getObjectInfo( context.getObjectId()).num_new_dynamic_points++;


    const gtsam::Key object_pose_k_1_key = frame_node_k_1->makeObjectPoseKey(context.getObjectId());
    const gtsam::Key object_pose_k_key = frame_node_k->makeObjectPoseKey(context.getObjectId());

    auto landmark_motion_noise = noise_models_.landmark_motion_noise;
    new_factors.emplace_shared<LandmarkMotionPoseFactor>(
        object_point_key_k_1,
        object_point_key_k,
        object_pose_k_1_key,
        object_pose_k_key,
        landmark_motion_noise
    );
    result.updateAffectedObject(frame_node_k_1->frame_id, context.getObjectId());
    result.updateAffectedObject(frame_node_k->frame_id, context.getObjectId());
    if(result.debug_info) result.debug_info->getObjectInfo(context.getObjectId()).num_motion_factors++;

    // mark as now in map
    is_dynamic_tracklet_in_map_.insert2(context.getTrackletId(), true);

}


void WorldPoseFormulation::objectUpdateContext(
        const ObjectUpdateContextType& context, UpdateObservationResult& result,
        gtsam::Values& new_values, gtsam::NonlinearFactorGraph& new_factors)
{
    auto frame_node_k = context.frame_node_k;
    const gtsam::Key object_pose_key_k = frame_node_k->makeObjectPoseKey(context.getObjectId());

    auto theta_accessor = this->accessorFromTheta();

    if(!is_other_values_in_map.exists(object_pose_key_k)) {
        FrameId frame_id_k_1 = context.getFrameId() - 1u;
        //try and propogate from previous initalisation
        StateQuery<gtsam::Pose3> pose_k_1_query = theta_accessor->getObjectPose(
            frame_id_k_1,
            context.getObjectId()
        );

        //first frame the object is seen in
        const FrameId first_seen_frame = context.object_node->getFirstSeenFrame();

        //takes me from k-1 to k
        Motion3 motion;
        gtsam::Pose3 object_pose_k;
        //we have a motion from k-1 to k and a pose at k
        if(map()->hasInitialObjectMotion(context.getFrameId(), context.getObjectId(), &motion) && pose_k_1_query) {
            object_pose_k = motion * pose_k_1_query.get();
            CHECK_NE(first_seen_frame, context.getFrameId());

        }
        else {
            //no motion or previous pose, so initalise with centroid translation and identity rotation
            const auto[centroid_initial, result] = theta_accessor->computeObjectCentroid(context.getFrameId(), context.getObjectId());
            CHECK(result);
            gtsam::Pose3 initial_object_pose(gtsam::Rot3::Identity(), centroid_initial);

            getSafeQuery(
                object_pose_k,
                theta_accessor->query<gtsam::Pose3>(object_pose_key_k),
                initial_object_pose
            );

            // CHECK_EQ(first_seen_frame, context.getFrameId());
        }


        //for experiments and testing
        if(FLAGS_init_LL_with_identity && pose_k_1_query) {
            object_pose_k = pose_k_1_query.get();
        }
        if(FLAGS_use_identity_rot_L_for_init) {
            auto tmp_pose = gtsam::Pose3(gtsam::Rot3::Identity(), object_pose_k.translation());
            object_pose_k = tmp_pose;
        }
        if(FLAGS_corrupt_L_for_init) {
            object_pose_k = utils::perturbWithNoise<gtsam::Pose3>(object_pose_k, FLAGS_corrupt_L_for_init_sigma);
        }

        LOG(INFO) << "Adding object pose " << object_pose_k << " object id " << context.getObjectId();

        new_values.insert(object_pose_key_k, object_pose_k);
        is_other_values_in_map.insert2(object_pose_key_k, true);
        VLOG(50) << "Adding object pose key " << DynoLikeKeyFormatter(object_pose_key_k);
    }

    if(FLAGS_use_smoothing_factor) {
        const auto frame_id = context.getFrameId();
        const auto object_id = context.getObjectId();
        if(frame_id < 2) return;

        auto frame_node_k_2 = map()->getFrame(frame_id - 2u);
        auto frame_node_k_1 = map()->getFrame(frame_id - 1u);

        if (!frame_node_k_2 || !frame_node_k_1) { return; }

        //pose key at k-2
        const gtsam::Symbol object_pose_key_k_2 = frame_node_k_2->makeObjectPoseKey(object_id);
        //pose key at previous frame (k-1)
        const gtsam::Symbol object_pose_key_k_1 = frame_node_k_1->makeObjectPoseKey(object_id);

        auto object_smoothing_noise = noise_models_.object_smoothing_noise;
        CHECK(object_smoothing_noise);
        CHECK_EQ(object_smoothing_noise->dim(), 6u);

        {
            ObjectId object_label_k_2, object_label_k_1, object_label_k;
            FrameId frame_id_k_2, frame_id_k_1, frame_id_k;
            CHECK(reconstructPoseInfo(object_pose_key_k_2, object_label_k_2, frame_id_k_2));
            CHECK(reconstructPoseInfo(object_pose_key_k_1, object_label_k_1, frame_id_k_1));
            CHECK(reconstructPoseInfo(object_pose_key_k, object_label_k, frame_id_k));
            CHECK_EQ(object_label_k_2, object_label_k);
            CHECK_EQ(object_label_k_1, object_label_k);
            CHECK_EQ(frame_id_k_1 + 1, frame_id_k); //assumes consequative frames
            CHECK_EQ(frame_id_k_2 + 1, frame_id_k_1); //assumes consequative frames

        }

        //if the motion key at k (motion from k-1 to k), and key at k-1 (motion from k-2 to k-1)
        //exists in the map or is about to exist via new values, add the smoothing factor
        if(is_other_values_in_map.exists(object_pose_key_k_1) &&
            is_other_values_in_map.exists(object_pose_key_k) &&
            is_other_values_in_map.exists(object_pose_key_k_2)) {

            new_factors.emplace_shared<LandmarkPoseSmoothingFactor>(
                object_pose_key_k_2,
                object_pose_key_k_1,
                object_pose_key_k,
                object_smoothing_noise
            );
            if(result.debug_info) result.debug_info->getObjectInfo(object_id).smoothing_factor_added=true;
            VLOG(50) << "Adding smoothing " << DynoLikeKeyFormatter(object_pose_key_k_2) << " -> " <<  DynoLikeKeyFormatter(object_pose_key_k);
        }


    }



}


} //dyno
````

## File: backend/BackendDefinitions.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */


#include "dynosam/backend/BackendDefinitions.hpp"
#include "dynosam/backend/DynamicPointSymbol.hpp"
#include "dynosam/logger/Logger.hpp"
#include "dynosam/utils/Metrics.hpp"

#include <gtsam/inference/LabeledSymbol.h>
#include <gtsam/inference/Symbol.h>

namespace dyno {

static bool internalReconstructInfo(gtsam::Key key, SymbolChar expected_chr, ObjectId& object_label, FrameId& frame_id) {
  //assume motion/pose key is a labelled symbol
  if(!checkIfLabeledSymbol(key)) {return false; }

  const gtsam::LabeledSymbol as_labeled_symbol(key);
  if(as_labeled_symbol.chr() != expected_chr) { return false;}

  frame_id = static_cast<FrameId>(as_labeled_symbol.index());

  char label = as_labeled_symbol.label();
  object_label = label - '0';
  return true;
}


bool checkIfLabeledSymbol(gtsam::Key key) {
  const gtsam::LabeledSymbol asLabeledSymbol(key);
  return (asLabeledSymbol.chr() > 0 && asLabeledSymbol.label() > 0);
}

bool reconstructMotionInfo(gtsam::Key key, ObjectId& object_label, FrameId& frame_id) {
  return internalReconstructInfo(key, kObjectMotionSymbolChar, object_label, frame_id);
}

bool reconstructPoseInfo(gtsam::Key key, ObjectId& object_label, FrameId& frame_id) {
  return internalReconstructInfo(key, kObjectPoseSymbolChar, object_label, frame_id);
}

std::string DynoLikeKeyFormatter(gtsam::Key key)
{
  const gtsam::LabeledSymbol asLabeledSymbol(key);
  if (asLabeledSymbol.chr() > 0 && asLabeledSymbol.label() > 0) {
    //if used as motion
    if(asLabeledSymbol.chr() == kObjectMotionSymbolChar || asLabeledSymbol.chr() == kObjectPoseSymbolChar) {
      return (std::string) asLabeledSymbol;
    }
    return (std::string) asLabeledSymbol;
  }

  const gtsam::Symbol asSymbol(key);
  if (asLabeledSymbol.chr() > 0) {
    if(asLabeledSymbol.chr() == kDynamicLandmarkSymbolChar) {
      const DynamicPointSymbol asDynamicPointSymbol(key);
      return (std::string) asDynamicPointSymbol;
    }
    else {
      return (std::string) asSymbol;
    }

  }
  else {
    return std::to_string(key);
  }
}



SymbolChar DynoChrExtractor(gtsam::Key key) {
   const gtsam::LabeledSymbol asLabeledSymbol(key);
  if (asLabeledSymbol.chr() > 0 && asLabeledSymbol.label() > 0) {
    return asLabeledSymbol.chr();
  }
  const gtsam::Symbol asSymbol(key);
  if (asLabeledSymbol.chr() > 0) {
    return asSymbol.chr();
  }
  else {
    return InvalidDynoSymbol;
  }

}

std::string DynoLikeKeyFormatterVerbose(gtsam::Key key) {
  const gtsam::LabeledSymbol asLabeledSymbol(key);
  if (asLabeledSymbol.chr() > 0 && asLabeledSymbol.label() > 0) {
    //if used as motion
    if(asLabeledSymbol.chr() == kObjectMotionSymbolChar) {
      ObjectId object_label;
      FrameId frame_id;
      CHECK(reconstructMotionInfo(asLabeledSymbol, object_label, frame_id));

      std::stringstream ss;
      ss << "H: label" << object_label << ", frames: " << frame_id - 1 << " -> " << frame_id;
      return ss.str();
    }
    else if(asLabeledSymbol.chr() == kObjectPoseSymbolChar) {
      ObjectId object_label;
      FrameId frame_id;
      CHECK(reconstructPoseInfo(asLabeledSymbol, object_label, frame_id));

      std::stringstream ss;
      ss << "K: label" << object_label << ", frame: " << frame_id;
      return ss.str();
    }
    return (std::string) asLabeledSymbol;
  }

  const gtsam::Symbol asSymbol(key);
  if (asLabeledSymbol.chr() > 0) {
    if(asLabeledSymbol.chr() == kDynamicLandmarkSymbolChar) {
      const DynamicPointSymbol asDynamicPointSymbol(key);

      FrameId frame_id = asDynamicPointSymbol.frameId();
      TrackletId tracklet_id = asDynamicPointSymbol.trackletId();
      std::stringstream ss;
      ss << kDynamicLandmarkSymbolChar << ": frame " << frame_id << ", tracklet " << tracklet_id;
      return ss.str();

    }
    else {
      return (std::string) asSymbol;
    }

  }
  else {
    return std::to_string(key);
  }
}

DebugInfo::ObjectInfo::operator std::string() const {
  std::stringstream ss;
    ss << "Num point factors: " << num_dynamic_factors << "\n";
    ss << "Num point variables: " << num_new_dynamic_points << "\n";
    ss << "Num motion factors: " << num_motion_factors << "\n";
    ss << "Smoothing factor added: " << std::boolalpha <<  smoothing_factor_added;
    return ss.str();
}

std::ostream& operator<<(std::ostream &os, const DebugInfo::ObjectInfo& object_info) {
    os << (std::string)object_info;
    return os;
}


DebugInfo::ObjectInfo& DebugInfo::getObjectInfo(ObjectId object_id) {
  return getObjectInfoImpl(object_id);
}

const DebugInfo::ObjectInfo& DebugInfo::getObjectInfo(ObjectId object_id) const {
  return getObjectInfoImpl(object_id);
}


BackendLogger::BackendLogger(const std::string& name_prefix)
  : EstimationModuleLogger(name_prefix + "_backend"),
    tracklet_to_object_id_file_name_("tracklet_to_object_id.csv")
{
  ellipsoid_radii_file_name_ = module_name_ + "_ellipsoid_radii.csv";

  tracklet_to_object_id_csv_ = std::make_unique<CsvWriter>(CsvHeader(
            "tracklet_id", "object_id"));

  ellipsoid_radii_csv_ = std::make_unique<CsvWriter>(CsvHeader(
            "object_id", "a", "b", "c"));
}

void BackendLogger::logTrackletIdToObjectId(const gtsam::FastMap<TrackletId, ObjectId>& mapping) {
  for(const auto&[tracklet_id, object_id] : mapping) {
    *tracklet_to_object_id_csv_ << tracklet_id << object_id;
  }
}

void BackendLogger::logEllipsoids(const gtsam::FastMap<ObjectId, gtsam::Vector3>& mapping) {
  for(const auto&[object_id, radii] : mapping) {
    *ellipsoid_radii_csv_ << object_id << radii(0) << radii(1) << radii(2);
  }
}


BackendLogger::~BackendLogger() {
  OfstreamWrapper::WriteOutCsvWriter(*ellipsoid_radii_csv_, ellipsoid_radii_file_name_);
  OfstreamWrapper::WriteOutCsvWriter(*tracklet_to_object_id_csv_, tracklet_to_object_id_file_name_);
}


} //dyno
````

## File: backend/BackendModule.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/backend/BackendModule.hpp"
#include "dynosam/backend/BackendDefinitions.hpp"

#include <glog/logging.h>

namespace dyno {

BackendModule::BackendModule(const BackendParams& params, ImageDisplayQueue* display_queue)
    :   Base("backend"),
        base_params_(params),
        display_queue_(display_queue)

{
    setFactorParams(params);

    //create callback to update gt_packet_map_ values so the derived classes dont need to manage this
    //TODO: this logic is exactly the same as in FrontendModule - functionalise!!
    registerInputCallback([=](BackendInputPacket::ConstPtr input) {
        if(input->gt_packet_) gt_packet_map_.insert2(input->getFrameId(), *input->gt_packet_);

        const BackendSpinState previous_spin_state = spin_state_;

        //update spin state
        spin_state_ = BackendSpinState(
            input->getFrameId(),
            input->getTimestamp(),
            previous_spin_state.iteration + 1
        );

    });
}

void BackendModule::setFactorParams(const BackendParams& backend_params) {
    //set static noise
    // noise_models_.static_pixel_noise = gtsam::noiseModel::Isotropic::Sigma(2u, backend_params.static_smart_projection_noise_sigma_);
    // CHECK(noise_models_.static_pixel_noise);

    //set dynamic noise
    // noise_models_.dynamic_pixel_noise = gtsam::noiseModel::Isotropic::Sigma(2u, backend_params.dynamic_smart_projection_noise_sigma_);
    // CHECK(noise_models_.dynamic_pixel_noise);

    gtsam::Vector6 odom_sigmas;
    odom_sigmas.head<3>().setConstant(backend_params.odometry_rotation_sigma_);
    odom_sigmas.tail<3>().setConstant(
        backend_params.odometry_translation_sigma_);
    noise_models_.odometry_noise = gtsam::noiseModel::Diagonal::Sigmas(odom_sigmas);
    CHECK(noise_models_.odometry_noise);

    noise_models_.initial_pose_prior =  gtsam::noiseModel::Isotropic::Sigma(6u, 0.0001);
    CHECK(noise_models_.initial_pose_prior);

    noise_models_.landmark_motion_noise = gtsam::noiseModel::Isotropic::Sigma(3u, backend_params.motion_ternary_factor_noise_sigma_);
    CHECK(noise_models_.landmark_motion_noise);

    gtsam::Vector6 object_constant_vel_sigmas;
    object_constant_vel_sigmas.head<3>().setConstant(backend_params.constant_object_motion_rotation_sigma_);
    object_constant_vel_sigmas.tail<3>().setConstant(
        backend_params.constant_object_motion_translation_sigma_);
    noise_models_.object_smoothing_noise = gtsam::noiseModel::Diagonal::Sigmas(object_constant_vel_sigmas);
    CHECK(noise_models_.object_smoothing_noise);

}


void BackendModule::validateInput(const BackendInputPacket::ConstPtr&) const {

}

} //dyno
````

## File: backend/BackendParams.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/backend/BackendParams.hpp"

DEFINE_double(static_point_sigma, 2.0, "Isotropic pixel noise used on static points");
DEFINE_double(dynamic_point_sigma, 2,"Isotropic pixel noise used on dynamic points");

DEFINE_double(constant_object_motion_rotation_sigma, 0.01, "Noise used on rotation componenent of smoothing factor");
DEFINE_double(constant_object_motion_translation_sigma,  0.1, "Noise used on translation componenent of smoothing factor");

DEFINE_double(motion_ternary_factor_noise_sigma,  0.01, "Noise used on motion ternary factor");

DEFINE_double(odometry_rotation_sigma, 0.02, "Noise used on rotation component of odometry");
DEFINE_double(odometry_translation_sigma, 0.01, "Noise used on translation component of odometry");


//TODO: need to make projection covariance!!
DEFINE_double(static_point_noise_sigma, 0.06, "Isotropic noise used on PoseToPointFactor for static points");
DEFINE_double(dynamic_point_noise_sigma, 0.0625, "Isotropic noise used on PoseToPointFactor for dynamic points");
````

## File: backend/BackendPipeline.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/backend/BackendPipeline.hpp"

#include <glog/logging.h>

namespace dyno {} //dyno
````

## File: backend/DynamicPointSymbol.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/backend/DynamicPointSymbol.hpp"
#include <glog/logging.h>

namespace dyno
{


std::uint64_t CantorPairingFunction::pair(const Pair& input) {
    const auto k1 = (input.first);
    const auto k2 = (input.second);
    return ((k1+k2)*(k1+k2+1)/2) + k2;
}

CantorPairingFunction::Pair CantorPairingFunction::depair(const std::uint64_t z) {
    std::uint64_t w = static_cast<std::uint64_t>(floor(((sqrt((z*8)+1))-1)/2));
	std::uint64_t t = static_cast<std::uint64_t>((w*(w+1))/2);

    std::uint64_t k2 = z - t;
    std::uint64_t k1 = w - k2;
    return std::make_pair(k1, k2);
}


DynamicPointSymbol::DynamicPointSymbol(const DynamicPointSymbol& key)
:   c_(key.c_), j_(key.j_), tracklet_id_(key.tracklet_id_), frame_id_(key.frame_id_) {}

DynamicPointSymbol::DynamicPointSymbol(unsigned char c, TrackletId tracklet_id, FrameId frame_id)
:   c_(c), j_(constructIndex(tracklet_id, frame_id)), tracklet_id_(tracklet_id), frame_id_(frame_id)
{
    //sanity check
    const auto result = CantorPairingFunction::depair(j_);
    CHECK_EQ(result.first, tracklet_id_);
    CHECK_EQ(result.second, frame_id_);
}

DynamicPointSymbol::DynamicPointSymbol(gtsam::Key key) {
    gtsam::Symbol sym(key);
    const unsigned char c = sym.chr();
    const std::uint64_t index = sym.index();

    c_ = c;
    j_ = index;

    recover(j_, tracklet_id_, frame_id_);
}

gtsam::Key DynamicPointSymbol::key() const {
    return (gtsam::Key)asSymbol();
}


void DynamicPointSymbol::print(const std::string& s) const {
    std::cout << s << ": " << (std::string) (*this) << std::endl;
}

bool DynamicPointSymbol::equals(const DynamicPointSymbol& expected, double) const {
    return (*this) == expected; //lazy?
}


DynamicPointSymbol::operator std::string() const {
    char buffer[20];
    snprintf(buffer, 20, "%c%ld-%lu", c_, tracklet_id_, static_cast<unsigned long>(frame_id_));
    return std::string(buffer);
}


gtsam::Symbol DynamicPointSymbol::asSymbol() const {
    return gtsam::Symbol(c_, j_);
}

std::uint64_t DynamicPointSymbol::constructIndex(TrackletId tracklet_id, FrameId frame_id) {
    if(tracklet_id == -1) {
        throw std::invalid_argument("DynamicPointSymbol cannot be constructed from invalid tracklet id (-1)");
    }

    return CantorPairingFunction::pair({tracklet_id, frame_id});
}

void DynamicPointSymbol::recover(std::uint64_t z, TrackletId& tracklet_id, FrameId& frame_id) {
    const auto result = CantorPairingFunction::depair(z);
    tracklet_id = static_cast<TrackletId>(result.first);
    frame_id = static_cast<FrameId>(result.second);
}

};  // namespace dyno
````

## File: backend/FactorGraphTools.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/backend/FactorGraphTools.hpp"
#include "dynosam/backend/BackendDefinitions.hpp"
#include "dynosam/utils/Numerical.hpp" //for saveMatrixAsUpperTriangular

#include "gtsam/linear/GaussianConditional.h"

#include <opencv4/opencv2/viz/types.hpp>
#include <gtsam/slam/BetweenFactor.h>
#include <gtsam_unstable/slam/PoseToPointFactor.h>
#include "dynosam/factors/LandmarkMotionTernaryFactor.hpp"


namespace dyno {

namespace factor_graph_tools {


SmartProjectionFactor::shared_ptr constructSmartProjectionFactor(
    gtsam::SharedNoiseModel smart_noise,
    boost::shared_ptr<CalibrationType> K,
    SmartProjectionFactorParams projection_params)
{
    CHECK(smart_noise);
    CHECK(K);

    return boost::make_shared<SmartProjectionFactor>(
                smart_noise,
                K,
                projection_params);
}

SmartProjectionFactor::shared_ptr constructSmartProjectionFactor(
    gtsam::SharedNoiseModel smart_noise,
    boost::shared_ptr<CalibrationType> K,
    SmartProjectionFactorParams projection_params,
    Keypoint measurement,
    FrameId frame_id)
{
    SmartProjectionFactor::shared_ptr smart_factor = constructSmartProjectionFactor(
        smart_noise,
        K,
        projection_params);
    CHECK(smart_factor);

    addSmartProjectionMeasurement(smart_factor, measurement, frame_id);
    return smart_factor;

}

void addBetweenFactor(FrameId from_frame, FrameId to_frame, const gtsam::Pose3 from_pose_to, gtsam::SharedNoiseModel noise_model, gtsam::NonlinearFactorGraph& graph) {
    CHECK(noise_model);
    CHECK_EQ(noise_model->dim(), 6u);

    graph.emplace_shared<gtsam::BetweenFactor<gtsam::Pose3>>(
        CameraPoseSymbol(from_frame),
        CameraPoseSymbol(to_frame),
        from_pose_to,
        noise_model
    );
}


void addSmartProjectionMeasurement(SmartProjectionFactor::shared_ptr smart_factor, Keypoint measurement, FrameId frame_id) {
    smart_factor->add(measurement, CameraPoseSymbol(frame_id));
}

SparsityStats computeHessianSparsityStats(gtsam::GaussianFactorGraph::shared_ptr gaussian_fg, const std::optional<gtsam::Ordering>& ordering) {
    if(ordering) {
        return SparsityStats(gaussian_fg->hessian(*ordering).first);
    }

    return SparsityStats(gaussian_fg->hessian().first);
}

SparsityStats computeJacobianSparsityStats(gtsam::GaussianFactorGraph::shared_ptr gaussian_fg, const std::optional<gtsam::Ordering>& ordering) {
    //NOTE: GaussianFactorGraph::jacobian(Ordering) SHOULD be the same as calling gtsam::JacobianFactor(GaussianFactorGraph, ordering)
    //and it is in the code when this was tested but I guess this could change, leading to consistencies with how the Jacobain factor is constructed
    //in differnet fg_tool functions
    if(ordering) {
        return SparsityStats(gaussian_fg->jacobian(*ordering).first);
    }

    return SparsityStats(gaussian_fg->jacobian().first);
}

std::pair<SparsityStats, cv::Mat> computeRFactor(gtsam::GaussianFactorGraph::shared_ptr gaussian_fg, const gtsam::Ordering ordering) {
    //TODO: why do we need to specify the ordering twice?
    //I think we should probaly construct the graph with native ordeing but then elimate with provided ordering?
    gtsam::JacobianFactor jacobian_factor(*gaussian_fg);

    gtsam::GaussianConditional::shared_ptr conditional;
    gtsam::JacobianFactor::shared_ptr joint_factor;

    /// Not entirely sure the difference between the joint factor we get out and the inital jacobian factor we get in
    /// I believe the difference is in the ordering which we do NOT apply when constructing the jacobian_factor. I'm not sure this is correct.
    std::tie(conditional, joint_factor) = jacobian_factor.eliminate(ordering);
    CHECK(conditional);

    const gtsam::VerticalBlockMatrix::constBlock R = conditional->R();

    //drawn image
    cv::Mat R_img(cv::Size(R.cols(), R.rows()), CV_8UC3, cv::viz::Color::white());
    for (int i = 0; i < R.rows(); ++i) {
        for (int j = 0; j < R.cols(); ++j) {
            //only draw if non zero
            if (std::fabs(R(i, j)) > 1e-15) {
                R_img.at<cv::Vec3b>(i, j) =  (cv::Vec3b)cv::viz::Color::black();
            }
        }
    }

    cv::imshow("R", R_img);
    cv::waitKey(0);


}

cv::Mat drawBlockJacobians(gtsam::GaussianFactorGraph::shared_ptr gaussian_fg, const gtsam::Ordering& ordering, const DrawBlockJacobiansOptions& options) {
    const gtsam::JacobianFactor jacobian_factor(*gaussian_fg, ordering);
    const gtsam::Matrix J = jacobian_factor.jacobian().first;

    //each cv::mat will be the drawing of a column block (associated with one variable) in the jacobian
    //Each block will have dimensions determiend by dim of the variable (determiend by the key)
    //and number of rows of the full jacobian matrix, J
    std::vector<std::pair<gtsam::Key, cv::Mat>> column_blocks;


    for(gtsam::Key key : ordering) {
        //this will have rows = J.rows(), cols = dimension of the variable
        const gtsam::VerticalBlockMatrix::constBlock Ablock = jacobian_factor.getA(jacobian_factor.find(key));
        const int var_dimensions = Ablock.cols();
        CHECK_EQ(Ablock.rows(), J.rows());

        //drawn image of each vertical block
        cv::Mat b_img(cv::Size(var_dimensions, Ablock.rows()), CV_8UC3, cv::viz::Color::white());
        for (int i = 0; i < Ablock.rows(); ++i) {
            for (int j = 0; j < var_dimensions; ++j) {
                //only draw if non zero
                if (std::fabs(Ablock(i, j)) > 1e-15) {
                    const auto colour = options.colour_selector(key);
                    b_img.at<cv::Vec3b>(i, j) =  cv::Vec3b(colour[0], colour[1], colour[2]);
                }
            }
        }

        column_blocks.push_back(std::make_pair(key, b_img));
    }

    CHECK(!options.desired_size.empty()); //cannot be empty

    const cv::Size& desired_size = options.desired_size;

    const int current_cols = J.cols();
    const int desired_cols = desired_size.width;
    const int desired_rows = desired_size.height;

    //define a ratio which which to scale each block (viz) individually
    //since we're going to add things like spacing we want to scale
    //up each block so its visible but dont want to scale up the line spacing etc.
    //the ratio ensures we get close to the final desired size of the TOTAL image
    double ratio;
    //want positive ratio
    if(current_cols < desired_cols) {
        ratio = (double)desired_cols/(double)current_cols;
    }
    else {
        ratio = (double)current_cols/(double)desired_cols;
    }

    auto scale_and_draw_label = [&options, &ratio, &desired_rows](cv::Mat& current_block, gtsam::Key key) {
        //we all each jacobiab horizintally so only need to scale the columns
        const int scaled_cols = ratio * (int)current_block.cols;

        //scale current block to the desired size with INTER_NEAREST - this keeps the pixels looking clear and visible
        cv::resize(current_block, current_block, cv::Size(scaled_cols, desired_rows), 0, 0, cv::INTER_NEAREST);

        if(options.draw_label) {
            //draw text info
            cv::Mat text_box(cv::Size(current_block.cols, options.text_box_height), CV_8UC3, cv::viz::Color::white());

            constexpr static double kFontScale = 0.5;
            constexpr static int kFontFace = cv::FONT_HERSHEY_DUPLEX;
            constexpr static int kThickness = 1;
            //draw text mid way in box
            cv::putText(text_box, options.label_formatter(key), cv::Point(2, text_box.rows/2), kFontFace, kFontScale, cv::viz::Color::black(), kThickness);
            //add text box ontop of jacobian block
            cv::vconcat(text_box, current_block, current_block);
        }
    }; //end scale_and_drawinternal::_label


    if(column_blocks.size() == 1) {
        gtsam::Key key = column_blocks.at(0).first;
        cv::Mat current_block = column_blocks.at(0).second;
        scale_and_draw_label(current_block, key);
        return current_block;
    }

    //the final drawn image
    cv::Mat concat_column_blocks;

    for(size_t i = 1; i < column_blocks.size(); i++) {
        gtsam::Key key = column_blocks.at(i).first;
        cv::Mat current_block = column_blocks.at(i).second;
        scale_and_draw_label(current_block, key);


        if(i == 1) {
            cv::Mat first_block = column_blocks.at(0).second;
            gtsam::Key key_0 = column_blocks.at(0).first;
            scale_and_draw_label(first_block, key_0);
            const cv::Mat vert_line(cv::Size(2, first_block.rows), CV_8UC3, cv::viz::Color::black());
            //concat first block with vertical loine
            cv::hconcat(first_block, vert_line, first_block);
            //concat current jacobian block with other blocks
            cv::hconcat(first_block, current_block, concat_column_blocks);
        }
        else {
            //concat current jacobian block with other blocks
            cv::hconcat(concat_column_blocks, current_block, concat_column_blocks);
        }

        //if not last, add vertical line
        if(i < column_blocks.size() - 1) {
            //concat blocks with vertial line
            const cv::Mat vert_line(cv::Size(2, concat_column_blocks.rows), CV_8UC3, cv::viz::Color::black());
            cv::hconcat(concat_column_blocks, vert_line, concat_column_blocks);
        }
    }
    return concat_column_blocks;
}

//specalisation of values
template<>
void toGraphFileFormat<gtsam::Point3>(std::ostream& os, const gtsam::Point3& t) {
    os << t.x() << " " << t.y() << " " << t.z();
}

template<>
void toGraphFileFormat<gtsam::Pose3>(std::ostream& os, const gtsam::Pose3& t) {
    const gtsam::Point3 p = t.translation();
    const auto q = t.rotation().toQuaternion();
    os << p.x() << " " << p.y() << " " << p.z() << " " << q.x() << " " << q.y() << " " << q.z() << " " << q.w();
}

//specalisation of factors
template<>
void toGraphFileFormat<gtsam::PriorFactor<gtsam::Pose3>>(std::ostream& os, const gtsam::PriorFactor<gtsam::Pose3>& t) {
    const gtsam::Pose3 measurement = t.prior();
    toGraphFileFormat(os, measurement);

    auto gaussianModel = boost::dynamic_pointer_cast<gtsam::noiseModel::Gaussian>(t.noiseModel());
    gtsam::Matrix Info = gaussianModel->information();
    saveMatrixAsUpperTriangular(os, Info);
}

template<>
void toGraphFileFormat<gtsam::BetweenFactor<gtsam::Pose3>>(std::ostream& os, const gtsam::BetweenFactor<gtsam::Pose3>& t) {
    const gtsam::Pose3 measurement = t.measured();
    toGraphFileFormat(os, measurement);

    auto gaussianModel = boost::dynamic_pointer_cast<gtsam::noiseModel::Gaussian>(t.noiseModel());
    gtsam::Matrix Info = gaussianModel->information();
    saveMatrixAsUpperTriangular(os, Info);
}

template<>
void toGraphFileFormat<LandmarkMotionTernaryFactor>(std::ostream& os, const LandmarkMotionTernaryFactor& t) {
    const gtsam::Point3 measurement(0, 0, 0);
    toGraphFileFormat(os, measurement);

    auto gaussianModel = boost::dynamic_pointer_cast<gtsam::noiseModel::Gaussian>(t.noiseModel());
    gtsam::Matrix Info = gaussianModel->information();
    saveMatrixAsUpperTriangular(os, Info);
}

template<>
void toGraphFileFormat<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(std::ostream& os, const gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>& t) {
    const gtsam::Point3 measurement = t.measured();
    toGraphFileFormat(os, measurement);

    auto gaussianModel = boost::dynamic_pointer_cast<gtsam::noiseModel::Gaussian>(t.noiseModel());
    gtsam::Matrix Info = gaussianModel->information();
    saveMatrixAsUpperTriangular(os, Info);
}



} //factor_graph_tools


void NonlinearFactorGraphManager::writeDynosamGraphFile(const std::string& filename) const {
    std::ofstream of(filename.c_str());

    LOG(INFO) << "Writing dynosam graph file: " << filename;

    using namespace factor_graph_tools;

    //specalisations for dynosam factor graph
    //gtsam does not have a write function for the base factor class so we dont actually know
    //the types contained within the NonLinearFactorGraph so we have to test them
    auto write_factor =[](std::ofstream& os, const gtsam::NonlinearFactor::shared_ptr& nl_factor) {
        //careful of mixing boost and std shared ptrs/casting here
        auto prior_factor = boost::dynamic_pointer_cast<gtsam::PriorFactor<gtsam::Pose3>>(nl_factor);
        if(prior_factor) seralizeFactorToGraphFileFormat<gtsam::PriorFactor<gtsam::Pose3>>(os, *prior_factor, "SE3_PRIOR_FACTOR");


        //TODO: should give a different name for odom/smoothing factors?
        //does not affect the graph structure but provides metadata?
        auto between_factor = boost::dynamic_pointer_cast<gtsam::BetweenFactor<gtsam::Pose3>>(nl_factor);
        if(between_factor) seralizeFactorToGraphFileFormat<gtsam::BetweenFactor<gtsam::Pose3>>(os, *between_factor, "SE3_BETWEEN_FACTOR");

        auto motion_factor = boost::dynamic_pointer_cast<LandmarkMotionTernaryFactor>(nl_factor);
        if(motion_factor) seralizeFactorToGraphFileFormat<LandmarkMotionTernaryFactor>(os, *motion_factor, "SE3_MOTION_FACTOR");

        auto pose_to_point_factor = boost::dynamic_pointer_cast<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(nl_factor);
        if(pose_to_point_factor) seralizeFactorToGraphFileFormat<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(os, *pose_to_point_factor, "POSE_TO_POINT_FACTOR");
    };

    auto write_value = [](std::ofstream& os, const gtsam::Values& value, gtsam::Key key) {
        SymbolChar chr = DynoChrExtractor(key);

        if(chr == InvalidDynoSymbol) {
            throw std::runtime_error("Cannot write value to (dynosam) graph file as the associated key is not a valid dynosam key!");
        }

        switch(chr) {
            case kPoseSymbolChar:
                seralizeValueToGraphFileFormat<gtsam::Pose3>(os, value.at<gtsam::Pose3>(key),key, "SE3_POSE_VALUE");
                break;
            case kObjectMotionSymbolChar:
                seralizeValueToGraphFileFormat<gtsam::Pose3>(os, value.at<gtsam::Pose3>(key), key, "SE3_MOTION_VALUE");
                break;
            case kStaticLandmarkSymbolChar:
                seralizeValueToGraphFileFormat<gtsam::Point3>(os, value.at<gtsam::Point3>(key), key, "POINT3_STATIC_VALUE");
                break;
            case kDynamicLandmarkSymbolChar:
                seralizeValueToGraphFileFormat<gtsam::Point3>(os, value.at<gtsam::Point3>(key), key, "POINT3_DYNAMIC_VALUE");
                break;
            default:
                LOG(FATAL) << "Should never reach here if DynoChrExtractor works!!";
        }
    };

    gtsam::KeySet seen_keys;
    for(const auto& factor : *this) {
        write_factor(of, factor);
        for(const auto key : *factor) {
            //we have not seen this key before write it out to avoid duplication
            if(seen_keys.find(key) == seen_keys.end()) {
                write_value(of, values_, key);
                seen_keys.insert(key);
            }
        }
    }

    std::flush(of);
}


} //dyno
````

## File: backend/RGBDBackendModule.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include "dynosam/backend/RGBDBackendModule.hpp"

#include <gflags/gflags.h>
#include <glog/logging.h>
#include <gtsam/nonlinear/LevenbergMarquardtOptimizer.h>
#include <gtsam_unstable/slam/PoseToPointFactor.h>

#include "dynosam/backend/Accessor.hpp"
#include "dynosam/backend/BackendInputPacket.hpp"
#include "dynosam/backend/FactorGraphTools.hpp"
#include "dynosam/backend/Formulation.hpp"
#include "dynosam/common/Flags.hpp"
#include "dynosam/factors/LandmarkMotionPoseFactor.hpp"
#include "dynosam/factors/LandmarkMotionTernaryFactor.hpp"
#include "dynosam/factors/LandmarkPoseSmoothingFactor.hpp"
#include "dynosam/factors/ObjectKinematicFactor.hpp"
#include "dynosam/logger/Logger.hpp"
#include "dynosam/utils/SafeCast.hpp"
#include "dynosam/utils/TimingStats.hpp"

DEFINE_int32(opt_window_size, 10, "Sliding window size for optimisation");
DEFINE_int32(opt_window_overlap, 4, "Overlap for window size optimisation");

DEFINE_bool(use_full_batch_opt, true,
            "Use full batch optimisation if true, else sliding window");
DEFINE_bool(
    use_vo_factor, true,
    "If true, use visual odometry measurement as factor from the frontend");

DEFINE_bool(
    use_identity_rot_L_for_init, false,
    "For experiments: set the initalisation point of L with identity rotation");
DEFINE_bool(corrupt_L_for_init, false,
            "For experiments: corrupt the initalisation point for L with "
            "gaussian noise");
DEFINE_double(corrupt_L_for_init_sigma, 0.2,
              "For experiments: sigma value to correupt initalisation point "
              "for L. When corrupt_L_for_init is true");

DEFINE_bool(init_LL_with_identity, false, "For experiments");
DEFINE_bool(init_H_with_identity, true, "For experiments");

DEFINE_string(updater_suffix, "",
              "Suffix for updater to denote specific experiments");

namespace dyno {

RGBDBackendModule::RGBDBackendModule(const BackendParams& backend_params,
                                     Map3d2d::Ptr map, Camera::Ptr camera,
                                     const UpdaterType& updater_type,
                                     ImageDisplayQueue* display_queue)
    : Base(backend_params, map, display_queue),
      camera_(CHECK_NOTNULL(camera)),
      updater_type_(updater_type) {
  CHECK_NOTNULL(map);
  CHECK_NOTNULL(map_);

  // TODO: functioanlise and streamline with BackendModule
  noise_models_.static_point_noise = gtsam::noiseModel::Isotropic::Sigma(
      3u, backend_params.static_point_noise_sigma_);
  noise_models_.dynamic_point_noise = gtsam::noiseModel::Isotropic::Sigma(
      3u, backend_params.dynamic_point_noise_sigma_);
  // set in base!
  noise_models_.landmark_motion_noise = gtsam::noiseModel::Isotropic::Sigma(
      3u, backend_params.motion_ternary_factor_noise_sigma_);

  if (backend_params.use_robust_kernals_) {
    noise_models_.static_point_noise = gtsam::noiseModel::Robust::Create(
        gtsam::noiseModel::mEstimator::Huber::Create(
            backend_params.k_huber_3d_points_),
        noise_models_.static_point_noise);

    noise_models_.dynamic_point_noise = gtsam::noiseModel::Robust::Create(
        gtsam::noiseModel::mEstimator::Huber::Create(
            backend_params.k_huber_3d_points_),
        noise_models_.dynamic_point_noise);

    // TODO: not k_huber_3d_points_ not just used for 3d points
    noise_models_.landmark_motion_noise = gtsam::noiseModel::Robust::Create(
        gtsam::noiseModel::mEstimator::Huber::Create(
            backend_params.k_huber_3d_points_),
        noise_models_.landmark_motion_noise);
  }

  CHECK_NOTNULL(noise_models_.static_point_noise);
  CHECK_NOTNULL(noise_models_.dynamic_point_noise);
  CHECK_NOTNULL(noise_models_.landmark_motion_noise);

  noise_models_.dynamic_point_noise->print("Dynamic Point Noise");
  noise_models_.landmark_motion_noise->print("Landmark motion noise");
  // CHECK(false);

  new_updater_ = std::move(makeUpdater());
  sliding_window_condition_ = std::make_unique<SlidingWindow>(
      FLAGS_opt_window_size, FLAGS_opt_window_overlap);
}

RGBDBackendModule::~RGBDBackendModule() {
  LOG(INFO) << "Destructing RGBDBackendModule";

  if (base_params_.use_logger_) {
    // hack to make sure things are updated!!
    BackendMetaData backend_info;
    backend_info.params = base_params_;
    backend_info.ground_truth_packets = this->getGroundTruthPackets();

    new_updater_->accessorFromTheta()->postUpdateCallback(backend_info);
    new_updater_->logBackendFromMap(backend_info);
  }
}

RGBDBackendModule::SpinReturn RGBDBackendModule::boostrapSpinImpl(
    RGBDInstanceOutputPacket::ConstPtr input) {
  const FrameId frame_k = input->getFrameId();
  first_frame_id_ = frame_k;
  CHECK_EQ(spin_state_.frame_id, frame_k);
  LOG(INFO) << "Running backend " << frame_k;
  CHECK(!(bool)sliding_window_condition_->check(
      frame_k));  // trigger the check to update the first frame call. Bit
                  // gross!

  // Pose estimate from the front-end
  gtsam::Pose3 T_world_cam_k_frontend;
  updateMap(T_world_cam_k_frontend, input);

  gtsam::Values new_values;
  gtsam::NonlinearFactorGraph new_factors;

  new_updater_->setInitialPose(T_world_cam_k_frontend, frame_k, new_values);
  new_updater_->setInitialPosePrior(T_world_cam_k_frontend, frame_k,
                                    new_factors);
  return {State::Nominal, nullptr};
}

RGBDBackendModule::SpinReturn RGBDBackendModule::nominalSpinImpl(
    RGBDInstanceOutputPacket::ConstPtr input) {
  const FrameId frame_k = input->getFrameId();
  const Timestamp timestamp = input->getTimestamp();
  LOG(INFO) << "Running backend " << frame_k;
  CHECK_EQ(spin_state_.frame_id, frame_k);

  // Pose estimate from the front-end
  gtsam::Pose3 T_world_cam_k_frontend;
  updateMap(T_world_cam_k_frontend, input);

  gtsam::Values new_values;
  gtsam::NonlinearFactorGraph new_factors;

  new_updater_->addOdometry(frame_k, T_world_cam_k_frontend, new_values,
                            new_factors);

  UpdateObservationParams update_params;
  update_params.enable_debug_info = true;
  update_params.do_backtrack =
      false;  // apparently this is v important for making the results == ICRA

  {
    LOG(INFO) << "Starting updateStaticObservations";
    utils::TimingStatsCollector timer("backend.update_static_obs");
    new_updater_->updateStaticObservations(frame_k, new_values, new_factors,
                                           update_params);
  }

  {
    LOG(INFO) << "Starting updateDynamicObservations";
    utils::TimingStatsCollector timer("backend.update_dynamic_obs");
    new_updater_->updateDynamicObservations(frame_k, new_values, new_factors,
                                            update_params);
  }

  if (FLAGS_use_full_batch_opt) {
    LOG(INFO) << " full batch frame " << base_params_.full_batch_frame;
    if (base_params_.full_batch_frame - 1 == (int)frame_k) {
      LOG(INFO) << " Doing full batch at frame " << frame_k;

      // graph.error(values);
      gtsam::LevenbergMarquardtParams opt_params;
      opt_params.verbosity = gtsam::NonlinearOptimizerParams::Verbosity::ERROR;

      const auto theta = new_updater_->getTheta();
      const auto graph = new_updater_->getGraph();
      utils::StatsCollector(new_updater_->getFullyQualifiedName() +
                            ".full_batch_opt_num_vars_all")
          .AddSample(theta.size());

      double error_before = graph.error(theta);
      utils::TimingStatsCollector timer(new_updater_->getFullyQualifiedName() +
                                        ".full_batch_opt");

      gtsam::LevenbergMarquardtOptimizer problem(graph, theta, opt_params);
      gtsam::Values optimised_values = problem.optimize();
      double error_after = graph.error(optimised_values);

      utils::StatsCollector(new_updater_->getFullyQualifiedName() +
                            ".inner_iterations")
          .AddSample(problem.getInnerIterations());
      utils::StatsCollector(new_updater_->getFullyQualifiedName() +
                            ".iterations")
          .AddSample(problem.iterations());

      new_updater_->updateTheta(optimised_values);
      LOG(INFO) << " Error before sliding window: " << error_before
                << " error after: " << error_after;
    }
  } else {
    double error_before, error_after;
    gtsam::Values optimised_values;
    if (buildSlidingWindowOptimisation(frame_k, optimised_values, error_before,
                                       error_after)) {
      LOG(INFO) << "Updating values with opt!";
      new_updater_->updateTheta(optimised_values);
      LOG(INFO) << " Error before sliding window: " << error_before
                << " error after: " << error_after;
    }
  }

  auto accessor = new_updater_->accessorFromTheta();

  utils::TimingStatsCollector timer(new_updater_->getFullyQualifiedName() +
                                    ".post_update");
  BackendMetaData backend_info;
  backend_info.params = base_params_;
  backend_info.ground_truth_packets = this->getGroundTruthPackets();
  new_updater_->accessorFromTheta()->postUpdateCallback(
      backend_info);  // force update every time (slow! and just for testing)

  BackendOutputPacket::Ptr backend_output = constructOutputPacket(frame_k, timestamp);

  debug_info_ = DebugInfo();

  return {State::Nominal, backend_output};
}

void RGBDBackendModule::updateMap(gtsam::Pose3& T_world_cam,
                                  RGBDInstanceOutputPacket::ConstPtr input) {
  utils::TimingStatsCollector timer("map.update_observations");

  const gtsam::Pose3 T_world_cam_k_frontend = input->T_world_camera_;
  const FrameId frame_k = input->getFrameId();

  map_->updateObservations(input->collectStaticLandmarkKeypointMeasurements());
  map_->updateObservations(input->collectDynamicLandmarkKeypointMeasurements());
  // update other measurements
  map_->updateSensorPoseMeasurement(frame_k, T_world_cam_k_frontend);
  map_->updateObjectMotionMeasurements(frame_k, input->estimated_motions_);

  T_world_cam = T_world_cam_k_frontend;
}

std::tuple<gtsam::Values, gtsam::NonlinearFactorGraph>
RGBDBackendModule::constructGraph(FrameId from_frame, FrameId to_frame,
                                  bool set_initial_camera_pose_prior,
                                  std::optional<gtsam::Values> initial_theta) {
  CHECK_LT(from_frame, to_frame);
  gtsam::Values new_values;
  gtsam::NonlinearFactorGraph new_factors;

  auto updater = std::move(makeUpdater());

  if (initial_theta) {
    // update initial linearisation points (could be from a previous
    // optimisation)
    // TODO: currently cannot set theta becuase this will be ALL the previous
    // values and not just the ones in
    // TODO: for now dont do this as we have to handle covariance/santiy checks
    // differently as some modules expect values to be new and will check that
    // a value does not exist yet (becuase it shouldn't in that iteration, but
    // overall it may!)
    //  updater->setTheta(*initial_theta);
  }

  UpdateObservationParams update_params;
  update_params.do_backtrack = false;
  update_params.enable_debug_info = true;

  UpdateObservationResult results;

  CHECK_GE(from_frame, map_->firstFrameId());
  CHECK_LE(to_frame, map_->lastFrameId());

  // TODO: pick how new values are going to be used as right now they get
  // appened internallty to the thera and I think this is very slow so we should
  // not do this...

  for (auto frame_id = from_frame; frame_id <= to_frame; frame_id++) {
    LOG(INFO) << "Constructing dynamic graph at frame " << frame_id
              << " in loop (" << from_frame << " -> " << to_frame << ")";

    // pose estimate from frontend
    gtsam::Pose3 T_world_camera_k;
    CHECK(map_->hasInitialSensorPose(frame_id, &T_world_camera_k));

    // if first frame
    if (frame_id == from_frame) {
      // add first pose
      updater->setInitialPose(T_world_camera_k, frame_id, new_values);

      if (set_initial_camera_pose_prior)
        updater->setInitialPosePrior(T_world_camera_k, frame_id, new_factors);
    } else {
      updater->addOdometry(frame_id, T_world_camera_k, new_values, new_factors);
      // no backtrack
      results += updater->updateDynamicObservations(frame_id, new_values,
                                                    new_factors, update_params);
    }
    results += updater->updateStaticObservations(frame_id, new_values,
                                                 new_factors, update_params);
  }

  return {new_values, new_factors};
}

bool RGBDBackendModule::buildSlidingWindowOptimisation(
    FrameId frame_k, gtsam::Values& optimised_values, double& error_before,
    double& error_after) {
  auto condition_result = sliding_window_condition_->check(frame_k);
  if (condition_result) {
    const auto start_frame = condition_result.starting_frame;
    const auto end_frame = condition_result.ending_frame;
    LOG(INFO) << "Running dynamic slam window on between frames " << start_frame
              << " - " << end_frame;

    gtsam::Values values;
    gtsam::NonlinearFactorGraph graph;
    {
      utils::TimingStatsCollector timer(new_updater_->getFullyQualifiedName() +
                                        ".sliding_window_construction");
      std::tie(values, graph) = constructGraph(start_frame, end_frame, true,
                                               new_updater_->getTheta());
      LOG(INFO) << " Finished graph construction";
    }

    error_before = graph.error(values);
    gtsam::LevenbergMarquardtParams opt_params;
    if (VLOG_IS_ON(20))
      opt_params.verbosity = gtsam::NonlinearOptimizerParams::Verbosity::ERROR;

    utils::TimingStatsCollector timer(new_updater_->getFullyQualifiedName() +
                                      ".sliding_window_optimise");
    utils::StatsCollector(new_updater_->getFullyQualifiedName() +
                          ".sliding_window_optimise_num_vars_all")
        .AddSample(values.size());
    try {
      optimised_values =
          gtsam::LevenbergMarquardtOptimizer(graph, values, opt_params)
              .optimize();
      LOG(INFO) << "Finished op!";
    } catch (const gtsam::ValuesKeyDoesNotExist& e) {
      LOG(FATAL)
          << "gtsam::ValuesKeyDoesNotExist: key does not exist in the values"
          << DynoLikeKeyFormatter(e.key());
    }
    error_after = graph.error(optimised_values);

    return true;
  }
  return false;
}

Formulation<RGBDBackendModule::RGBDMap>::UniquePtr
RGBDBackendModule::makeUpdater() {
  FormulationParams formulation_params;
  formulation_params.min_static_observations = base_params_.min_static_obs_;
  formulation_params.min_dynamic_observations = base_params_.min_dynamic_obs_;
  formulation_params.suffix = FLAGS_updater_suffix;

  if (updater_type_ == UpdaterType::MotionInWorld) {
    LOG(INFO) << "Using MotionInWorld";
    return std::make_unique<WorldMotionFormulation>(formulation_params,
                                                    getMap(), noise_models_);

  } else if (updater_type_ == UpdaterType::LLWorld) {
    LOG(INFO) << "Using LLWorld";
    return std::make_unique<WorldPoseFormulation>(formulation_params, getMap(),
                                                  noise_models_);
  } else {
    CHECK(false) << "Not implemented";
  }
}

BackendOutputPacket::Ptr RGBDBackendModule::constructOutputPacket(FrameId frame_k, Timestamp timestamp) const {
  CHECK_NOTNULL(new_updater_);
  return RGBDBackendModule::constructOutputPacket(new_updater_, frame_k, timestamp);
}

BackendOutputPacket::Ptr RGBDBackendModule::constructOutputPacket(const Formulation<RGBDMap>::UniquePtr& formulation, FrameId frame_k, Timestamp timestamp) {
  auto accessor = formulation->accessorFromTheta();

  auto backend_output = std::make_shared<BackendOutputPacket>();
  backend_output->timestamp = timestamp;
  backend_output->frame_id = frame_k;
  backend_output->T_world_camera = accessor->getSensorPose(frame_k).get();
  backend_output->static_landmarks = accessor->getFullStaticMap();
  backend_output->optimized_object_motions = accessor->getObjectMotions(frame_k);
  backend_output->dynamic_landmarks =
      accessor->getDynamicLandmarkEstimates(frame_k);


  auto map = formulation->map();
  for (FrameId frame_id : map->getFrameIds()) {
    backend_output->optimized_camera_poses.push_back(
        accessor->getSensorPose(frame_id).get());
  }

  backend_output->optimized_object_poses = accessor->getObjectPoses();
  return backend_output;
}


}  // namespace dyno
````

## File: common/byte_tracker/ByteTracker.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/common/byte_tracker/ByteTracker.hpp"
#include "dynosam/common/byte_tracker/Detection.hpp"
#include "dynosam/common/byte_tracker/Rect.hpp"
#include "dynosam/common/byte_tracker/Track.hpp"
#include "dynosam/common/byte_tracker/Lapjv.hpp"

#include <algorithm>
#include <array>
#include <cstddef>
#include <limits>
#include <map>
#include <set>
#include <stdexcept>
#include <tuple>
#include <utility>
#include <vector>

#include <glog/logging.h>

namespace dyno {
namespace byte_track {

ByteTracker::ByteTracker(const ByteTrackerParams& params)
    : track_thresh_(params.track_thresh),
      high_thresh_(params.high_thresh),
      match_thresh_(params.match_thresh),
      max_time_lost_(static_cast<size_t>(params.frame_rate / 30.0 * params.track_buffer)),
      frame_id_(params.starting_frame),
      track_id_count_(0),
      count_(0) {}

std::vector<Track::Ptr> ByteTracker::update(
    const std::vector<DetectionBase::Ptr> &input_detections, size_t frame_id) {
  CHECK_GE(frame_id, frame_id_);
  // if first call of update, check incoming frame is the same as the starting frame
  // TODO: (jesse) this is super weird logic - there is a dependancy on ByteTrackerParams::params::starting_frame
  // but nothing inside this class enforces consistnecy, yet we check that the frame calls incrementing!
  // if(count_ == 0) CHECK_EQ(frame_id, frame_id_);
  // // if not first call of update, (when frame id should be set t)
  // if(count_ > 0) CHECK_EQ(frame_id, frame_id_ + 1u);

  //update frame count
  frame_id_ = frame_id;
  count_++;

  ////////// Step 1: Get detections                                   //////////

  // Sort new tracks from detection by score
  std::vector<DetectionBase::Ptr> detections;
  std::vector<DetectionBase::Ptr> low_score_detections;
  for (const auto &detection : input_detections) {
    if (detection->score() >= track_thresh_)
      detections.push_back(detection);
    else
      low_score_detections.push_back(detection);
  }

  // Sort existing tracks by confirmed status
  std::vector<Track::Ptr> confirmed_tracks;
  std::vector<Track::Ptr> unconfirmed_tracks;

  for (const auto &track : tracked_tracks_) {
    if (!track->is_confirmed())
      unconfirmed_tracks.push_back(track);
    else
      confirmed_tracks.push_back(track);
  }

  std::vector<Track::Ptr> track_pool;
  track_pool = joint_tracks(confirmed_tracks, lost_tracks_);

  // Predict current pose by KF
  for (auto &track : track_pool) track->predict();

  ////////// Step 2: Find matches between tracks and detections       //////////
  ////////// Step 2: First association, with IoU                      //////////
  auto [matched_tracks, unmatched_tracked_tracks, unmatched_detections] =
      iou_association(track_pool, detections);

  ////////// Step 3: Second association, using low score dets         //////////
  auto new_lost_tracks = low_score_association(
      matched_tracks, low_score_detections, unmatched_tracked_tracks);

  ////////// Step 4: Init new tracks                                  //////////
  auto removed_tracks =
      init_new_tracks(matched_tracks, unconfirmed_tracks, unmatched_detections);

  ////////// Step 5: Update state                                     //////////
  for (auto &lost_track : lost_tracks_) {
    if (frame_id_ - lost_track->get_frame_id() > max_time_lost_) {
      removed_tracks.push_back(lost_track);
    }
  }

  lost_tracks_ = sub_tracks(
      joint_tracks(sub_tracks(lost_tracks_, matched_tracks), new_lost_tracks),
      removed_tracks);

  std::tie(tracked_tracks_, lost_tracks_) =
      remove_duplicate_tracks(matched_tracks, lost_tracks_);

  std::vector<Track::Ptr> output_tracks;
  for (const auto &track : tracked_tracks_) {
    if (track->is_confirmed()) output_tracks.push_back(track);
  }

  return output_tracks;
}

void ByteTracker::clear() {
  tracked_tracks_.clear();
  lost_tracks_.clear();
  frame_id_ = 0;
  track_id_count_ = 0;
}

std::tuple<std::vector<Track::Ptr>, std::vector<Track::Ptr>,
           std::vector<DetectionBase::Ptr>>
ByteTracker::iou_association(const std::vector<Track::Ptr> &track_pool,
                             const std::vector<DetectionBase::Ptr> &detections) {
  auto [matches, unmatched_tracks, unmatched_detections] =
      linear_assignment(track_pool, detections, match_thresh_);

  std::vector<Track::Ptr> matched_tracks;
  for (const auto &match : matches) {
    const auto track = match.first;
    const auto detection = match.second;
    track->update(detection, frame_id_);
    matched_tracks.push_back(track);
  }

  std::vector<Track::Ptr> unmatched_tracked_tracks;
  for (const auto &unmatch : unmatched_tracks) {
    if (unmatch->get_track_state() == TrackState::Tracked) {
      unmatched_tracked_tracks.push_back(unmatch);
    }
  }
  return {std::move(matched_tracks), std::move(unmatched_tracked_tracks),
          std::move(unmatched_detections)};
}

std::vector<Track::Ptr> ByteTracker::low_score_association(
    std::vector<Track::Ptr> &matched_tracks,
    const std::vector<DetectionBase::Ptr> &low_score_detections,
    const std::vector<Track::Ptr> &unmatched_tracked_tracks) {
  auto [matches, unmatched_tracks, unmatch_detection] =
      linear_assignment(unmatched_tracked_tracks, low_score_detections, 0.5);

  for (const auto &match : matches) {
    const auto track = match.first;
    const auto detection = match.second;
    track->update(detection, frame_id_);
    matched_tracks.push_back(track);
  }

  std::vector<Track::Ptr> new_lost_tracks;
  for (const auto &track : unmatched_tracks) {
    if (track->get_track_state() != TrackState::Lost) {
      track->mark_as_lost();
      new_lost_tracks.push_back(track);
    }
  }
  return new_lost_tracks;
}

std::vector<Track::Ptr> ByteTracker::init_new_tracks(
    std::vector<Track::Ptr> &matched_tracks,
    const std::vector<Track::Ptr> &unconfirmed_tracks,
    const std::vector<DetectionBase::Ptr> &unmatched_detections) {
  // Deal with unconfirmed tracks, usually tracks with only one beginning frame
  auto [matches, unmatched_unconfirmed_tracks, new_detections] =
      linear_assignment(unconfirmed_tracks, unmatched_detections, 0.7);

  for (const auto &match : matches) {
    match.first->update(match.second, frame_id_);
    matched_tracks.push_back(match.first);
  }

  std::vector<Track::Ptr> new_removed_tracks;
  for (const auto &track : unmatched_unconfirmed_tracks) {
    new_removed_tracks.push_back(track);
  }

  // Add new tracks
  for (const auto &detection : new_detections) {
    if (detection->score() < track_thresh_) continue;
    track_id_count_++;
    Track::Ptr new_track =
        std::make_shared<Track>(detection, frame_id_, track_id_count_);
    if (detection->score() >= high_thresh_) {
      new_track->mark_as_confirmed();
    }
    matched_tracks.push_back(new_track);
  }
  return new_removed_tracks;
}

std::vector<Track::Ptr> ByteTracker::joint_tracks(
    const std::vector<Track::Ptr> &a_tlist,
    const std::vector<Track::Ptr> &b_tlist) const {
  std::set<int> exists;
  std::vector<Track::Ptr> res;
  for (auto &track : a_tlist) {
    exists.emplace(track->get_track_id());
    res.push_back(track);
  }
  for (auto &track : b_tlist) {
    if (exists.count(track->get_track_id()) == 0) res.push_back(track);
  }
  return res;
}

std::vector<Track::Ptr> ByteTracker::sub_tracks(
    const std::vector<Track::Ptr> &a_tlist,
    const std::vector<Track::Ptr> &b_tlist) const {
  std::map<int, Track::Ptr> tracks;
  for (auto &track : a_tlist) tracks.emplace(track->get_track_id(), track);
  for (auto &track : b_tlist) tracks.erase(track->get_track_id());

  std::vector<Track::Ptr> res;
  for (auto &[_, track] : tracks) res.push_back(track);
  return res;
}

std::tuple<std::vector<Track::Ptr>, std::vector<Track::Ptr>>
ByteTracker::remove_duplicate_tracks(
    const std::vector<Track::Ptr> &a_tracks,
    const std::vector<Track::Ptr> &b_tracks) const {
  if (a_tracks.empty() || b_tracks.empty()) return {a_tracks, b_tracks};

  std::vector<std::vector<float>> ious;
  ious.resize(a_tracks.size());
  for (size_t i = 0; i < ious.size(); i++) ious[i].resize(b_tracks.size());
  for (size_t ai = 0; ai < a_tracks.size(); ai++) {
    for (size_t bi = 0; bi < b_tracks.size(); bi++) {
      ious[ai][bi] = 1 - calc_iou(b_tracks[bi]->predicted_rect_,
                                  a_tracks[ai]->predicted_rect_);
    }
  }

  std::vector<bool> a_overlapping(a_tracks.size(), false),
      b_overlapping(b_tracks.size(), false);
  for (size_t ai = 0; ai < ious.size(); ai++) {
    for (size_t bi = 0; bi < ious[ai].size(); bi++) {
      if (ious[ai][bi] < 0.15) {
        const int timep =
            a_tracks[ai]->get_frame_id() - a_tracks[ai]->get_start_frame_id();
        const int timeq =
            b_tracks[bi]->get_frame_id() - b_tracks[bi]->get_start_frame_id();
        if (timep > timeq) {
          b_overlapping[bi] = true;
        } else {
          a_overlapping[ai] = true;
        }
      }
    }
  }

  std::vector<Track::Ptr> a_tracks_out;
  for (size_t ai = 0; ai < a_tracks.size(); ai++) {
    if (!a_overlapping[ai]) a_tracks_out.push_back(a_tracks[ai]);
  }

  std::vector<Track::Ptr> b_tracks_out;
  for (size_t bi = 0; bi < b_tracks.size(); bi++) {
    if (!b_overlapping[bi]) b_tracks_out.push_back(b_tracks[bi]);
  }
  return {std::move(a_tracks_out), std::move(b_tracks_out)};
}

std::tuple<std::vector<std::pair<Track::Ptr, DetectionBase::Ptr>>,
           std::vector<Track::Ptr>, std::vector<DetectionBase::Ptr>>
ByteTracker::linear_assignment(const std::vector<Track::Ptr> &tracks,
                               const std::vector<DetectionBase::Ptr> &detections,
                               float thresh) const {
  if (tracks.empty() || detections.empty()) return {{}, tracks, detections};

  size_t n_rows = tracks.size();
  size_t n_cols = detections.size();
  std::vector<float> cost_matrix(n_rows * n_cols);
  for (size_t i = 0; i < n_rows; i++) {
    for (size_t j = 0; j < n_cols; j++) {
      cost_matrix[i * n_cols + j] =
          1 - calc_iou(detections[j]->rect(), tracks[i]->predicted_rect_);
    }
  }

  std::vector<std::pair<Track::Ptr, DetectionBase::Ptr>> matches;
  std::vector<Track::Ptr> a_unmatched;
  std::vector<DetectionBase::Ptr> b_unmatched;

  auto [rowsol, colsol, _] =
      exec_lapjv(std::move(cost_matrix), n_rows, n_cols, true, thresh);
  for (size_t i = 0; i < rowsol.size(); i++) {
    if (rowsol[i] >= 0)
      matches.push_back({tracks[i], detections[rowsol[i]]});
    else
      a_unmatched.push_back(tracks[i]);
  }

  for (size_t i = 0; i < colsol.size(); i++) {
    if (colsol[i] < 0) b_unmatched.push_back(detections[i]);
  }
  return {std::move(matches), std::move(a_unmatched), std::move(b_unmatched)};
}

std::tuple<std::vector<int>, std::vector<int>, double> ByteTracker::exec_lapjv(
    std::vector<float> &&cost, size_t n_rows, size_t n_cols, bool extend_cost,
    float cost_limit, bool return_cost) const {
  std::vector<int> rowsol(n_rows);
  std::vector<int> colsol(n_cols);

  if (n_rows != n_cols && !extend_cost) {
    throw std::runtime_error("The `extend_cost` variable should set True");
  }

  size_t n = 0;
  std::vector<float> cost_c;
  if (extend_cost || cost_limit < std::numeric_limits<float>::max()) {
    n = n_rows + n_cols;
    cost_c.resize(n * n);
    if (cost_limit < std::numeric_limits<float>::max()) {
      cost_c.assign(cost_c.size(), cost_limit / 2.0);
    } else {
      cost_c.assign(cost_c.size(),
                    *std::max_element(cost.begin(), cost.end()) + 1);
    }
    // Assign cost to top-left corner
    for (size_t i = 0; i < n_rows; i++) {
      for (size_t j = 0; j < n_cols; j++) {
        cost_c[i * n + j] = cost[i * n_cols + j];
      }
    }
    // Set bottom-right corner to 0
    for (size_t i = n_rows; i < n; i++) {
      for (size_t j = n_cols; j < n; j++) {
        cost_c[i * n + j] = 0;
      }
    }
  } else {
    n = n_rows;
    cost_c = std::move(cost);
  }

  std::vector<int> x_c(n), y_c(n);

  int ret = lapjv_internal(n, cost_c, x_c, y_c);
  if (ret != 0) {
    throw std::runtime_error("The result of lapjv_internal() is invalid.");
  }

  double opt = 0.0;

  if (n != n_rows) {
    for (size_t i = 0; i < n; i++) {
      if (x_c[i] >= static_cast<int>(n_cols)) x_c[i] = -1;
      if (y_c[i] >= static_cast<int>(n_rows)) y_c[i] = -1;
    }
    for (size_t i = 0; i < n_rows; i++) {
      rowsol[i] = x_c[i];
    }
    for (size_t i = 0; i < n_cols; i++) {
      colsol[i] = y_c[i];
    }

    if (return_cost) {
      for (size_t i = 0; i < rowsol.size(); i++) {
        if (rowsol[i] != -1) {
          opt += cost_c[i * n_cols + rowsol[i]];
        }
      }
    }
  } else if (return_cost) {
    for (size_t i = 0; i < rowsol.size(); i++) {
      opt += cost_c[i * n_cols + rowsol[i]];
    }
  }

  return {std::move(rowsol), std::move(colsol), opt};
}
}  // namespace byte_track
}  // namespace dyno
````

## File: common/byte_tracker/Detection.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/common/byte_tracker/Detection.hpp"
#include "dynosam/common/byte_tracker/Rect.hpp"

namespace dyno {
namespace byte_track {

Detection::Detection(const TlwhRect &rect, float score)
    : rect_(rect), score_(score) {}

const TlwhRect &Detection::rect() const { return rect_; }
float Detection::score() const { return score_; }

void Detection::set_rect(const RectBase &rect) { rect_ = TlwhRect(rect); }
void Detection::set_score(float score) { score_ = score; }

}  // namespace byte_track
}  // namespace dyno
````

## File: common/byte_tracker/KalmanFilter.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/common/byte_tracker/KalmanFilter.hpp"
#include "dynosam/common/byte_tracker/Rect.hpp"

#include <eigen3/Eigen/Dense>

#include <cstddef>

namespace dyno {
namespace byte_track {
KalmanFilter::KalmanFilter(float std_weight_position, float std_weight_velocity)
    : std_weight_position_(std_weight_position),
      std_weight_velocity_(std_weight_velocity) {
  motion_mat_ = Eigen::MatrixXf::Identity(8, 8);
  motion_mat_.block<4, 4>(0, 4) = Eigen::Matrix4f::Identity();
  update_mat_ = Eigen::MatrixXf::Identity(4, 8);
}

void KalmanFilter::initiate(const RectBase &measurement) {
  mean_.block<1, 4>(0, 0) = rect_to_xyah(measurement);
  mean_.block<1, 4>(0, 4) = Eigen::Vector4f::Zero();

  Matrix<1, 8> std;
  std(0) = 2 * std_weight_position_ * measurement.height();
  std(1) = 2 * std_weight_position_ * measurement.height();
  std(2) = 1e-2;
  std(3) = 2 * std_weight_position_ * measurement.height();
  std(4) = 10 * std_weight_velocity_ * measurement.height();
  std(5) = 10 * std_weight_velocity_ * measurement.height();
  std(6) = 1e-5;
  std(7) = 10 * std_weight_velocity_ * measurement.height();

  Matrix<1, 8> tmp = std.array().square();
  covariance_ = tmp.asDiagonal();
}

TlwhRect KalmanFilter::predict(bool mean_eight_to_zero) {
  if (mean_eight_to_zero) mean_[7] = 0;
  Matrix<1, 8> std;
  std(0) = std_weight_position_ * mean_(3);
  std(1) = std_weight_position_ * mean_(3);
  std(2) = 1e-2;
  std(3) = std_weight_position_ * mean_(3);
  std(4) = std_weight_velocity_ * mean_(3);
  std(5) = std_weight_velocity_ * mean_(3);
  std(6) = 1e-5;
  std(7) = std_weight_velocity_ * mean_(3);

  Matrix<1, 8> tmp = std.array().square();
  Matrix<8, 8> motion_cov = tmp.asDiagonal();

  mean_ = motion_mat_ * mean_.transpose();
  covariance_ =
      motion_mat_ * covariance_ * motion_mat_.transpose() + motion_cov;
  return xyah_to_tlwh(mean_.block<1, 4>(0, 0));
}

TlwhRect KalmanFilter::update(const RectBase &measurement) {
  Matrix<1, 4> projected_mean;
  Matrix<4, 4> projected_cov;
  project(projected_mean, projected_cov);

  Eigen::Matrix<float, 4, 8> B =
      (covariance_ * update_mat_.transpose()).transpose();
  Eigen::Matrix<float, 8, 4> kalman_gain =
      (projected_cov.llt().solve(B)).transpose();
  Eigen::Matrix<float, 1, 4> innovation =
      rect_to_xyah(measurement) - projected_mean;

  const auto tmp = innovation * kalman_gain.transpose();
  mean_ = (mean_.array() + tmp.array()).matrix();
  covariance_ =
      covariance_ - kalman_gain * projected_cov * kalman_gain.transpose();
  return xyah_to_tlwh(mean_.block<1, 4>(0, 0));
}

void KalmanFilter::project(Matrix<1, 4> &projected_mean,
                           Matrix<4, 4> &projected_covariance) {
  Matrix<1, 4> std{std_weight_position_ * mean_(3),
                   std_weight_position_ * mean_(3), 1e-1,
                   std_weight_position_ * mean_(3)};

  projected_mean = update_mat_ * mean_.transpose();
  projected_covariance = update_mat_ * covariance_ * update_mat_.transpose();

  Eigen::Matrix<float, 4, 4> diag = std.asDiagonal();
  projected_covariance += diag.array().square().matrix();
}

KalmanFilter::Matrix<1, 4> KalmanFilter::rect_to_xyah(
    const RectBase &rect) const {
  return Matrix<1, 4>{rect.left() + rect.width() / 2,
                      rect.top() + rect.height() / 2,
                      rect.width() / rect.height(), rect.height()};
}

TlwhRect KalmanFilter::xyah_to_tlwh(const Matrix<1, 4> &xyah) const {
  float xyah_width = xyah(2) * xyah(3);
  return TlwhRect{xyah(1) - xyah(3) / 2, xyah(0) - xyah_width / 2, xyah_width,
                  xyah(3)};
}

}  // namespace byte_track
}  // namespace dyno
````

## File: common/byte_tracker/Lapjv.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/common/byte_tracker/Lapjv.hpp"

#include <cstddef>
#include <cstring>
#include <stdexcept>
#include <utility>
#include <vector>

namespace {
constexpr size_t LARGE = 1000000;

enum class fp_t {
  FP_1 = 1,
  FP_2 = 2,
  FP_DYNAMIC = 3,
};

/** Column-reduction and reduction transfer for a dense cost matrix.
 */
int _ccrrt_dense(const size_t n, const std::vector<float> &cost,
                 std::vector<int> &free_rows, std::vector<int> &x,
                 std::vector<int> &y, std::vector<double> &v) {
  int n_free_rows;
  std::vector<bool> unique(n, true);

  for (size_t i = 0; i < n; i++) {
    x[i] = -1;
    v[i] = LARGE;
    y[i] = 0;
  }
  for (size_t i = 0; i < n; i++) {
    for (size_t j = 0; j < n; j++) {
      const double c = cost[i * n + j];
      if (c < v[j]) {
        v[j] = c;
        y[j] = i;
      }
    }
  }
  {
    int j = n;
    do {
      j--;
      const int i = y[j];
      if (x[i] < 0) {
        x[i] = j;
      } else {
        unique[i] = false;
        y[j] = -1;
      }
    } while (j > 0);
  }
  n_free_rows = 0;
  for (size_t i = 0; i < n; i++) {
    if (x[i] < 0) {
      free_rows[n_free_rows++] = i;
    } else if (unique[i]) {
      const int j = x[i];
      double min = LARGE;
      for (size_t j2 = 0; j2 < n; j2++) {
        if (j2 == (size_t)j) {
          continue;
        }
        const double c = cost[i * n + j2] - v[j2];
        if (c < min) {
          min = c;
        }
      }
      v[j] -= min;
    }
  }
  return n_free_rows;
}

/** Augmenting row reduction for a dense cost matrix.
 */
int _carr_dense(const size_t n, const std::vector<float> &cost,
                const size_t n_free_rows, std::vector<int> &free_rows,
                std::vector<int> &x, std::vector<int> &y,
                std::vector<double> &v) {
  size_t current = 0;
  int new_free_rows = 0;
  size_t rr_cnt = 0;
  while (current < n_free_rows) {
    int i0;
    int j1, j2;
    double v1, v2, v1_new;
    bool v1_lowers;

    rr_cnt++;
    const int free_i = free_rows[current++];
    j1 = 0;
    v1 = cost[free_i * n] - v[0];
    j2 = -1;
    v2 = LARGE;
    for (size_t j = 1; j < n; j++) {
      const double c = cost[free_i * n + j] - v[j];
      if (c < v2) {
        if (c >= v1) {
          v2 = c;
          j2 = j;
        } else {
          v2 = v1;
          v1 = c;
          j2 = j1;
          j1 = j;
        }
      }
    }
    i0 = y[j1];
    v1_new = v[j1] - (v2 - v1);
    v1_lowers = v1_new < v[j1];
    if (rr_cnt < current * n) {
      if (v1_lowers) {
        v[j1] = v1_new;
      } else if (i0 >= 0 && j2 >= 0) {
        j1 = j2;
        i0 = y[j2];
      }
      if (i0 >= 0) {
        if (v1_lowers) {
          free_rows[--current] = i0;
        } else {
          free_rows[new_free_rows++] = i0;
        }
      }
    } else {
      if (i0 >= 0) {
        free_rows[new_free_rows++] = i0;
      }
    }
    x[free_i] = j1;
    y[j1] = free_i;
  }
  return new_free_rows;
}

/** Find columns with minimum d[j] and put them on the SCAN list.
 */
size_t _find_dense(const size_t n, size_t lo, const std::vector<double> &d,
                   std::vector<int> &cols) {
  size_t hi = lo + 1;
  double mind = d[cols[lo]];
  for (size_t k = hi; k < n; k++) {
    int j = cols[k];
    if (d[j] <= mind) {
      if (d[j] < mind) {
        hi = lo;
        mind = d[j];
      }
      cols[k] = cols[hi];
      cols[hi++] = j;
    }
  }
  return hi;
}

// Scan all columns in TODO starting from arbitrary column in SCAN
// and try to decrease d of the TODO columns using the SCAN column.
int _scan_dense(const size_t n, const std::vector<float> &cost, size_t *plo,
                size_t *phi, std::vector<double> &d, std::vector<int> &cols,
                std::vector<int> &pred, const std::vector<int> &y,
                const std::vector<double> &v) {
  size_t lo = *plo;
  size_t hi = *phi;
  double h, cred_ij;

  while (lo != hi) {
    int j = cols[lo++];
    const int i = y[j];
    const double mind = d[j];
    h = cost[i * n + j] - v[j] - mind;
    // For all columns in TODO
    for (size_t k = hi; k < n; k++) {
      j = cols[k];
      cred_ij = cost[i * n + j] - v[j] - h;
      if (cred_ij < d[j]) {
        d[j] = cred_ij;
        pred[j] = i;
        if (cred_ij == mind) {
          if (y[j] < 0) {
            return j;
          }
          cols[k] = cols[hi];
          cols[hi++] = j;
        }
      }
    }
  }
  *plo = lo;
  *phi = hi;
  return -1;
}

/** Single iteration of modified Dijkstra shortest path algorithm as explained
 * in the JV paper.
 *
 * This is a dense matrix version.
 *
 * \return The closest free column index.
 */
int find_path_dense(const size_t n, const std::vector<float> &cost,
                    const int start_i, const std::vector<int> &y,
                    std::vector<double> &v, std::vector<int> &pred) {
  size_t lo = 0, hi = 0;
  int final_j = -1;
  size_t n_ready = 0;
  std::vector<int> cols(n);
  std::vector<double> d(n);

  for (size_t i = 0; i < n; i++) {
    cols[i] = i;
    pred[i] = start_i;
    d[i] = cost[start_i * n + i] - v[i];
  }

  while (final_j == -1) {
    // No columns left on the SCAN list.
    if (lo == hi) {
      n_ready = lo;
      hi = _find_dense(n, lo, d, cols);
      for (size_t k = lo; k < hi; k++) {
        const int j = cols[k];
        if (y[j] < 0) {
          final_j = j;
        }
      }
    }
    if (final_j == -1) {
      final_j = _scan_dense(n, cost, &lo, &hi, d, cols, pred, y, v);
    }
  }

  {
    const double mind = d[cols[lo]];
    for (size_t k = 0; k < n_ready; k++) {
      const int j = cols[k];
      v[j] += d[j] - mind;
    }
  }

  return final_j;
}

/** Augment for a dense cost matrix.
 */
int _ca_dense(const size_t n, const std::vector<float> &cost,
              const size_t n_free_rows, const std::vector<int> &free_rows,
              std::vector<int> &x, std::vector<int> &y,
              std::vector<double> &v) {
  std::vector<int> pred(n);

  for (size_t row_n = 0; row_n < n_free_rows; ++row_n) {
    const int free_row = free_rows[row_n];
    int i = -1;
    size_t k = 0;

    int j = find_path_dense(n, cost, free_row, y, v, pred);
    if (j < 0) {
      throw std::runtime_error("Error occured in _ca_dense(): j < 0");
    }
    if (j >= static_cast<int>(n)) {
      throw std::runtime_error("Error occured in _ca_dense(): j >= n");
    }
    while (i != free_row) {
      i = pred[j];
      y[j] = i;
      std::swap(j, x[i]);
      k++;
      if (k >= n) {
        throw std::runtime_error("Error occured in _ca_dense(): k >= n");
      }
    }
  }
  return 0;
}
}  // namespace

/** Solve dense sparse LAP. */
int dyno::byte_track::lapjv_internal(const size_t n, const std::vector<float> &cost,
                               std::vector<int> &x, std::vector<int> &y) {
  int ret;
  std::vector<int> free_rows(n);
  std::vector<double> v(n);
  ret = _ccrrt_dense(n, cost, free_rows, x, y, v);
  int i = 0;
  while (ret > 0 && i < 2) {
    ret = _carr_dense(n, cost, ret, free_rows, x, y, v);
    i++;
  }
  if (ret > 0) {
    ret = _ca_dense(n, cost, ret, free_rows, x, y, v);
  }
  return ret;
}
````

## File: common/byte_tracker/Rect.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/common/byte_tracker/Rect.hpp"

#include <algorithm>

namespace dyno {
namespace byte_track {

float calc_iou(const RectBase& A, const RectBase& B) {
  const float iw = std::min(A.left() + A.width(), B.left() + B.width()) -
                   std::max(A.left(), B.left());
  float iou = 0;
  if (iw > 0) {
    const float ih = std::min(A.top() + A.height(), B.top() + B.height()) -
                     std::max(A.top(), B.top());
    if (ih > 0) {
      const float ua =
          A.width() * A.height() + B.width() * B.height() - iw * ih;
      iou = iw * ih / ua;
    }
  }
  return iou;
}

TlwhRect::TlwhRect(float top, float left, float width, float height)
    : top_(top), left_(left), width_(width), height_(height) {}

TlwhRect::TlwhRect(const cv::Rect& cv_other)
  : top_(static_cast<float>(cv_other.y)),
    left_(static_cast<float>(cv_other.x)),
    width_(static_cast<float>(cv_other.width)),
    height_(static_cast<float>(cv_other.height)) {}


TlwhRect::TlwhRect(const RectBase& other)
    : top_(other.top()),
      left_(other.left()),
      width_(other.width()),
      height_(other.height()) {}

float TlwhRect::top() const { return top_; }
float TlwhRect::left() const { return left_; }
float TlwhRect::width() const { return width_; }
float TlwhRect::height() const { return height_; }

}  // namespace byte_track
}  // namespace dyno
````

## File: common/byte_tracker/Track.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/common/byte_tracker/Track.hpp"
#include "dynosam/common/byte_tracker/Detection.hpp"

#include <cstddef>

namespace dyno {
namespace byte_track {

Track::Track(DetectionBase::Ptr detection, size_t start_frame_id, size_t track_id)
    : detection_(detection),
      predicted_rect_(detection->rect()),
      kalman_filter_(),
      state_(TrackState::Tracked),
      // Detections registered on first frame are considered as confirmed
      is_confirmed_(start_frame_id == 1 ? true : false),
      track_id_(track_id),
      frame_id_(start_frame_id),
      start_frame_id_(start_frame_id),
      tracklet_len_(0) {
  kalman_filter_.initiate(detection->rect());
}

const TrackState& Track::get_track_state() const { return state_; }

bool Track::is_confirmed() const { return is_confirmed_; }

size_t Track::get_track_id() const { return track_id_; }

size_t Track::get_frame_id() const { return frame_id_; }

size_t Track::get_start_frame_id() const { return start_frame_id_; }

size_t Track::get_tracklet_length() const { return tracklet_len_; }

const DetectionBase::ConstPtr Track::get_detection() const { return detection_; }
TlwhRect Track::get_prediction() const { return predicted_rect_; }

void Track::predict() {
  predicted_rect_ = kalman_filter_.predict(state_ != TrackState::Tracked);
}

void Track::update(const DetectionBase::Ptr& matched_detection, size_t frame_id) {
  detection_->set_rect(matched_detection->rect());
  detection_->set_score(matched_detection->score());
  predicted_rect_ = kalman_filter_.update(matched_detection->rect());

  // If the track was actively tracked, just increment the tracklet length
  // Otherwise, mark the track as tracked again and reset the tracklet length
  if (state_ == TrackState::Tracked) {
    tracklet_len_++;
  } else {
    state_ = TrackState::Tracked;
    tracklet_len_ = 0;
  }
  is_confirmed_ = true;
  frame_id_ = frame_id;
}

void Track::mark_as_lost() { state_ = TrackState::Lost; }

void Track::mark_as_confirmed() { is_confirmed_ = true; }

}  // namespace byte_track
}  // namespace dyno
````

## File: common/Algorithms.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/common/Algorithms.hpp"

#include <cfloat> // for DBL_MAX
#include <cmath>  // for fabs()


#include <glog/logging.h>

namespace dyno {

namespace internal {

double HungarianAlgorithm::solve(const Eigen::MatrixXd& cost_matrix, Eigen::VectorXi& assignment) const {
    const auto nRows = cost_matrix.rows();
	const auto nCols = cost_matrix.cols();

	double *distMatrixIn = new double[nRows * nCols];
	int *assignmentArr = new int[nRows];
	double cost = 0.0;

	// Fill in the distMatrixIn. Mind the index is "i + nRows * j".
	// Here the cost matrix of size MxN is defined as a double precision array of N*M elements.
	// In the solving functions matrices are seen to be saved MATLAB-internally in row-order.
	// (i.e. the matrix [1 2; 3 4] will be stored as a vector [1 3 2 4], NOT [1 2 3 4]).
	for (unsigned int i = 0; i < nRows; i++)
		for (unsigned int j = 0; j < nCols; j++)
			distMatrixIn[i + nRows * j] = cost_matrix(i, j);


	// call solving function
	assignmentoptimal(assignmentArr, &cost, distMatrixIn, nRows, nCols);

    assignment.resize(nRows);
	for (unsigned int r = 0; r < nRows; r++)
		assignment(r) = assignmentArr[r];

	delete[] distMatrixIn;
	delete[] assignmentArr;
	return cost;
}


void HungarianAlgorithm::assignmentoptimal(int *assignment, double *cost, double *distMatrixIn, int nOfRows, int nOfColumns) const {
    double *distMatrix, *distMatrixTemp, *distMatrixEnd, *columnEnd, value, minValue;
	bool *coveredColumns, *coveredRows, *starMatrix, *newStarMatrix, *primeMatrix;
	int nOfElements, minDim, row, col;

	/* initialization */
	*cost = 0;
	for (row = 0; row<nOfRows; row++)
		assignment[row] = -1;

	/* generate working copy of distance Matrix */
	/* check if all matrix elements are positive */
	nOfElements = nOfRows * nOfColumns;
	distMatrix = (double *)malloc(nOfElements * sizeof(double));
	distMatrixEnd = distMatrix + nOfElements;

	for (row = 0; row<nOfElements; row++)
	{
		value = distMatrixIn[row];
		if (value < 0)
			throw std::runtime_error("HungarianAlgorithm failed: All matrix elements have to be non-negative.");
		distMatrix[row] = value;
	}


	/* memory allocation */
	coveredColumns = (bool *)calloc(nOfColumns, sizeof(bool));
	coveredRows = (bool *)calloc(nOfRows, sizeof(bool));
	starMatrix = (bool *)calloc(nOfElements, sizeof(bool));
	primeMatrix = (bool *)calloc(nOfElements, sizeof(bool));
	newStarMatrix = (bool *)calloc(nOfElements, sizeof(bool)); /* used in step4 */

	/* preliminary steps */
	if (nOfRows <= nOfColumns)
	{
		minDim = nOfRows;

		for (row = 0; row<nOfRows; row++)
		{
			/* find the smallest element in the row */
			distMatrixTemp = distMatrix + row;
			minValue = *distMatrixTemp;
			distMatrixTemp += nOfRows;
			while (distMatrixTemp < distMatrixEnd)
			{
				value = *distMatrixTemp;
				if (value < minValue)
					minValue = value;
				distMatrixTemp += nOfRows;
			}

			/* subtract the smallest element from each element of the row */
			distMatrixTemp = distMatrix + row;
			while (distMatrixTemp < distMatrixEnd)
			{
				*distMatrixTemp -= minValue;
				distMatrixTemp += nOfRows;
			}
		}

		/* Steps 1 and 2a */
		for (row = 0; row<nOfRows; row++)
			for (col = 0; col<nOfColumns; col++)
				if (fabs(distMatrix[row + nOfRows*col]) < DBL_EPSILON)
					if (!coveredColumns[col])
					{
						starMatrix[row + nOfRows*col] = true;
						coveredColumns[col] = true;
						break;
					}
	}
	else /* if(nOfRows > nOfColumns) */
	{
		minDim = nOfColumns;

		for (col = 0; col<nOfColumns; col++)
		{
			/* find the smallest element in the column */
			distMatrixTemp = distMatrix + nOfRows*col;
			columnEnd = distMatrixTemp + nOfRows;

			minValue = *distMatrixTemp++;
			while (distMatrixTemp < columnEnd)
			{
				value = *distMatrixTemp++;
				if (value < minValue)
					minValue = value;
			}

			/* subtract the smallest element from each element of the column */
			distMatrixTemp = distMatrix + nOfRows*col;
			while (distMatrixTemp < columnEnd)
				*distMatrixTemp++ -= minValue;
		}

		/* Steps 1 and 2a */
		for (col = 0; col<nOfColumns; col++)
			for (row = 0; row<nOfRows; row++)
				if (fabs(distMatrix[row + nOfRows*col]) < DBL_EPSILON)
					if (!coveredRows[row])
					{
						starMatrix[row + nOfRows*col] = true;
						coveredColumns[col] = true;
						coveredRows[row] = true;
						break;
					}
		for (row = 0; row<nOfRows; row++)
			coveredRows[row] = false;

	}

	/* move to step 2b */
	step2b(assignment, distMatrix, starMatrix, newStarMatrix, primeMatrix, coveredColumns, coveredRows, nOfRows, nOfColumns, minDim);

	/* compute cost and remove invalid assignments */
	computeassignmentcost(assignment, cost, distMatrixIn, nOfRows);

	/* free allocated memory */
	free(distMatrix);
	free(coveredColumns);
	free(coveredRows);
	free(starMatrix);
	free(primeMatrix);
	free(newStarMatrix);

}

void HungarianAlgorithm::buildassignmentvector(int *assignment, bool *starMatrix, int nOfRows, int nOfColumns) const {
	int row, col;

	for (row = 0; row<nOfRows; row++)
		for (col = 0; col<nOfColumns; col++)
			if (starMatrix[row + nOfRows*col])
			{
#ifdef ONE_INDEXING
				assignment[row] = col + 1; /* MATLAB-Indexing */
#else
				assignment[row] = col;
#endif
				break;
			}
}

void HungarianAlgorithm::computeassignmentcost(int *assignment, double *cost, double *distMatrix, int nOfRows) const {
	int row, col;

	for (row = 0; row<nOfRows; row++)
	{
		col = assignment[row];
		if (col >= 0)
			*cost += distMatrix[row + nOfRows*col];
	}
}

void HungarianAlgorithm::step2a(int *assignment, double *distMatrix, bool *starMatrix, bool *newStarMatrix, bool *primeMatrix, bool *coveredColumns, bool *coveredRows, int nOfRows, int nOfColumns, int minDim) const {
	bool *starMatrixTemp, *columnEnd;
	int col;

	/* cover every column containing a starred zero */
	for (col = 0; col<nOfColumns; col++)
	{
		starMatrixTemp = starMatrix + nOfRows*col;
		columnEnd = starMatrixTemp + nOfRows;
		while (starMatrixTemp < columnEnd){
			if (*starMatrixTemp++)
			{
				coveredColumns[col] = true;
				break;
			}
		}
	}

	/* move to step 3 */
	step2b(assignment, distMatrix, starMatrix, newStarMatrix, primeMatrix, coveredColumns, coveredRows, nOfRows, nOfColumns, minDim);
}

void HungarianAlgorithm::step2b(int *assignment, double *distMatrix, bool *starMatrix, bool *newStarMatrix, bool *primeMatrix, bool *coveredColumns, bool *coveredRows, int nOfRows, int nOfColumns, int minDim) const {
	int col, nOfCoveredColumns;

	/* count covered columns */
	nOfCoveredColumns = 0;
	for (col = 0; col<nOfColumns; col++)
		if (coveredColumns[col])
			nOfCoveredColumns++;

	if (nOfCoveredColumns == minDim)
	{
		/* algorithm finished */
		buildassignmentvector(assignment, starMatrix, nOfRows, nOfColumns);
	}
	else
	{
		/* move to step 3 */
		step3(assignment, distMatrix, starMatrix, newStarMatrix, primeMatrix, coveredColumns, coveredRows, nOfRows, nOfColumns, minDim);
	}
}

void HungarianAlgorithm::step3(int *assignment, double *distMatrix, bool *starMatrix, bool *newStarMatrix, bool *primeMatrix, bool *coveredColumns, bool *coveredRows, int nOfRows, int nOfColumns, int minDim) const {
	bool zerosFound;
	int row, col, starCol;

	zerosFound = true;
	while (zerosFound)
	{
		zerosFound = false;
		for (col = 0; col<nOfColumns; col++)
			if (!coveredColumns[col])
				for (row = 0; row<nOfRows; row++)
					if ((!coveredRows[row]) && (fabs(distMatrix[row + nOfRows*col]) < DBL_EPSILON))
					{
						/* prime zero */
						primeMatrix[row + nOfRows*col] = true;

						/* find starred zero in current row */
						for (starCol = 0; starCol<nOfColumns; starCol++)
							if (starMatrix[row + nOfRows*starCol])
								break;

						if (starCol == nOfColumns) /* no starred zero found */
						{
							/* move to step 4 */
							step4(assignment, distMatrix, starMatrix, newStarMatrix, primeMatrix, coveredColumns, coveredRows, nOfRows, nOfColumns, minDim, row, col);
							return;
						}
						else
						{
							coveredRows[row] = true;
							coveredColumns[starCol] = false;
							zerosFound = true;
							break;
						}
					}
	}

	/* move to step 5 */
	step5(assignment, distMatrix, starMatrix, newStarMatrix, primeMatrix, coveredColumns, coveredRows, nOfRows, nOfColumns, minDim);
}

void HungarianAlgorithm::step4(int *assignment, double *distMatrix, bool *starMatrix, bool *newStarMatrix, bool *primeMatrix, bool *coveredColumns, bool *coveredRows, int nOfRows, int nOfColumns, int minDim, int row, int col) const {
	int n, starRow, starCol, primeRow, primeCol;
	int nOfElements = nOfRows*nOfColumns;

	/* generate temporary copy of starMatrix */
	for (n = 0; n<nOfElements; n++)
		newStarMatrix[n] = starMatrix[n];

	/* star current zero */
	newStarMatrix[row + nOfRows*col] = true;

	/* find starred zero in current column */
	starCol = col;
	for (starRow = 0; starRow<nOfRows; starRow++)
		if (starMatrix[starRow + nOfRows*starCol])
			break;

	while (starRow<nOfRows)
	{
		/* unstar the starred zero */
		newStarMatrix[starRow + nOfRows*starCol] = false;

		/* find primed zero in current row */
		primeRow = starRow;
		for (primeCol = 0; primeCol<nOfColumns; primeCol++)
			if (primeMatrix[primeRow + nOfRows*primeCol])
				break;

		/* star the primed zero */
		newStarMatrix[primeRow + nOfRows*primeCol] = true;

		/* find starred zero in current column */
		starCol = primeCol;
		for (starRow = 0; starRow<nOfRows; starRow++)
			if (starMatrix[starRow + nOfRows*starCol])
				break;
	}

	/* use temporary copy as new starMatrix */
	/* delete all primes, uncover all rows */
	for (n = 0; n<nOfElements; n++)
	{
		primeMatrix[n] = false;
		starMatrix[n] = newStarMatrix[n];
	}
	for (n = 0; n<nOfRows; n++)
		coveredRows[n] = false;

	/* move to step 2a */
	step2a(assignment, distMatrix, starMatrix, newStarMatrix, primeMatrix, coveredColumns, coveredRows, nOfRows, nOfColumns, minDim);
}

void HungarianAlgorithm::step5(int *assignment, double *distMatrix, bool *starMatrix, bool *newStarMatrix, bool *primeMatrix, bool *coveredColumns, bool *coveredRows, int nOfRows, int nOfColumns, int minDim) const {
	double h, value;
	int row, col;

	/* find smallest uncovered element h */
	h = DBL_MAX;
	for (row = 0; row<nOfRows; row++)
		if (!coveredRows[row])
			for (col = 0; col<nOfColumns; col++)
				if (!coveredColumns[col])
				{
					value = distMatrix[row + nOfRows*col];
					if (value < h)
						h = value;
				}

	/* add h to each covered row */
	for (row = 0; row<nOfRows; row++)
		if (coveredRows[row])
			for (col = 0; col<nOfColumns; col++)
				distMatrix[row + nOfRows*col] += h;

	/* subtract h from each uncovered column */
	for (col = 0; col<nOfColumns; col++)
		if (!coveredColumns[col])
			for (row = 0; row<nOfRows; row++)
				distMatrix[row + nOfRows*col] -= h;

	/* move to step 3 */
	step3(assignment, distMatrix, starMatrix, newStarMatrix, primeMatrix, coveredColumns, coveredRows, nOfRows, nOfColumns, minDim);
}

} //internal
} //dyno
````

## File: common/Camera.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/common/Camera.hpp"

#include <glog/logging.h>

namespace dyno {

Camera::Camera(const CameraParams& camera_params) : camera_params_(camera_params)
{
    camera_impl_ = std::make_unique<CameraImpl>(
        gtsam::Pose3::Identity(), //this should be the body pose cam from the calibration but currently we want everything in camera frame,
        camera_params_.constructGtsamCalibration<CalibrationType>()
    );

    CHECK(camera_impl_);
}

void Camera::project(const Landmark& lmk, Keypoint* kpt) const {
  CHECK_NOTNULL(kpt);
  // I believe this project call in gtsam is quite inefficient as it goes
  // through a cascade of calls... Only useful if you want gradients I guess...
  *kpt = camera_impl_->project2(lmk);
}

void Camera::project(const Landmarks& lmks, Keypoints* kpts) const {
  CHECK_NOTNULL(kpts)->clear();
  const auto& n_lmks = lmks.size();
  kpts->resize(n_lmks);
  // Can be greatly optimized with matrix mult or vectorization
  for (size_t i = 0u; i < n_lmks; i++) {
    project(lmks[i], &(*kpts)[i]);
  }
}

bool Camera::isKeypointContained(const Keypoint& kpt, Depth depth) const {
    return isKeypointContained(kpt) && depth > 0.0;
}

bool Camera::isKeypointContained(const Keypoint& kpt) const {
    return kpt(0) >= 0.0 && kpt(0) < camera_params_.ImageWidth() &&
           kpt(1) >= 0.0 && kpt(1) < camera_params_.ImageHeight();
}

void Camera::backProject(const Keypoints& kps, const Depths& depths, Landmarks* lmks) const {
    CHECK_NOTNULL(lmks)->clear();
  lmks->reserve(kps.size());
  CHECK_EQ(kps.size(), depths.size());
  CHECK(camera_impl_);
  for (size_t i = 0u; i < kps.size(); i++) {
    Landmark lmk;
    backProject(kps[i], depths[i], &lmk);
    lmks->push_back(lmk);
  }
}
void Camera::backProject(const Keypoint& kp, const Depth& depth, Landmark* lmk) const {
    CHECK(lmk);
    backProject(kp, depth, lmk, gtsam::Pose3::Identity());
}

//TODO: no tests
void Camera::backProject(const Keypoint& kp, const Depth& depth, Landmark* lmk, const gtsam::Pose3& X_world) const {
  CHECK(lmk);
  *lmk = X_world * camera_impl_->backproject(kp, depth);
}

void Camera::backProjectFromZ(const Keypoint& kp, const double Z, Landmark* lmk, gtsam::OptionalJacobian<3, 3> Dp) const {
  CHECK(lmk);
  backProjectFromZ(kp, Z, lmk, gtsam::Pose3::Identity(), Dp);
}

void Camera::backProjectFromZ(const Keypoint& kp, const double Z, Landmark* lmk, const gtsam::Pose3& X_world, gtsam::OptionalJacobian<3, 3> Dp) const {
  CHECK(lmk);
  const auto calibration = camera_impl_->calibration();
  //Convert (distorted) image coordinates uv to intrinsic coordinates xy
  Eigen::Matrix<double, 2, decltype(calibration)::dimension> Dcal; //unused
  gtsam::Matrix22 Dcal_point;
  gtsam::Point2 pc = calibration.calibrate(kp, Dcal, Dcal_point);

  // projection equation x = f(X/Z). The f is used to normalize the image cooridinates and since we already have
  // normalized cordinates xy, we can omit the f
  const double X = pc(0) * Z;
  const double Y = pc(1) * Z;

  const gtsam::Point3 P_camera(X, Y, Z);

  //jacobian of the transforFrom function w.r.t P_camera
  gtsam::Matrix33 H_point;
  *lmk = X_world.transformFrom(P_camera, boost::none, H_point);

  if(Dp) {
    //TODO: test!!!
    //cal1 should be top row of Dcal_point which determines how the pc(0) changes with input u, v
    const double dcal1_du = Dcal_point(0,0);
    const double dcal1_dv = Dcal_point(0,1);

    //cal1 should be top row of Dcal_point which determines how the pc(1) changes with input u, v
    const double dcal2_du = Dcal_point(1, 0);
    const double dcal2_dv = Dcal_point(1, 1);

    gtsam::Matrix33 J;
    //top row is how X changes with u, v and Z
    J(0, 0) = dcal1_du * Z;
    J(0, 1) = dcal1_dv * Z;
    J(0, 2) = pc(0);

    //how Y changes with u, v and Z
    J(1, 0) = dcal2_du * Z;
    J(1, 1) = dcal2_dv * Z;
    J(1, 2) = pc(1);

    //how Z changes with Z
    J(2, 0) = 0;
    J(2, 1) = 0;
    J(2, 2) = 1;

    //J is computed as the jacobian without the affect of X_world
    // f = X_world * C(u, v, Z)
    //   = X_world * P_camera
    //df/dfuvz = df/dC * dC/duvz
    //         = df/dC * J
    //since C -> C(u, v, z) = point in R^3, df/dC is the Jacobian of X_world * P_camera -> Rotation component of X_world, or H_point
    *Dp = H_point * J;
  }

}


Landmark Camera::cameraToWorldConvention(const Landmark& lmk) {
  //expecting camera convention to be z forward, x, right and y down
  return Landmark(lmk(2), lmk(0), -lmk(2));
}

bool Camera::isLandmarkContained(const Landmark& lmk, Keypoint* keypoint) const {
    Keypoint kp;

    try {
      project(lmk, &kp);
    }
    catch(const gtsam::CheiralityException&) {
      //if CheiralityException then point is behind camera
      return false;
    }

    const bool result = isKeypointContained(kp);
    if(keypoint) *keypoint = kp;
    return result;
}







} //dyno
````

## File: common/CameraParams.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include "dynosam/common/CameraParams.hpp"

#include <glog/logging.h>

#include <memory>

#include "dynosam/common/Types.hpp"
#include "dynosam/utils/GtsamUtils.hpp"   //for cv equals
#include "dynosam/utils/Numerical.hpp"    //for equals
#include "dynosam/utils/OpenCVUtils.hpp"  //for cv equals

namespace dyno {

template <>
std::string to_string(const DistortionModel& distortion) {
  switch (distortion) {
    case DistortionModel::NONE:
      return "None";
      break;
    case DistortionModel::RADTAN:
      return "Radtan";
      break;
    case DistortionModel::EQUIDISTANT:
      return "Equidistant";
      break;
    case DistortionModel::FISH_EYE:
      return "Fish-eye";
      break;
    default:
      return "Unknown distortion model";
      break;
  }
}

void declare_config(CameraParams& config) {
  using namespace config;
  name("Camera Params");

  cv::Size image_size;
  field(image_size, "resolution");

  // camera to robot pose
  std::vector<double> vector_pose;
  field(vector_pose, "T_BS");
  checkCondition(
      vector_pose.size() == 16u,
      "param 'T_BS' must be a 16 length vector in homogenous matrix form");
  gtsam::Pose3 T_robot_camera = utils::poseVectorToGtsamPose3(vector_pose);

  std::vector<double> intrinsics_v;
  field(intrinsics_v, "intrinsics");
  checkCondition(intrinsics_v.size() == 4,
                 "param 'intrinsics' must be a 4 length vector.");
  CameraParams::IntrinsicsCoeffs intrinsics;
  intrinsics.resize(4u);
  // Move elements from one to the other.
  std::copy_n(std::make_move_iterator(intrinsics_v.begin()), intrinsics.size(),
              intrinsics.begin());

  CameraParams::DistortionCoeffs distortion;
  field(distortion, "distortion_coefficients");

  std::string distortion_model, camera_model;
  field(distortion_model, "distortion_model");
  field(camera_model, "camera_model");

  DistortionModel model =
      CameraParams::stringToDistortion(distortion_model, camera_model);
  if (model == DistortionModel::NONE) {
    throw InvalidCameraCalibration(
        "Invalid distortion model/camera model combination when constructing "
        "camera params from yaml file - "
        " distortion model: " +
        distortion_model + ", camera model: " + camera_model);
  }

  config =
      CameraParams(intrinsics, distortion, image_size, model, T_robot_camera);
}

CameraParams::CameraParams(const IntrinsicsCoeffs& intrinsics,
                           const DistortionCoeffs& distortion,
                           const cv::Size& image_size,
                           const std::string& distortion_model,
                           const gtsam::Pose3& T_robot_camera)
    : CameraParams(
          intrinsics, distortion, image_size,
          CameraParams::stringToDistortion(distortion_model, "pinhole"),
          T_robot_camera) {}

CameraParams::CameraParams(const IntrinsicsCoeffs& intrinsics,
                           const DistortionCoeffs& distortion,
                           const cv::Size& image_size,
                           const DistortionModel& distortion_model,
                           const gtsam::Pose3& T_robot_camera)
    : intrinsics_(intrinsics),
      distortion_coeff_(distortion),
      image_size_(image_size),
      distortion_model_(distortion_model),
      T_robot_camera_(T_robot_camera) {
  CHECK_EQ(intrinsics_.size(), 4u)
      << "Intrinsics must be of length 4 - [fx fy cu cv]";
  CHECK_GT(distortion_coeff_.size(), 0u);

  CameraParams::convertDistortionVectorToMatrix(distortion_coeff_, &D_);
  CameraParams::convertIntrinsicsVectorToMatrix(intrinsics_, &K_);

  cv::cv2eigen(K_, K_eigen_);
  K_.copyTo(P_);
}

void CameraParams::convertDistortionVectorToMatrix(
    const DistortionCoeffs& distortion_coeffs, cv::Mat* distortion_coeffs_mat) {
  *distortion_coeffs_mat = cv::Mat::zeros(1, distortion_coeffs.size(), CV_64F);
  for (int k = 0; k < distortion_coeffs_mat->cols; k++) {
    distortion_coeffs_mat->at<double>(0, k) = distortion_coeffs[k];
  }
}

void CameraParams::convertIntrinsicsVectorToMatrix(
    const IntrinsicsCoeffs& intrinsics, cv::Mat* camera_matrix) {
  *camera_matrix = cv::Mat::eye(3, 3, CV_64F);
  camera_matrix->at<double>(0, 0) = intrinsics[0];
  camera_matrix->at<double>(1, 1) = intrinsics[1];
  camera_matrix->at<double>(0, 2) = intrinsics[2];
  camera_matrix->at<double>(1, 2) = intrinsics[3];
}

void CameraParams::convertKMatrixToIntrinsicsCoeffs(
    const cv::Mat& K, IntrinsicsCoeffs& intrinsics) {
  CHECK_EQ(K.type(), CV_64F) << "Cannot convert: Camera Matrix (K) is expected "
                                "to be of type CV_64F but is "
                             << utils::cvTypeToString(K.type());
  CHECK(K.rows == 3 && K.cols) << "Cannot convert: Camera Matrix (K) is "
                                  "expected to be of size (3 x 3) but is ["
                               << K.rows << " x " << K.cols << "]";
  ;

  intrinsics.resize(4);
  intrinsics[0] = K.at<double>(0, 0);
  intrinsics[1] = K.at<double>(1, 1);
  intrinsics[2] = K.at<double>(0, 2);
  intrinsics[3] = K.at<double>(1, 2);
}

DistortionModel CameraParams::stringToDistortion(
    const std::string& distortion_model, const std::string& camera_model) {
  std::string lower_case_distortion_model = distortion_model;
  std::string lower_case_camera_model = camera_model;

  std::transform(lower_case_distortion_model.begin(),
                 lower_case_distortion_model.end(),
                 lower_case_distortion_model.begin(), ::tolower);
  std::transform(lower_case_camera_model.begin(), lower_case_camera_model.end(),
                 lower_case_camera_model.begin(), ::tolower);

  if (lower_case_camera_model == "pinhole") {
    if (lower_case_distortion_model == "none") {
      return DistortionModel::NONE;
    } else if ((lower_case_distortion_model == "plumb_bob") ||
               (lower_case_distortion_model == "radial_tangential") ||
               (lower_case_distortion_model == "radtan")) {
      return DistortionModel::RADTAN;
    } else if (lower_case_distortion_model == "equidistant") {
      return DistortionModel::EQUIDISTANT;
    } else if (lower_case_distortion_model == "kannala_brandt") {
      return DistortionModel::FISH_EYE;
    } else {
      LOG(ERROR) << "Unrecognized distortion model for pinhole camera. Valid "
                    "pinhole distortion model options are 'none', 'radtan', "
                    "'equidistant', 'fish eye'.";
    }
  } else {
    LOG(ERROR)
        << "Unrecognized camera model. Valid camera models are 'pinhole'";
  }
  // Return no distortion model if invalid model is provided
  return DistortionModel::NONE;
}

bool CameraParams::distortionModelToString(const DistortionModel& model,
                                           std::string& distortion_model,
                                           std::string& camera_model) {
  switch (model) {
    case DistortionModel::NONE:
      return false;
    case DistortionModel::RADTAN:
      // in ROS only plumb_bob is defined so we use this
      distortion_model = "plumb_bob";
      camera_model = "pinhole";
      return true;
    case DistortionModel::EQUIDISTANT:
      distortion_model = "equidistant";
      camera_model = "pinhole";
      return true;
    case DistortionModel::FISH_EYE:
      distortion_model = "kannala_brandt";
      camera_model = "pinhole";
      return true;
    default:
      LOG(WARNING) << "Cannot convert distortion model to string: "
                   << to_string(model);
      return false;
  }
}

bool CameraParams::equals(const CameraParams& other, double tol) const {
  return dyno::equals_with_abs_tol(intrinsics_, other.intrinsics_, tol) &&
         dyno::equals_with_abs_tol(distortion_coeff_, other.distortion_coeff_,
                                   tol) &&
         (image_size_.width == other.image_size_.width &&
          image_size_.height == other.image_size_.height) &&
         distortion_model_ == other.distortion_model_ &&
         T_robot_camera_.equals(other.T_robot_camera_, tol) &&
         utils::compareCvMatsUpToTol(K_, other.K_, tol) &&
         utils::compareCvMatsUpToTol(
             D_, other.D_);  // NOTE: do not comapre with P matrix as currently
                             // not calculated
}

const std::string CameraParams::toString() const {
  std::stringstream out;
  out << "\nIntrinsics: \n- fx " << fx() << "\n- fy " << fy() << "\n- cu "
      << cu() << "\n- cv " << cv()
      << "\nimage_size: \n- width: " << ImageWidth()
      << "\n- height: " << ImageHeight() << "\n- K: " << K_ << '\n'
      << "- Distortion Model: " << to_string(distortion_model_) << '\n'
      << "- D: " << D_ << '\n'
      << "- P: " << P_ << '\n';

  return out.str();
}

template <>
gtsam::Cal3_S2 CameraParams::constructGtsamCalibration<gtsam::Cal3_S2>() const {
  static const auto requested_calibration_name =
      type_name<gtsam::Cal3_S2>();  // only used for debug so seems waste to
                                    // allocate everytime

  if (distortion_model_ != DistortionModel::RADTAN) {
    throw InvalidCameraCalibration(
        "Requested gtsam calibration was " + requested_calibration_name +
        " which is unsupported by this camera model: " +
        to_string(distortion_model_));
  }

  // if(distortion_coeff_.size() < 4u) {
  //   throw InvalidCameraCalibration("Distortion coefficients have size <4 for
  //   camera params with distortion model"
  //     + to_string(distortion_model_) + " and requested gtsam calibration of
  //     type " + requested_calibration_name);
  // }

  constexpr static double skew = 0.0;
  return gtsam::Cal3_S2(fx(), fy(), skew, cu(), cv());
}

template <>
gtsam::Cal3DS2 CameraParams::constructGtsamCalibration<gtsam::Cal3DS2>() const {
  static const auto requested_calibration_name =
      type_name<gtsam::Cal3DS2>();  // only used for debug so seems waste to
                                    // allocate everytime

  if (distortion_model_ != DistortionModel::RADTAN) {
    throw InvalidCameraCalibration(
        "Requested gtsam calibration was " + requested_calibration_name +
        " which is unsupported by this camera model: " +
        to_string(distortion_model_));
  }

  if (distortion_coeff_.size() < 4u) {
    throw InvalidCameraCalibration(
        "Distortion coefficients have size <4 for camera params with "
        "distortion model" +
        to_string(distortion_model_) +
        " and requested gtsam calibration of type " +
        requested_calibration_name);
  }

  constexpr static double skew = 0.0;
  return gtsam::Cal3DS2(fx(), fy(), skew, cu(), cv(), distortion_coeff_.at(0),
                        distortion_coeff_.at(1), distortion_coeff_.at(2),
                        distortion_coeff_.at(3));
}

template <>
gtsam::Cal3Fisheye CameraParams::constructGtsamCalibration<gtsam::Cal3Fisheye>()
    const {
  static const auto requested_calibration_name =
      type_name<gtsam::Cal3Fisheye>();  // only used for debug so seems waste to
                                        // allocate everytime

  // jesse: not sure if gtsam well supports equidistant models!
  if (distortion_model_ != DistortionModel::FISH_EYE &&
      distortion_model_ != DistortionModel::EQUIDISTANT) {
    throw InvalidCameraCalibration(
        "Requested gtsam calibration was " + requested_calibration_name +
        " which is unsupported by this camera model: " +
        to_string(distortion_model_));
  }

  if (distortion_coeff_.size() < 4u) {
    throw InvalidCameraCalibration(
        "Distortion coefficients have size <4 for camera params with "
        "distortion model" +
        to_string(distortion_model_) +
        " and requested gtsam calibration of type " +
        requested_calibration_name);
  }

  constexpr static double skew = 0.0;
  return gtsam::Cal3Fisheye(fx(), fy(), skew, cu(), cv(),
                            distortion_coeff_.at(0), distortion_coeff_.at(1),
                            distortion_coeff_.at(2), distortion_coeff_.at(3));
}

}  // namespace dyno
````

## File: common/DynamicObjects.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include "dynosam/common/DynamicObjects.hpp"

#include "dynosam/common/GroundTruthPacket.hpp"

namespace dyno {

gtsam::Vector3 calculateBodyMotion(const gtsam::Pose3& w_k_1_H_k,
                                   const gtsam::Pose3& w_L_k_1) {
  const gtsam::Point3& t_motion = w_k_1_H_k.translation();
  const gtsam::Rot3& R_motion = w_k_1_H_k.rotation();
  const gtsam::Point3& t_pose = w_L_k_1.translation();

  static const gtsam::Rot3 I = gtsam::Rot3::Identity();

  return t_motion - (gtsam::Rot3(I.matrix() - R_motion.matrix())) * t_pose;
}

void propogateObjectPoses(
    ObjectPoseMap& object_poses, const MotionEstimateMap& object_motions_k,
    const gtsam::Point3Vector& object_centroids_k_1,
    const gtsam::Point3Vector&
        object_centroids_k,  // TODO: dont actually need this one!!
    FrameId frame_id_k, std::optional<GroundTruthPacketMap> gt_packet_map,
    PropogatePoseResult* result) {
  CHECK_EQ(object_motions_k.size(), object_centroids_k_1.size());
  CHECK_EQ(object_centroids_k.size(), object_centroids_k_1.size());
  const FrameId frame_id_k_1 = frame_id_k - 1;

  // get centroid for object at k-1 using the gt pose if available, or the
  // centroid if not
  auto get_centroid = [=](ObjectId object_id, const gtsam::Point3& centroid_k_1,
                          gtsam::Pose3& pose_k_1) -> PropogateType {
    bool initalised_with_gt = false;
    // if gt packet exists for this frame, use that as the rotation
    if (gt_packet_map) {
      if (gt_packet_map->exists(frame_id_k_1)) {
        const GroundTruthInputPacket& gt_packet_k_1 =
            gt_packet_map->at(frame_id_k_1);

        ObjectPoseGT object_pose_gt_k_1;
        if (gt_packet_k_1.getObject(object_id, object_pose_gt_k_1)) {
          pose_k_1 = object_pose_gt_k_1.L_world_;
          initalised_with_gt = true;
        }
      }
    }

    if (!initalised_with_gt) {
      // could not init with gt, use identity rotation and centroid
      pose_k_1 = gtsam::Pose3(gtsam::Rot3::Identity(), centroid_k_1);

      return PropogateType::InitCentroid;
    } else {
      return PropogateType::InitGT;
    }
  };

  size_t i = 0;  // used to index the object centroid vectors
  for (const auto& [object_id, motion] : object_motions_k) {
    const auto centroid_k = object_centroids_k.at(i);
    const auto centroid_k_1 = object_centroids_k_1.at(i);
    const gtsam::Pose3 prev_H_world_curr = motion;
    // new object - so we need to add at k-1 and k
    if (!object_poses.exists(object_id)) {
      gtsam::Pose3 pose_k_1;
      auto propgate_result = get_centroid(object_id, centroid_k_1, pose_k_1);
      if (result) result->insert22(object_id, frame_id_k_1, propgate_result);

      // bool initalised_with_gt = false;
      // gtsam::Pose3 pose_k_1;

      // //if gt packet exists for this frame, use that as the rotation
      // if(gt_packet_map) {
      //     if(gt_packet_map->exists(frame_id_k_1)) {
      //         const GroundTruthInputPacket& gt_packet_k_1 =
      //         gt_packet_map->at(frame_id_k_1);

      //         ObjectPoseGT object_pose_gt_k_1;
      //         if(gt_packet_k_1.getObject(object_id, object_pose_gt_k_1)) {
      //             pose_k_1 = object_pose_gt_k_1.L_world_;
      //             initalised_with_gt = true;
      //         }
      //     }
      // }

      // if(!initalised_with_gt) {
      //     //could not init with gt, use identity rotation and centroid
      //     pose_k_1 = gtsam::Pose3(gtsam::Rot3::Identity(), centroid_k_1);

      //     if(result) result->insert22(object_id, frame_id_k_1,
      //     PropogateType::InitCentroid);
      // }
      // else {
      //     if(result) result->insert22(object_id, frame_id_k_1,
      //     PropogateType::InitGT);
      // }

      object_poses.insert2(object_id, gtsam::FastMap<FrameId, gtsam::Pose3>{});
      object_poses.at(object_id).insert2(frame_id_k_1, pose_k_1);
    }

    auto& per_frame_poses = object_poses.at(object_id);
    // if we have a pose at the previous frame, simply apply motion
    if (per_frame_poses.exists(frame_id_k_1)) {
      const gtsam::Pose3& object_pose_k_1 = per_frame_poses.at(frame_id_k_1);
      // assuming in world
      gtsam::Pose3 object_pose_k = prev_H_world_curr * object_pose_k_1;
      per_frame_poses.insert2(frame_id_k, object_pose_k);

      // update result map
      if (result)
        result->insert22(object_id, frame_id_k, PropogateType::Propogate);
    } else {
      // no motion at the previous frame - if close, interpolate between last
      // pose and this pose no motion used
      const size_t min_diff_frames = 3;

      // last frame SHOULD be the largest frame (as we use a std::map with
      // std::less)
      auto last_record_itr = per_frame_poses.rbegin();
      const FrameId last_frame = last_record_itr->first;
      const gtsam::Pose3 last_recorded_pose = last_record_itr->second;

      // construct current pose using last poses rotation (I guess?)
      gtsam::Pose3 current_pose =
          gtsam::Pose3(last_recorded_pose.rotation(), object_centroids_k.at(i));

      CHECK_LT(last_frame, frame_id_k_1);
      if (frame_id_k - last_frame < min_diff_frames) {
        // apply interpolation
        // need to map [last_frame:frame_id_k] -> [0,1] for the interpolation
        // function with N values such that frame_id_k - last_frame + 1= N (to
        // be inclusive)
        const size_t N = frame_id_k - last_frame + 1;
        const double divisor = (double)(frame_id_k - last_frame);
        for (size_t j = 0; j < N; j++) {
          double t = (double)j / divisor;

          gtsam::Pose3 interpolated_pose = last_recorded_pose.slerp(
              t, current_pose, boost::none, boost::none);

          FrameId frame = last_frame + j;
          per_frame_poses.insert2(frame, interpolated_pose);

          // update result map
          if (result)
            result->insert22(object_id, frame, PropogateType::Interpolate);
        }

      } else {
        gtsam::Pose3 pose_k_1;
        // last frame too far away - reinitalise with centroid!
        VLOG(20) << "Frames too far away - current frame is " << frame_id_k
                 << " previous frame is " << last_frame << " for object "
                 << object_id;
        auto propogate_result = get_centroid(object_id, centroid_k_1, pose_k_1);
        object_poses.at(object_id).insert2(frame_id_k_1, pose_k_1);

        if (result) result->insert22(object_id, frame_id_k_1, propogate_result);

        gtsam::Pose3 object_pose_k = prev_H_world_curr * pose_k_1;
        per_frame_poses.insert2(frame_id_k, object_pose_k);

        // update result map
        if (result)
          result->insert22(object_id, frame_id_k, PropogateType::Propogate);
      }
    }
    i++;
  }
}

}  // namespace dyno
````

## File: common/GroundTruthPacket.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/common/GroundTruthPacket.hpp"
#include "dynosam/common/Exceptions.hpp"
#include "dynosam/utils/GtsamUtils.hpp"

#include "dynosam/utils/OpenCVUtils.hpp"
#include "dynosam/visualizer/ColourMap.hpp"

#include "dynosam/utils/JsonUtils.hpp" //for adl_seralizer for optional and gtsam-json
#include <opencv4/opencv2/opencv.hpp>
#include <glog/logging.h>




namespace dyno {

//copied from KittiSemanticToMotion.cc but not linked so might mismatch with semantic image?
bool isMoving(const gtsam::Pose3& prev_L_world, const gtsam::Pose3& curr_L_world, double tol_m = 0.4) {
    gtsam::Vector3 t_diff = curr_L_world.translation() - prev_L_world.translation();
    // L2 norm - ie. magnitude
    double t_error = t_diff.norm();
    return t_error > tol_m;
}

void ObjectPoseGT::setMotions(const ObjectPoseGT& previous_object_gt, const gtsam::Pose3& prev_X_world, const gtsam::Pose3& curr_X_world) {
    checkAndThrow(previous_object_gt.frame_id_ == frame_id_ - 1, "Previous object gt frame is not at k-1. Current frame = " + std::to_string(frame_id_) + " and previous frame =" + std::to_string(previous_object_gt.frame_id_));
    checkAndThrow(previous_object_gt.object_id_ == object_id_, "Previous object gt does not have the same object id");


    // for t-1
    const gtsam::Pose3 L_camera_prev = previous_object_gt.L_camera_;
    const gtsam::Pose3 L_world_prev = previous_object_gt.L_world_;
    //changed tol becuase seems to be numerical issues
    // CHECK(L_world_prev.equals(prev_X_world * L_camera_prev));
    CHECK(gtsam::assert_equal(L_world_prev, prev_X_world * L_camera_prev, 1e-5));
    // for t
    const gtsam::Pose3 L_camera_curr = L_camera_;
    const gtsam::Pose3 L_world_curr = L_world_;
    //changed tol becuase seems to be numerical issues
    // CHECK(L_world_curr.equals(curr_X_world * L_camera_curr));
    CHECK(gtsam::assert_equal(L_world_curr, curr_X_world * L_camera_curr, 1e-5));

    // H between t-1 and t in the object frame ground truth
    gtsam::Pose3 H_L_gt = L_world_prev.inverse() * L_world_curr;
    prev_H_current_L_ = H_L_gt;

    // H between t-1 and t in the camera frame at t-1
    gtsam::Pose3 H_X_gt = L_camera_prev * H_L_gt * L_camera_prev.inverse();
    prev_H_current_X_ = H_X_gt;

    // H between t-1 and t in the world frame
    // equ.5 (morris2023importance)
    gtsam::Pose3 H_W_gt = L_world_curr * L_world_prev.inverse();
    prev_H_current_world_ = H_W_gt;

    MotionInfo motion_info;
    motion_info.is_moving_ = isMoving(L_world_prev, L_world_curr);
    motion_info.has_stopped_ = false; //default

    if(previous_object_gt.motion_info_) {
        //moving in the previous frame but not in this frame
        if(previous_object_gt.motion_info_->is_moving_ && !motion_info.is_moving_) {
            motion_info.has_stopped_ = true;
        }
    }

    motion_info_ = motion_info;

}

void ObjectPoseGT::drawBoundingBox(cv::Mat& img) const {
    const cv::Scalar colour = Color::uniqueId(object_id_).bgra();
    const std::string label = "Object: " + std::to_string(object_id_);
    utils::drawLabeledBoundingBox(img, label, colour, bounding_box_);
}


ObjectPoseGT::operator std::string() const {
    std::stringstream ss;
    ss << "FrameId: " << frame_id_
       << " ObjectId: " << object_id_ << "\n";

    ss << " L world: " << L_world_ << "\n";
    ss << " L camera: " << L_camera_ << "\n";
    ss << " BB: " << to_string(bounding_box_) << "\n";

    ss << "Object dimensions: ";
    if(object_dimensions_) ss << *object_dimensions_ << "\n";
    else ss << "none\n";

    ss << "prev H current world: ";
    if(prev_H_current_world_) ss << *prev_H_current_world_ << "\n";
    else ss << " one\n";

    ss << "prev H current L: ";
    if(prev_H_current_L_) ss << *prev_H_current_L_ << "\n";
    else ss << "none\n";

    ss << "prev H current X: ";
    if(prev_H_current_X_) ss << *prev_H_current_X_ << "\n";
    else ss << "none\n";

     ss << "Motion info: ";
    if(motion_info_) {
        ss << " Is moving " << motion_info_->is_moving_
           << " Has stopped " << motion_info_->has_stopped_;
    }
    else {
        ss << "none";
    }
    return ss.str();
}


bool ObjectPoseGT::operator==(const ObjectPoseGT& other) const {
    return frame_id_ == other.frame_id_ &&
        object_id_ == other.object_id_ &&
        L_camera_.equals(other.L_camera_) &&
        L_world_.equals(other.L_world_) &&
        bounding_box_ == other.bounding_box_ &&
        utils::equateGtsamOptionalValues(object_dimensions_, other.object_dimensions_) &&
        utils::equateGtsamOptionalValues(prev_H_current_world_, other.prev_H_current_world_) &&
        utils::equateGtsamOptionalValues(prev_H_current_L_, other.prev_H_current_L_) &&
        utils::equateGtsamOptionalValues(prev_H_current_X_, other.prev_H_current_X_);
        //TODO: not sure how to compare the std::optional as the underlying objects dont have operator==
}


std::ostream& operator<<(std::ostream &os, const ObjectPoseGT& object_pose) {
    os << (std::string)object_pose;
    return os;
}

bool GroundTruthInputPacket::getObject(ObjectId object_id, ObjectPoseGT& object_pose_gt, int log_severity) const {
    auto it_this = std::find_if(object_poses_.begin(), object_poses_.end(),
                [=](const ObjectPoseGT& gt_object) { return gt_object.object_id_ == object_id; });
    if(it_this == object_poses_.end()) {
        std::stringstream ss;
        ss << "Could not find object at frame " << this->frame_id_ << " for object Id " << object_id << " and packet " << *this;

        if(log_severity == 1) {
            LOG(WARNING) << ss.str();
        }
        else if(log_severity > 1) {
            throw DynosamException(ss.str());
        }
        return false;
    }

    object_pose_gt = *it_this;
    return true;

}

ObjectIds GroundTruthInputPacket::getObjectIds() const {
    ObjectIds object_ids;
    for(const ObjectPoseGT& objects : object_poses_) {
        object_ids.push_back(objects.object_id_);
    }
    return object_ids;
}


bool GroundTruthInputPacket::findAssociatedObject(ObjectId label, GroundTruthInputPacket& other, ObjectPoseGT** obj, ObjectPoseGT** other_obj) {
    size_t obj_idx, other_obj_idx;

    if(findAssociatedObject(label, other, obj_idx, other_obj_idx)) {
        *obj = &object_poses_.at(obj_idx);
        *other_obj = &other.object_poses_.at(other_obj_idx);
        return true;
    }


    return false;
}

bool GroundTruthInputPacket::findAssociatedObject(ObjectId label, const GroundTruthInputPacket& other, const ObjectPoseGT** obj, const ObjectPoseGT** other_obj) const {
    size_t obj_idx, other_obj_idx;

    if(findAssociatedObject(label, other, obj_idx, other_obj_idx)) {
        *obj = &object_poses_.at(obj_idx);
        *other_obj = &other.object_poses_.at(other_obj_idx);
        return true;
    }

    return false;
}

bool GroundTruthInputPacket::findAssociatedObject(ObjectId label, const GroundTruthInputPacket& other, ObjectPoseGT** obj, const ObjectPoseGT** other_obj) {
    size_t obj_idx, other_obj_idx;

    if(findAssociatedObject(label, other, obj_idx, other_obj_idx)) {
        *obj = &object_poses_.at(obj_idx);
        *other_obj = &other.object_poses_.at(other_obj_idx);
        return true;
    }

    return false;
}

bool GroundTruthInputPacket::findAssociatedObject(ObjectId label, const GroundTruthInputPacket& other, size_t& obj_idx, size_t& other_obj_idx) const {
    auto it_this = std::find_if(object_poses_.begin(), object_poses_.end(),
                [=](const ObjectPoseGT& gt_object) { return gt_object.object_id_ == label; });
    if(it_this == object_poses_.end()) {
        return false;
    }

    auto it_other = std::find_if(other.object_poses_.cbegin(), other.object_poses_.cend(),
                [=](const ObjectPoseGT& gt_object) { return gt_object.object_id_ == label; });
    if(it_other == other.object_poses_.cend()) {
        return false;
    }

    obj_idx = std::distance(object_poses_.begin(), it_this);
    other_obj_idx = std::distance(other.object_poses_.cbegin(), it_other);
    return true;
}



size_t GroundTruthInputPacket::calculateAndSetMotions(const GroundTruthInputPacket& previous_object_packet, ObjectIds& motions_set) {
    motions_set.clear();
    if(previous_object_packet.frame_id_ != frame_id_ -1) { return false; }

    //iterate over all objects and try to find assications in previous packet
    for(ObjectPoseGT& object_pose_gt : object_poses_) {
        const ObjectId label = object_pose_gt.object_id_;

        ObjectPoseGT* current_object_pose_gt;
        const ObjectPoseGT* previous_object_pose_gt;
        if(findAssociatedObject(label, previous_object_packet, &current_object_pose_gt, &previous_object_pose_gt)) {
            CHECK_NOTNULL(current_object_pose_gt);
            CHECK_NOTNULL(previous_object_pose_gt);
            CHECK_EQ(*current_object_pose_gt, object_pose_gt);

            //set motions for current motion
            current_object_pose_gt->setMotions(*previous_object_pose_gt, previous_object_packet.X_world_, X_world_);
            motions_set.push_back(label);
        }

    }

    return motions_set.size();
}

size_t GroundTruthInputPacket::calculateAndSetMotions(const GroundTruthInputPacket& previous_object_packet) {
    ObjectIds motion_set;
    return calculateAndSetMotions(previous_object_packet, motion_set);
}

bool GroundTruthInputPacket::operator==(const GroundTruthInputPacket& other) const
{
    return frame_id_ == other.frame_id_ &&
        timestamp_ == other.timestamp_ &&
        gtsam::traits<gtsam::Pose3>::Equals(X_world_, other.X_world_) &&
        object_poses_ == other.object_poses_;
}

GroundTruthInputPacket::operator std::string() const {
    std::stringstream ss;
    ss << "FrameId: " << frame_id_
        << " objects: " << container_to_string(getObjectIds());
    return ss.str();
}


std::ostream& operator<<(std::ostream &os, const GroundTruthInputPacket& gt_packet) {
    os << (std::string)gt_packet;
    return os;
}


void to_json(json& j, const ObjectPoseGT::MotionInfo& motion_info) {
    j["is_moving"] = motion_info.is_moving_;
    j["has_stopped"] = motion_info.has_stopped_;
}

void from_json(const json& j, ObjectPoseGT::MotionInfo& motion_info) {
    motion_info.is_moving_ = j["is_moving"].template get<bool>();
    motion_info.has_stopped_ = j["has_stopped"].template get<bool>();
}

void to_json(json& j, const ObjectPoseGT& object_pose_gt) {
    j["frame_id"] = object_pose_gt.frame_id_;
    j["object_id"] = object_pose_gt.object_id_;

    // gtsam conversion provided by nlohmann::adl_serializer specalization in JsonUtils.hpp
    j["L_camera"] = object_pose_gt.L_camera_;
    j["L_world"] = object_pose_gt.L_world_;
    j["bounding_box"] = object_pose_gt.bounding_box_;

    // we have json conversions for both std::optional AND eigen
    j["object_dimensions"] = object_pose_gt.object_dimensions_;

    j["prev_H_current_world"] = object_pose_gt.prev_H_current_world_;
    j["prev_H_current_L"] = object_pose_gt.prev_H_current_L_;
    j["prev_H_current_X"] = object_pose_gt.prev_H_current_X_;
    j["motion_info"] = object_pose_gt.motion_info_;

}

void from_json(const json& j, ObjectPoseGT& object_pose_gt) {
    object_pose_gt.frame_id_ = j["frame_id"].template get<FrameId>();
    object_pose_gt.object_id_ = j["object_id"].template get<ObjectId>();

    object_pose_gt.L_camera_ = j["L_camera"].template get<gtsam::Pose3>();
    object_pose_gt.L_world_ = j["L_world"].template get<gtsam::Pose3>();
    object_pose_gt.bounding_box_ = j["bounding_box"].template get<cv::Rect>();

    object_pose_gt.object_dimensions_ = j["object_dimensions"].template get<std::optional<gtsam::Vector3>>();

    object_pose_gt.prev_H_current_world_ = j["prev_H_current_world"].template get<std::optional<gtsam::Pose3>>();
    object_pose_gt.prev_H_current_L_ = j["prev_H_current_L"].template get<std::optional<gtsam::Pose3>>();
    object_pose_gt.prev_H_current_X_ = j["prev_H_current_X"].template get<std::optional<gtsam::Pose3>>();
    object_pose_gt.motion_info_ = j["motion_info"].template get<std::optional<ObjectPoseGT::MotionInfo>>();

}

void to_json(json& j, const GroundTruthInputPacket& gt_packet) {
    j["timestamp"] = gt_packet.timestamp_;
    j["frame_id"] = gt_packet.frame_id_;
    j["X_world"] = gt_packet.X_world_;
    j["objects"] = gt_packet.object_poses_;
}

void from_json(const json& j, GroundTruthInputPacket& gt_packet) {
    gt_packet.timestamp_ = j["timestamp"].template get<Timestamp>();
    gt_packet.frame_id_ = j["frame_id"].template get<FrameId>();
    gt_packet.X_world_ = j["X_world"].template get<gtsam::Pose3>();
    gt_packet.object_poses_ = j["objects"].template get<std::vector<ObjectPoseGT>>();
}

void to_json(json& j, const GroundTruthPacketMap& gt_packet_map) {
    //have to do manual conversion to stl map so the json library can do automagic conversion
    //lots of copying
    std::map<FrameId, GroundTruthInputPacket> stl_map = gt_packet_map;
    j["ground_truths"] = stl_map;
}

void from_json(const json& j, GroundTruthPacketMap& gt_packet_map){
    std::map<FrameId, GroundTruthInputPacket> stl_map =  j["ground_truths"].template get<std::map<FrameId, GroundTruthInputPacket>>();
    //have to do manual conversion to stl map so the json library can do automagic conversion
    //lots of copying
    gt_packet_map = GroundTruthPacketMap(stl_map.begin(), stl_map.end());
}


} // dyno
````

## File: common/ImageContainer.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/common/ImageContainer.hpp"
#include "dynosam/utils/OpenCVUtils.hpp"
#include "dynosam/frontend/vision/UndistortRectifier.hpp" //annoything this has to be here - better make (undisort iamges) into a free fucntion later!!

#include <opencv4/opencv2/opencv.hpp>
#include <exception>

namespace dyno {


ImageContainer::Ptr ImageContainer::Create(
        const Timestamp timestamp,
        const FrameId frame_id,
        const ImageWrapper<ImageType::RGBMono>& img,
        const ImageWrapper<ImageType::Depth>& depth,
        const ImageWrapper<ImageType::OpticalFlow>& optical_flow,
        const ImageWrapper<ImageType::SemanticMask>& semantic_mask,
        const ImageWrapper<ImageType::MotionMask>& motion_mask,
        const ImageWrapper<ImageType::ClassSegmentation>& class_segmentation)
{
    std::shared_ptr<ImageContainer> container(
        new ImageContainer(
            timestamp,
            frame_id,
            img,
            depth,
            optical_flow,
            semantic_mask,
            motion_mask,
            class_segmentation)
        );
    container->validateSetup();
    return container;
}


ImageContainer::Ptr ImageContainer::RectifyImages(ImageContainer::Ptr images, const UndistorterRectifier& undistorter) {
    //do deep copy (the underlying ImageWrapper class has a deep copy in its copy constructor so
    //the image data is cloned)
    ImageContainer::Ptr undistorted_images = std::make_shared<ImageContainer>(*images);

    static constexpr size_t N = ImageContainer::N;
    for (size_t i = 0; i < N; i++) {
        internal::select_apply<N>(i, [&](auto I){
            using ImageType = typename ImageContainer::ImageTypeStruct<I>;

            //get reference to image and modify in place
            //TODO: bug!!?
            cv::Mat& distorted_image = undistorted_images->template get<ImageType>();
            undistorter.undistortRectifyImage(distorted_image, distorted_image);

        });
    }

    return undistorted_images;
}


ImageContainer::ImageContainer(
        const Timestamp timestamp,
        const FrameId frame_id,
        const ImageWrapper<ImageType::RGBMono>& img,
        const ImageWrapper<ImageType::Depth>& depth,
        const ImageWrapper<ImageType::OpticalFlow>& optical_flow,
        const ImageWrapper<ImageType::SemanticMask>& semantic_mask,
        const ImageWrapper<ImageType::MotionMask>& motion_mask,
        const ImageWrapper<ImageType::ClassSegmentation>& class_segmentation)
    :  Base(img, depth, optical_flow, semantic_mask, motion_mask, class_segmentation),
       timestamp_(timestamp),
       frame_id_(frame_id)
    { }


std::string ImageContainer::toString() const {
    std::stringstream ss;
    ss << "Timestamp: " << timestamp_ << "\n";
    ss << "Frame Id: " << frame_id_ << "\n";

    const std::string image_config = isImageRGB() ? "RGB" : "Grey";
    ss << "Configuration: " << image_config
        << " Depth (" << hasDepth() << ") Semantic Mask (" << hasSemanticMask()
        << ") Motion Mask (" << hasMotionMask()
        << ") Class Segmentation (" << hasClassSegmentation() << ")" << "\n";

    return ss.str();
}


ImageContainer::Ptr ImageContainer::Create(
        const Timestamp timestamp,
        const FrameId frame_id,
        const ImageWrapper<ImageType::RGBMono>& img,
        const ImageWrapper<ImageType::Depth>& depth,
        const ImageWrapper<ImageType::OpticalFlow>& optical_flow,
        const ImageWrapper<ImageType::SemanticMask>& semantic_mask) {

    return Create(timestamp, frame_id, img, depth, optical_flow, semantic_mask, ImageWrapper<ImageType::MotionMask>(), ImageWrapper<ImageType::ClassSegmentation>());

}

ImageContainer::Ptr ImageContainer::Create(
        const Timestamp timestamp,
        const FrameId frame_id,
        const ImageWrapper<ImageType::RGBMono>& img,
        const ImageWrapper<ImageType::Depth>& depth,
        const ImageWrapper<ImageType::OpticalFlow>& optical_flow,
        const ImageWrapper<ImageType::MotionMask>& motion_mask)
{
    return Create(timestamp, frame_id, img, depth, optical_flow, ImageWrapper<ImageType::SemanticMask>(), motion_mask, ImageWrapper<ImageType::ClassSegmentation>());

}

ImageContainer::Ptr ImageContainer::Create(
        const Timestamp timestamp,
        const FrameId frame_id,
        const ImageWrapper<ImageType::RGBMono>& img,
        const ImageWrapper<ImageType::Depth>& depth,
        const ImageWrapper<ImageType::OpticalFlow>& optical_flow,
        const ImageWrapper<ImageType::MotionMask>& motion_mask,
        const ImageWrapper<ImageType::ClassSegmentation>& class_segmentation) {

    return Create(timestamp, frame_id, img, depth, optical_flow, ImageWrapper<ImageType::SemanticMask>(), motion_mask, class_segmentation);
}



void ImageContainer::validateSetup() const {
    //check image sizes are the same
    Base::validateSetup();

    //TODO: shoudl eventually change to exception
    CHECK(!getImage().empty()) << "RGBMono image must not be empty!";
    CHECK(!getOpticalFlow().empty()) << "OPticalFlow image must not be empty!";

    //must have at least one of semantic mask or motion mask
    CHECK(!getSemanticMask().empty() || !getMotionMask().empty()) << "At least one of the masks (semantic or motion) must be valid. Both are empty";

    //should only provide one mask (for clarity - which one to use if both given?)
    if(hasSemanticMask()) {
        CHECK(!hasMotionMask()) << "Semantic mask has been provided, so motion mask should be empty!";
    }

    if(hasMotionMask()) {
        CHECK(!hasSemanticMask()) << "Motion mask has been provided, so semantic mask should be empty!";
    }
}

} //dyno
````

## File: common/ImageTypes.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include "dynosam/common/ImageTypes.hpp"

#include <opencv4/opencv2/opencv.hpp>

#include "dynosam/utils/OpenCVUtils.hpp"

namespace dyno {

void validateMask(const cv::Mat& input, const std::string& name) {
  // we guanrantee with a static assert that the SemanticMask and MotionMask
  // types are the same
  const static std::string expected_type =
      utils::cvTypeToString(ImageType::MotionMask::OpenCVType);
  if (input.type() != ImageType::MotionMask::OpenCVType) {
    throw InvalidImageTypeException(name + " image was not " + expected_type +
                                    ". Input image type was " +
                                    utils::cvTypeToString(input.type()));
  }
}

void validateRGBMono(const cv::Mat& input) {
  if (input.type() != CV_8UC1 && input.type() != CV_8UC3 &&
      input.type() != CV_8UC4) {
    throw InvalidImageTypeException(
        "RGBMono image was not CV_8UC1, CV_8UC3 or CV_8UC4. Input image type "
        "was " +
        utils::cvTypeToString(input.type()));
  }
}

void validateSingleImage(const cv::Mat& input, int expected_type,
                         const std::string& name) {
  if (input.type() != expected_type) {
    throw InvalidImageTypeException(
        name + " image was not " + utils::cvTypeToString(expected_type) +
        " - Input image type was " + utils::cvTypeToString(input.type()));
  }
}

cv::Mat RGBMonoToRGB(const cv::Mat& mat) {
  const auto channels = mat.channels();
  if (channels == 3) {
    return mat;
  } else if (channels == 4) {
    cv::Mat rgb;
    cv::cvtColor(mat, rgb, cv::COLOR_RGBA2RGB);
    return rgb;
  } else if (channels == 1) {
    // grey scale but we want rgb so that we can draw colours on it or whatever
    cv::Mat rgb;
    cv::cvtColor(mat, rgb, cv::COLOR_GRAY2RGB);
    return rgb;
  } else {
    return mat;
  }
}

cv::Mat RGBMonoToMono(const cv::Mat& mat) {
  const auto channels = mat.channels();
  if (channels == 1) {
    return mat;
  }
  if (channels == 3) {
    cv::Mat mono;
    cv::cvtColor(mat, mono, cv::COLOR_RGB2GRAY);
    return mono;
  } else if (channels == 4) {
    cv::Mat mono;
    cv::cvtColor(mat, mono, cv::COLOR_RGBA2GRAY);
    return mono;
  } else {
    return mat;
  }
}

// RGBMono
void ImageType::RGBMono::validate(const cv::Mat& input) {
  validateRGBMono(input);
}

cv::Mat ImageType::RGBMono::toRGB(const ImageWrapper<RGBMono>& image) {
  return RGBMonoToRGB(image);
}

cv::Mat ImageType::RGBMono::toMono(const ImageWrapper<RGBMono>& image) {
  return RGBMonoToMono(image);
}

// Depth
void ImageType::Depth::validate(const cv::Mat& input) {
  validateSingleImage(input, OpenCVType, name());
}

// TODO: should this use getDisparityVis?
// depth is metric so we should actually just scale it!
cv::Mat ImageType::Depth::toRGB(const ImageWrapper<Depth>& image) {
  cv::Mat metric_depth_map = image.clone();
  // convert to floats to allow patch nan's to work
  metric_depth_map.convertTo(metric_depth_map, CV_32F);

  static constexpr float max_value = 25.0;

  cv::patchNaNs(metric_depth_map, max_value);
  for (int i = 0; i < metric_depth_map.rows; ++i) {
    for (int j = 0; j < metric_depth_map.cols; ++j) {
      float& pixel = metric_depth_map.at<float>(i, j);
      if (std::isinf(pixel)) {
        pixel = 0.0;  // Replace Inf with small value so image is not saturated
                      // to a large number
      }
    }
  }

  // // truncate to max value (e.g. there is some outlier in the depth value
  // that is a super larger number) cv::threshold(metric_depth_map,
  // metric_depth_map, max_value, max_value, cv::THRESH_TRUNC);

  // //fix a pixel as the maximum depth to avoid flickering!!
  // metric_depth_map.at<float>(1, 1) = max_value;

  // Normalize the depth map to the range [0, 1]
  cv::Mat normalised_depth_map;
  cv::normalize(metric_depth_map, normalised_depth_map, 0.0, 1.0,
                cv::NORM_MINMAX);
  // // Convert the normalized depth map to an 8-bit image [0, 255]
  cv::Mat depth_map_8u;
  normalised_depth_map.convertTo(depth_map_8u, CV_8U, 255.0, 20.0);

  return depth_map_8u;
}

void ImageType::OpticalFlow::validate(const cv::Mat& input) {
  validateSingleImage(input, OpenCVType, name());
}

cv::Mat ImageType::OpticalFlow::toRGB(const ImageWrapper<OpticalFlow>& image) {
  const cv::Mat& optical_flow = image;
  cv::Mat flow_viz;
  utils::flowToRgb(optical_flow, flow_viz);
  return flow_viz;
}

void ImageType::SemanticMask::validate(const cv::Mat& input) {
  validateMask(input, name());
}

cv::Mat ImageType::SemanticMask::toRGB(
    const ImageWrapper<SemanticMask>& image) {
  const cv::Mat& semantic_mask = image;
  return utils::labelMaskToRGB(semantic_mask, background_label);
}

void ImageType::MotionMask::validate(const cv::Mat& input) {
  validateMask(input, name());
}

cv::Mat ImageType::MotionMask::toRGB(const ImageWrapper<MotionMask>& image) {
  const cv::Mat motion_mask = image.clone();
  return utils::labelMaskToRGB(motion_mask, background_label);
}

void ImageType::ClassSegmentation::validate(const cv::Mat& input) {
  validateMask(input, name());
}

cv::Mat ImageType::ClassSegmentation::toRGB(
    const ImageWrapper<ClassSegmentation>& image) {
  const cv::Mat& class_seg_mask = image;
  return utils::labelMaskToRGB(class_seg_mask,
                               (int)ClassSegmentation::Labels::Undefined);
}

}  // namespace dyno
````

## File: common/Map.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/common/Map.hpp"

namespace dyno {


} //dyno
````

## File: common/MapNodes.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/common/MapNodes.hpp"

namespace dyno {

} //dyno
````

## File: common/ModuleFactory.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

// #include "dynosam/common/ModuleFactory.hpp"


// namespace dyno {

// Return ModuleFactory::Create(const DynoParams& params, ImageDisplayQueue* display_queue) {

// }

// } //dyno
````

## File: common/PointCloudProcess.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au), Yiduo Wang (yiduo.wang@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include "dynosam/common/PointCloudProcess.hpp"

namespace dyno {

ObjectBBX findAABBFromCloud(
    const pcl::PointCloud<pcl::PointXYZRGB>::Ptr obj_cloud_ptr) {
  pcl::PointCloud<pcl::PointXYZRGB> filtered_cloud;

  // statistical outlier removal
  pcl::StatisticalOutlierRemoval<pcl::PointXYZRGB> outlier_remover;
  outlier_remover.setInputCloud(obj_cloud_ptr);
  outlier_remover.setMeanK(100);
  outlier_remover.setStddevMulThresh(1.0);
  outlier_remover.filter(filtered_cloud);

  // axis aligned bounding box
  pcl::MomentOfInertiaEstimation<pcl::PointXYZRGB> bbx_extractor;
  bbx_extractor.setInputCloud(
      pcl::make_shared<pcl::PointCloud<pcl::PointXYZRGB> >(filtered_cloud));
  bbx_extractor.compute();

  pcl::PointXYZRGB min_point_AABB;
  pcl::PointXYZRGB max_point_AABB;
  bbx_extractor.getAABB(min_point_AABB, max_point_AABB);

  ObjectBBX aabb;
  aabb.min_bbx_point_ = gtsam::Point3(static_cast<double>(min_point_AABB.x),
                                      static_cast<double>(min_point_AABB.y),
                                      static_cast<double>(min_point_AABB.z));
  aabb.max_bbx_point_ = gtsam::Point3(static_cast<double>(max_point_AABB.x),
                                      static_cast<double>(max_point_AABB.y),
                                      static_cast<double>(max_point_AABB.z));

  return aabb;
}

CloudPerObject groupObjectCloud(const StatusLandmarkVector& landmarks,
                                const gtsam::Pose3& T_world_camera) {
  CloudPerObject clouds_per_obj;

  for (const auto& status_estimate : landmarks) {
    Landmark lmk_world = status_estimate.value();
    const ObjectId object_id = status_estimate.objectId();
    if (status_estimate.referenceFrame() == ReferenceFrame::LOCAL) {
      lmk_world = T_world_camera * status_estimate.value();
    } else if (status_estimate.referenceFrame() == ReferenceFrame::OBJECT) {
      throw DynosamException(
          "Cannot display object point in the object reference frame");
    }

    pcl::PointXYZRGB pt;
    if (status_estimate.isStatic()) {
      // publish static lmk's as white
      pt = pcl::PointXYZRGB(lmk_world(0), lmk_world(1), lmk_world(2), 0, 0, 0);
    } else {
      const cv::Scalar colour = Color::uniqueId(object_id);
      pt = pcl::PointXYZRGB(lmk_world(0), lmk_world(1), lmk_world(2), colour(0),
                            colour(1), colour(2));
    }
    clouds_per_obj[object_id].push_back(pt);
  }

  return clouds_per_obj;
}

}  // namespace dyno
````

## File: common/StereoCamera.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/common/StereoCamera.hpp"
#include "dynosam/utils/GtsamUtils.hpp"
#include "dynosam/utils/OpenCVUtils.hpp"


#include <Eigen/Core>
#include <opencv2/calib3d.hpp>
#include <opencv2/core.hpp>

#include <opencv2/calib3d.hpp>

#include <glog/logging.h>

namespace dyno {

StereoCamera::StereoCamera(Camera::ConstPtr left_camera, Camera::ConstPtr right_camera)
        :   original_left_camera_(CHECK_NOTNULL(left_camera)),
            original_right_camera_(CHECK_NOTNULL(right_camera)),
            undistorted_rectified_stereo_camera_impl_(),
            stereo_calibration_(nullptr),
            left_cam_undistort_rectifier_(nullptr),
            right_cam_undistort_rectifier_(nullptr),
            stereo_baseline_(0.0) {

    computeRectificationParameters(left_camera->getParams(),
                                 right_camera->getParams(),
                                 R1_,
                                 R2_,
                                 P1_,
                                 P2_,
                                 Q_,
                                 ROI1_,
                                 ROI2_);
    calculateBaseLine(stereo_baseline_, Q_);

    //! Create stereo camera calibration after rectification and undistortion.
    const auto left_undist_rect_cam_mat = utils::Cvmat2Cal3_S2(P1_);
    stereo_calibration_.reset(
      new gtsam::Cal3_S2Stereo(left_undist_rect_cam_mat.fx(),
                               left_undist_rect_cam_mat.fy(),
                               left_undist_rect_cam_mat.skew(),
                               left_undist_rect_cam_mat.px(),
                               left_undist_rect_cam_mat.py(),
                               stereo_baseline_));

    //! Create undistort rectifiers: these should be called after
    //! computeRectificationParameters.
    left_cam_undistort_rectifier_ =
        std::make_unique<UndistorterRectifier>(P1_, left_camera->getParams(), R1_);
    right_cam_undistort_rectifier_ =
        std::make_unique<UndistorterRectifier>(P2_, right_camera->getParams(), R2_);

    //! Create stereo camera implementation. As per all dynosam cameras, make camera in the local frame
    //TODO: is this right? In the original implementation (Kimera) we use the rectified pose of the left image
    // which SHOULD still be local!!
    undistorted_rectified_stereo_camera_impl_ =
      gtsam::StereoCamera(gtsam::Pose3::Identity(), stereo_calibration_);

    }

StereoCamera::StereoCamera(
    const CameraParams& left_cam_params,
    const CameraParams& right_cam_params)
    :   StereoCamera(std::make_shared<Camera>(left_cam_params),std::make_shared<Camera>(right_cam_params)) {}

void StereoCamera::undistortRectifyImages(cv::Mat& rectify_left, cv::Mat& rectify_right, const cv::Mat& left, const cv::Mat& right) const {
    left_cam_undistort_rectifier_->undistortRectifyImage(left, rectify_left);
    right_cam_undistort_rectifier_->undistortRectifyImage(right, rectify_right);
}


void StereoCamera::computeRectificationParameters(
    const CameraParams& left_cam_params,
    const CameraParams& right_cam_params,
    cv::Mat& R1,
    cv::Mat& R2,
    cv::Mat& P1,
    cv::Mat& P2,
    cv::Mat& Q,
    cv::Rect& ROI1,
    cv::Rect& ROI2) {

    // ! Extrinsics of the stereo (not rectified) relative pose between cameras
    gtsam::Pose3 camL_Pose_camR =
        (left_cam_params.getExtrinsics()).between(right_cam_params.getExtrinsics());


    //! check extrinsics are not identity!
    CHECK(!camL_Pose_camR.equals(gtsam::Pose3::Identity())) << "Relative pose between cameras cannot be identity - camL_Pose_camR = " << camL_Pose_camR;

    // Get extrinsics in open CV format.
    // NOTE: openCV pose convention is the opposite, that's why we have to
    // invert
    cv::Mat camL_Rot_camR, camL_Tran_camR;
    std::tie(camL_Rot_camR, camL_Tran_camR) =
        utils::Pose2cvmats(camL_Pose_camR.inverse());

    LOG(INFO) << camL_Rot_camR;
    LOG(INFO) << camL_Tran_camR;

    const auto left_k = left_cam_params.getCameraMatrix();
    const auto left_d = left_cam_params.getDistortionCoeffs();

    const auto right_k = right_cam_params.getCameraMatrix();
    const auto right_d = right_cam_params.getDistortionCoeffs();

    const auto size = left_cam_params.imageSize();

    // kAlpha is -1 by default, but that introduces invalid keypoints!
    // here we should use kAlpha = 0 so we get only valid pixels...
    // But that has an issue that it removes large part of the image, check:
    // https://github.com/opencv/opencv/issues/7240 for this issue with kAlpha
    // Setting to -1 to make it easy, but it should NOT be -1!
    static constexpr int kAlpha = 0;
    switch (left_cam_params.getDistortionModel()) {
        case DistortionModel::RADTAN: {
        cv::stereoRectify(
            // Input
            left_k,
            left_d,
            right_k,
            right_d,
            size,
            camL_Rot_camR,
            camL_Tran_camR,
            // Output
            R1,
            R2,
            P1,
            P2,
            Q,
            cv::CALIB_ZERO_DISPARITY,
            kAlpha,
            cv::Size(),
            &ROI1,
            &ROI2);
        } break;
        case DistortionModel::EQUIDISTANT: {
        cv::fisheye::stereoRectify(
            // Input
            left_k,
            left_d,
            right_k,
            right_d,
            size,
            camL_Rot_camR,
            camL_Tran_camR,
            // Output
            R1,
            R2,
            P1,
            P2,
            Q,
            // TODO: Flag to maximise area???
            cv::CALIB_ZERO_DISPARITY);
        } break;
        default: {
        LOG(FATAL) << "Unknown DistortionModel: " << to_string(left_cam_params.getDistortionModel());
        }
    }

    LOG(INFO) << Q;
}

void StereoCamera::calculateBaseLine(Baseline& base_line, const cv::Mat& Q) {
  // Calc baseline (see L.2700 and L.2616 in
  // https://github.com/opencv/opencv/blob/master/modules/calib3d/src/calibration.cpp
  // NOTE: OpenCV pose convention is the opposite, therefore the missing -1.0
  CHECK_NE(Q.at<double>(3, 2), 0.0);
  base_line = 1.0 / Q.at<double>(3, 2);
  CHECK_GT(base_line, 0.0);
}

} //dyno
````

## File: common/StructuredContainers.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/common/StructuredContainers.hpp"

#include <unordered_map>

namespace dyno {

} //dyno
````

## File: common/Types.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/common/Types.hpp"
#include "dynosam/common/Flags.hpp"
#include <gflags/gflags.h>

#include <string>
#include <cxxabi.h>

//common glags used in multiple modules
DEFINE_bool(init_object_pose_from_gt, false , "If true, then the viz pose from the frontend/backend will start from the gt");
DEFINE_bool(save_frontend_json, false, "If true, then the output of the frontend will be saved as a binarized json file");
DEFINE_bool(frontend_from_file, false, "If true, the frontend will try and load all the data from a seralized json file");
DEFINE_bool(use_smoothing_factor, true, "If the backend should use the smoothing factor between motions");

DEFINE_int32(backend_updater_enum, 0, "Which UpdaterType the backend should use and should match an enum in whatever backend module type is loaded");
DEFINE_bool(use_byte_tracker, false, "If true, the ByteTracking will be used to associated objects between frames. Else, the instance label will be used as the object id");




namespace dyno {


template<>
std::string to_string(const ReferenceFrame& reference_frame) {
  switch (reference_frame)
  {
  case ReferenceFrame::LOCAL:
    return "local";
  case ReferenceFrame::GLOBAL:
    return "global";
  case ReferenceFrame::OBJECT:
    return "object";
  default:
    return "Unknown reference frame";
  }
};

std::string demangle(const char* name)
{
  // by default set to the original mangled name
  std::string demangled_name = std::string(name);

  // g++ version of demangle
  char* demangled = nullptr;
  int status = -1;  // some arbitrary value to eliminate the compiler warning
  demangled = abi::__cxa_demangle(name, nullptr, nullptr, &status);

  demangled_name = (status == 0) ? std::string(demangled) : std::string(name);
  std::free(demangled);

  return demangled_name;
}


} //dyno
````

## File: common/ZEDCamera.cc
````
// common/ZEDCamera.cc
#include "dynosam/common/ZEDCamera.hpp"

namespace dyno {
namespace common {
ZEDCamera::ZEDCamera(const ZEDCameraConfig& config)
  : config_(config) {
  populateInitParams();
}

ZEDCamera::~ZEDCamera() {
  shutdown();
}


std::vector<ZEDCamera::Ptr> ZEDCamera::discoverAndCreateCameras(
    const std::function<ZEDCameraConfig(const sl::DeviceProperties&)>& configProvider,
    bool openCameras) {
    std::vector<ZEDCamera::Ptr> result_cameras;
    std::vector<sl::DeviceProperties> device_list = sl::Camera::getDeviceList();

    LOG(INFO) << "Discovering ZED cameras. Found " << device_list.size() << " ZED device(s).";

    for (const auto& props : device_list) {
        LOG(INFO) << "Processing ZED camera with S/N: " << props.serial_number
                  << ", ID: " << props.id
                  << ", Model: " << sl::toString(props.camera_model).c_str();

        ZEDCameraConfig config = configProvider(props); // The configProvider is responsible for setting camera_device_id correctly.

        ZEDCamera::Ptr camera = std::make_shared<ZEDCamera>(config);

        if (openCameras) {
            if (camera->open()) {
                LOG(INFO) << "Successfully opened ZED camera with S/N: " << props.serial_number << " and ID: " << props.id;
                result_cameras.push_back(camera);
            } else {
                LOG(ERROR) << "Failed to open ZED camera with S/N: " << props.serial_number << " and ID: " << props.id;
                // Do not add to the list if open fails
            }
        } else {
            LOG(INFO) << "Configured ZED camera with S/N: " << props.serial_number << " and ID: " << props.id << ". Not opening as per request.";
            result_cameras.push_back(camera);
        }
    }
    return result_cameras;
}

std::vector<ZEDCamera::Ptr> ZEDCamera::discoverAndCreateCameras(
    const ZEDCameraConfig& defaultConfig,
    bool openCameras) {

    auto default_config_provider = [&](const sl::DeviceProperties& props) -> ZEDCameraConfig {
        ZEDCameraConfig specific_config = defaultConfig;
        specific_config.camera_device_id = props.id;
        return specific_config;
    };

    return discoverAndCreateCameras(default_config_provider, openCameras);
}

void ZEDCamera::populateInitParams() {
  zed_init_params_.camera_resolution = config_.resolution;
  zed_init_params_.camera_fps = config_.fps;
  zed_init_params_.depth_mode = config_.depth_mode;
  zed_init_params_.coordinate_units = config_.coordinate_units;
  zed_init_params_.coordinate_system = config_.coordinate_system_3d;
  zed_init_params_.sdk_verbose = config_.sdk_verbose > 0;
  zed_init_params_.camera_disable_self_calib = config_.
      camera_disable_self_calib;
  zed_init_params_.enable_image_enhancement = config_.enable_image_enhancement;
  zed_init_params_.open_timeout_sec = config_.open_timeout_sec;
  zed_init_params_.enable_right_side_measure = config_.
      enable_right_side_measure;
  zed_init_params_.async_grab_camera_recovery = config_.
      async_grab_camera_recovery;
  zed_init_params_.grab_compute_capping_fps = config_.grab_compute_capping_fps;
  zed_init_params_.enable_image_validity_check = config_.
      enable_image_validity_check;

  if (!config_.svo_file_path.empty()) {
    zed_init_params_.input.setFromSVOFile(config_.svo_file_path.c_str());
    zed_init_params_.svo_real_time_mode = config_.svo_real_time_mode;
  }
  zed_init_params_.sensors_required = config_.enable_imu;
}

bool ZEDCamera::open() {
  if (is_opened_.load()) {
    LOG(WARNING) << "ZEDCamera::open() called but camera is already open.";
    return true;
  }
  sl::ERROR_CODE err = zed_camera_.open(zed_init_params_);
  if (err != sl::ERROR_CODE::SUCCESS) {
    LOG(ERROR) << "ZEDCamera: Failed to open ZED camera: " << sl::toString(err);
    is_opened_.store(false);
    return false;
  }
  is_opened_.store(true);
  LOG(INFO) << "ZEDCamera: ZED camera opened successfully.";
  return true;
}

void ZEDCamera::close() {
  if (is_opened_.exchange(false)) {
    // Atomically set to false and get previous value
    disableObjectDetection();
    disableBodyTracking();
    zed_camera_.close();
    LOG(INFO) << "ZEDCamera: ZED camera closed.";
  }
}

void ZEDCamera::shutdown() {
  close();
}

bool ZEDCamera::isOpened() const {
  return is_opened_.load() && zed_camera_.isOpened();
}

sl::ERROR_CODE ZEDCamera::grab(const sl::RuntimeParameters& runtime_params) {
  if (!isOpened()) {
    LOG(ERROR) << "ZEDCamera::grab() called but camera is not open.";
    return sl::ERROR_CODE::CAMERA_NOT_INITIALIZED;
  }
  return zed_camera_.grab(runtime_params);
}

sl::ERROR_CODE ZEDCamera::retrieveRawImage(sl::VIEW view_identifier,
                                           sl::Mat& out_image, sl::MEM mem_type,
                                           const sl::Resolution& resolution) {
  if (!isOpened()) return sl::ERROR_CODE::CAMERA_NOT_INITIALIZED;
  // If resolution width/height are 0, ZED SDK uses default resolution.
  return zed_camera_.retrieveImage(out_image, view_identifier, mem_type,
                                   resolution);
}

sl::ERROR_CODE ZEDCamera::retrieveRawDepth(sl::Mat& out_depth, sl::MEM mem_type,
                                           const sl::Resolution& resolution) {
  return retrieveRawMeasure(sl::MEASURE::DEPTH, out_depth, mem_type,
                            resolution);
}

sl::ERROR_CODE ZEDCamera::retrieveRawMeasure(sl::MEASURE measure,
                                             sl::Mat& out_measure,
                                             sl::MEM mem_type,
                                             const sl::Resolution& resolution) {
  if (!isOpened()) return sl::ERROR_CODE::CAMERA_NOT_INITIALIZED;
  return zed_camera_.
      retrieveMeasure(out_measure, measure, mem_type, resolution);
}

sl::ERROR_CODE ZEDCamera::retrieveRawSensorsData(
    sl::SensorsData& out_sensors_data, sl::TIME_REFERENCE time_ref) {
  if (!isOpened()) {
    LOG(ERROR) <<
        "ZEDCamera::retrieveRawSensorsData() called but camera is not open.";
    return sl::ERROR_CODE::CAMERA_NOT_INITIALIZED;
  }
  if (!config_.enable_imu) {
    LOG(WARNING) <<
        "ZEDCamera::retrieveRawSensorsData() called but IMU is not enabled in config.";
    return sl::ERROR_CODE::SENSORS_NOT_AVAILABLE;
  }
  return zed_camera_.getSensorsData(out_sensors_data, time_ref);
}

bool ZEDCamera::enableObjectDetection() {
  if (!isOpened()) {
    LOG(ERROR) <<
        "ZEDCamera::enableObjectDetection() called but camera is not open.";
    return false;
  }
  if (is_object_detection_enabled_.load()) {
    LOG(WARNING) <<
        "ZEDCamera::enableObjectDetection() called but OD is already enabled.";
    return true;
  }

  sl::ObjectDetectionParameters od_params;
  od_params.detection_model = config_.object_detection_model;
  od_params.enable_tracking = config_.object_detection_enable_tracking;
  od_params.enable_segmentation = config_.object_detection_enable_segmentation;
  // od_params.image_sync = true; // Default behavior, consider if explicit setting is needed

  sl::ERROR_CODE err = zed_camera_.enableObjectDetection(od_params);
  if (err != sl::ERROR_CODE::SUCCESS) {
    LOG(ERROR) << "ZEDCamera: Failed to enable Object Detection: " <<
        sl::toString(err);
    is_object_detection_enabled_.store(false);
    return false;
  }
  is_object_detection_enabled_.store(true);
  LOG(INFO) << "ZEDCamera: Object Detection enabled.";
  return true;
}

void ZEDCamera::disableObjectDetection() {
  if (is_opened_.load() && is_object_detection_enabled_.exchange(false)) {
    zed_camera_.disableObjectDetection();
    LOG(INFO) << "ZEDCamera: Object Detection disabled.";
  }
}

sl::ERROR_CODE ZEDCamera::retrieveObjects(sl::Objects& out_objects,
                                          const
                                          sl::ObjectDetectionRuntimeParameters&
                                          params) {
  if (!isOpened()) return sl::ERROR_CODE::CAMERA_NOT_INITIALIZED;
  if (!is_object_detection_enabled_.load()) {
    LOG(WARNING) <<
        "ZEDCamera::retrieveObjects() called but OD is not enabled.";
    return sl::ERROR_CODE::MODULE_NOT_COMPATIBLE_WITH_CAMERA;
  }
  return zed_camera_.retrieveObjects(out_objects, params);
}

bool ZEDCamera::enableBodyTracking() {
  if (!isOpened()) {
    LOG(ERROR) <<
        "ZEDCamera::enableBodyTracking() called but camera is not open.";
    return false;
  }
  if (is_body_tracking_enabled_.load()) {
    LOG(WARNING) <<
        "ZEDCamera::enableBodyTracking() called but BT is already enabled.";
    return true;
  }

  sl::BodyTrackingParameters bt_params;
  bt_params.detection_model = config_.body_tracking_model;
  bt_params.body_format = config_.body_format;
  bt_params.enable_tracking = config_.object_detection_enable_tracking;
  // Reuse general tracking flag
  bt_params.enable_body_fitting = config_.body_tracking_enable_fitting;
  bt_params.enable_segmentation = config_.body_tracking_enable_segmentation;
  bt_params.allow_reduced_precision_inference = config_.
      body_tracking_allow_reduced_precision_inference;
  // bt_params.image_sync = true;

  sl::ERROR_CODE err = zed_camera_.enableBodyTracking(bt_params);
  if (err != sl::ERROR_CODE::SUCCESS) {
    LOG(ERROR) << "ZEDCamera: Failed to enable Body Tracking: " <<
        sl::toString(err);
    is_body_tracking_enabled_.store(false);
    return false;
  }
  is_body_tracking_enabled_.store(true);
  LOG(INFO) << "ZEDCamera: Body Tracking enabled.";
  return true;
}

void ZEDCamera::disableBodyTracking() {
  if (is_opened_.load() && is_body_tracking_enabled_.exchange(false)) {
    zed_camera_.disableBodyTracking();
    LOG(INFO) << "ZEDCamera: Body Tracking disabled.";
  }
}

sl::ERROR_CODE ZEDCamera::retrieveBodies(sl::Bodies& out_bodies,
                                         const sl::BodyTrackingRuntimeParameters
                                         & params) {
  if (!isOpened()) return sl::ERROR_CODE::CAMERA_NOT_INITIALIZED;
  if (!is_body_tracking_enabled_.load()) {
    LOG(WARNING) << "ZEDCamera::retrieveBodies() called but BT is not enabled.";
    return sl::ERROR_CODE::MODULE_NOT_COMPATIBLE_WITH_CAMERA;
  }
  return zed_camera_.retrieveBodies(out_bodies, params);
}

sl::CameraInformation ZEDCamera::getCameraInformation() const {
  if (!isOpened()) {
    LOG(ERROR) <<
        "ZEDCamera::getCameraInformation() called but camera is not open. Returning default.";
    return sl::CameraInformation();
  }
  return zed_camera_.getCameraInformation();
}

sl::CalibrationParameters ZEDCamera::getCalibrationParameters(bool raw) const {
  if (!isOpened()) {
    LOG(ERROR) <<
        "ZEDCamera::getCalibrationParameters() called but camera is not open. Returning default.";
    return {};
  }
  if (raw) {
    // This is a bit of a simplification. The "raw" parameters might be part of a different substruct
    // or require different handling depending on ZED SDK version and specific needs.
    // Typically, ZED provides rectified images, so `camera_configuration.calibration_parameters` is for those.
    // Accessing truly "raw" (pre-rectification) might need specific SDK features if available.
    // For now, this illustrates the intent.
#if ZED_SDK_MAJOR_VERSION >= 3 && ZED_SDK_MINOR_VERSION >= 8 // Example, check actual SDK versioning for raw params


    // return getCameraInformation().camera_configuration.calibration_parameters_raw; // Hypothetical
#endif
    LOG(WARNING) <<
        "Raw calibration parameters requested but might not be distinctly available or handled in this simplified version. Returning rectified.";
  }
  return getCameraInformation().camera_configuration.calibration_parameters;
  // Returns rectified by default
}

sl::InitParameters ZEDCamera::getCameraInitParams()  {
  if (!isOpened()) {
    LOG(ERROR) << "ZEDCamera::getCameraParams() called but camera is not open. Returning default.";
    return {};
  }
  // Return the left camera parameters (rectified)
  return zed_init_params_;
}

bool ZEDCamera::isSVOMode() const {
  return !config_.svo_file_path.empty();
}

int ZEDCamera::getSVONumberOfFrames() {
  if (!isOpened() || !isSVOMode()) {
    return 0;
  }
  return zed_camera_.getSVONumberOfFrames();
}

unsigned int ZEDCamera::getSVOFrameRate() const {
  if (!isOpened() || !isSVOMode()) {
    return 0;
  }
  // The SVO's original FPS is stored in CameraInformation after opening.
  return getCameraInformation().camera_configuration.fps;
}

sl::ERROR_CODE ZEDCamera::setSVOPosition(int frame_position) {
  if (!isOpened() || !isSVOMode()) {
    return isSVOMode()
             ? sl::ERROR_CODE::CAMERA_NOT_INITIALIZED
             : sl::ERROR_CODE::MODULE_NOT_COMPATIBLE_WITH_CAMERA;
  }
  zed_camera_.setSVOPosition(frame_position);
  return sl::ERROR_CODE::SUCCESS;
}
} // namespace common
} // namespace dyno
````

## File: dataprovider/ClusterSlamDataProvider.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/dataprovider/ClusterSlamDataProvider.hpp"
#include "dynosam/dataprovider/DataProviderUtils.hpp"
#include "dynosam/utils/OpenCVUtils.hpp"
#include "dynosam/visualizer/ColourMap.hpp"
#include "dynosam/utils/GtsamUtils.hpp"

#include "dynosam/frontend/vision/VisionTools.hpp" //for getObjectLabels
#include "dynosam/common/Algorithms.hpp"

#include "dynosam/common/StereoCamera.hpp"
#include "dynosam/frontend/vision/StereoMatcher.hpp"

#include <glog/logging.h>
#include <fstream>
#include <png++/png.hpp> //libpng-dev


namespace dyno {



/**
 * @brief As there is so much interdependancy between the folders in the dataset,
 * we just have the one loader so all the data can be accessed in the same class.
 * This class then provides interfaces for each dyno::DataFolder<....> so that the Dynodataset
 * can use it!
 *
 */
class ClusterSlamAllLoader {

public:
    DYNO_POINTER_TYPEDEFS(ClusterSlamAllLoader)

    /**
     * @brief Construct a new Cluster Slam Gt Loader object.
     *
     * File path is the full file path to a clusterslam folder, containing
     * /images
     * /pose....
     *
     * @param file_path const std::string&
     */
    ClusterSlamAllLoader(const std::string& file_path)
    :   left_images_folder_path_(file_path + "/images/left"),
        right_images_folder_path_(file_path + "/images/right"),
        optical_flow_folder_path_(file_path + "/optical_flow"),
        pose_folder_path_(file_path + "/pose"),
        instance_masks_folder_(file_path + "/instance_masks"),
        left_landmarks_folder_(file_path + "/landmarks/left"),
        landmark_mapping_file_path_(file_path + "/landmark_mapping.txt"),
        intrinsics_file_path_(file_path + "/intrinsic.txt")
    {

        throwExceptionIfPathInvalid(left_images_folder_path_);
        throwExceptionIfPathInvalid(right_images_folder_path_);
        throwExceptionIfPathInvalid(optical_flow_folder_path_);
        throwExceptionIfPathInvalid(pose_folder_path_);
        throwExceptionIfPathInvalid(instance_masks_folder_);
        throwExceptionIfPathInvalid(left_landmarks_folder_);
        throwExceptionIfPathInvalid(landmark_mapping_file_path_);
        throwExceptionIfPathInvalid(intrinsics_file_path_);

        //first load images and size
        //the size will be used as a refernce for all other loaders
        //size is number of images
        //we use flow to set the size as for this dataset, there will be one less
        //optical flow as the flow ids index t to t+1 (which is gross, I know!!)
        loadFlowImagesAndSize(optical_flow_image_paths_, dataset_size_);
        //left
        loadImages(left_rgb_image_paths_, left_images_folder_path_);
        //right
        loadImages(right_rgb_image_paths_, right_images_folder_path_);
        CHECK_EQ(right_rgb_image_paths_.size(), left_rgb_image_paths_.size());

        loadInstanceMasksImages(instance_masks_image_paths_);
        loadLeftLandmarks(left_landmarks_map_);

        loadLandMarkMapping(landmark_mapping_);

        CHECK_EQ(optical_flow_image_paths_.size(), left_rgb_image_paths_.size() - 1);
        CHECK_EQ(instance_masks_image_paths_.size(), optical_flow_image_paths_.size());
        CHECK_EQ(size(), optical_flow_image_paths_.size());
        setGroundTruthPacket(ground_truth_packets_);

        setIntrisics();

    }

    size_t size() const {
        return dataset_size_;
    }

    cv::Mat getOpticalFlow(size_t idx) const {
        CHECK_LT(idx,optical_flow_image_paths_.size());

        cv::Mat flow;
        loadFlow(optical_flow_image_paths_.at(idx), flow);
        CHECK(!flow.empty());
        return flow;
    }

    cv::Mat getRGB(size_t idx) const {
        CHECK_LT(idx, left_rgb_image_paths_.size());


        cv::Mat rgb;
        loadRGB(left_rgb_image_paths_.at(idx), rgb);
        CHECK(!rgb.empty());

        //this set of images are loaded as 8UC4
        // CHECK_EQ(rgb.type(), CV_8UC4) << "Somehow the image type has changed...";
        // rgb.convertTo(rgb, CV_8UC3);

        //debug check -> draw keypoints on image!
        // const LandmarksMap& kps_map = left_landmarks_map_.at(idx);

        // for(const auto&[landmark_id, kp] : kps_map) {
        //     const auto cluster_id = landmark_mapping_.at(landmark_id);

        //     cv::Point2f pt(utils::gtsamPointToCv(kp));
        //     utils::drawCircleInPlace(rgb, pt, ColourMap::getObjectColour(cluster_id));
        // }

        return rgb;
    }

    cv::Mat getInstanceMask(size_t idx) const {
        CHECK_LT(idx, left_rgb_image_paths_.size());
        CHECK_LT(idx, dataset_size_);

        cv::Mat mask, relabelled_mask;
        loadMask(instance_masks_image_paths_.at(idx), mask);
        CHECK(!mask.empty());

        associateDetectedBBWithObject(mask, idx, relabelled_mask);

        return relabelled_mask;
    }

    cv::Mat getDepthImage(size_t idx) const {
        return denseStereoReconstruction(idx);
    }

    const GroundTruthInputPacket& getGtPacket(size_t idx) const {
        return ground_truth_packets_.at(idx);
    }

    const CameraParams& getLeftCameraParams() const {
        return left_camera_params_;
    }

    double getTimestamp(size_t idx) {
        //we dont have a time? for now just make idx
        return static_cast<double>(idx);
    }

private:

   void loadFlowImagesAndSize(std::vector<std::string>& images_paths, size_t& dataset_size) {
    std::vector<std::filesystem::path> files_in_directory = getAllFilesInDir(optical_flow_folder_path_);
    dataset_size = files_in_directory.size();
    CHECK_GT(dataset_size, 0);

    for (const std::string file_path : files_in_directory) {
        throwExceptionIfPathInvalid(file_path);
        images_paths.push_back(file_path);
    }
   }

    void loadImages(std::vector<std::string>& images_paths, const std::string& image_folder) {
        std::vector<std::filesystem::path> files_in_directory = getAllFilesInDir(image_folder);

        for (const std::string file_path : files_in_directory) {
            throwExceptionIfPathInvalid(file_path);
            images_paths.push_back(file_path);
        }
    }

    void loadInstanceMasksImages(std::vector<std::string>& images_paths) {
        std::vector<std::filesystem::path> files_in_directory = getAllFilesInDir(instance_masks_folder_);

        for (const std::string file_path : files_in_directory) {
            throwExceptionIfPathInvalid(file_path);
            images_paths.push_back(file_path);
        }
    }

    //set all intrinsics/camera parameters related variables include stereo camera/matcher etc...
    void setIntrisics() {
        std::ifstream fstream;
        fstream.open(intrinsics_file_path_, std::ios::in);
        if(!fstream) {
            throw std::runtime_error("Could not open file " + intrinsics_file_path_ + " when trying to load Cluster Slam intrinsics!");
        }

        auto load_projection_matrix = [](std::ifstream& fstream, gtsam::Matrix34& projection_matrix) -> void {
            //read the first 3 liens of the projection matrix and populate camera matrix
            for(size_t i = 0; i < 3; i++) {
                // std::vector<std::string> split_line;
                //hmmm my getline function does not seem to work here (maybe because doubles so we'll do it manually!)
                // CHECK(getLine(fstream, split_line));
                // CHECK_EQ(split_line.size(), 4) << "Line was: " << container_to_string(split_line, ", ");

                std::string s;
                std::getline(fstream, s);

                std::stringstream ss;
                ss << s;

                LOG(INFO) << ss.str();
                for(size_t j = 0; j < 4; j++) {
                    double tmp;
                    ss >> tmp;
                    projection_matrix(i, j) = tmp;
                }
            }
        }; //load_projection_matrix

        //intrinscis file is
        //3 x 4 projection matrix, over 3 lines
        // new line
        //3 x 4 projection matrix, over 3 lines
        //populate camera 1 matrix
        load_projection_matrix(fstream, projection_matrix_cam1_);

        //read the line line
        std::string s;
        std::getline(fstream, s);
        //populate camera 2 matrix
        load_projection_matrix(fstream, projection_matrix_cam2_);

        //set K matrices
        K_cam_1 = projection_matrix_cam1_.topLeftCorner(3, 3);
        K_cam_2 = projection_matrix_cam2_.topLeftCorner(3, 3);

        double fx_left = K_cam_1(0, 0);
        double fy_left = K_cam_1(1, 1);
        double cu_left = K_cam_1(0, 2);
        double cv_left = K_cam_1(1, 2);

        double fx_right = K_cam_2(0, 0);
        double fy_right = K_cam_2(1, 1);
        double cu_right = K_cam_2(0, 2);
        double cv_right = K_cam_2(1, 2);

        //Projection_matrix = K * extrinsics
        gtsam::Pose3 extrinsics_left = gtsam::Pose3(K_cam_1.inverse() * projection_matrix_cam1_);
        gtsam::Pose3 extrinsics_right = gtsam::Pose3(K_cam_2.inverse() * projection_matrix_cam2_);

        //I think this dataset has the projection matrix in the local left frame (i.e projection of 2 into 1)
        //becuase the translation component of extrinsics_right is negative
        //we want extrinsice to be in the world frame because this is what the camera params expect
        //OR the dataset is in OpenCV convention...
        extrinsics_right = extrinsics_right.inverse();

        LOG(INFO) << "left e " << extrinsics_left;
        LOG(INFO) << "right e " << extrinsics_right;

        left_camera_params_ = CameraParams(
            CameraParams::IntrinsicsCoeffs({fx_left, fy_left, cu_left, cv_left}),
            CameraParams::DistortionCoeffs({0, 0, 0, 0}),
            getRGB(0).size(),
            DistortionModel::RADTAN,
            extrinsics_left
        );

        CameraParams right_camera_params(
            CameraParams::IntrinsicsCoeffs({fx_right, fy_right, cu_right, cv_right}),
            CameraParams::DistortionCoeffs({0, 0, 0, 0}),
            getRGB(0).size(),
            DistortionModel::RADTAN,
            extrinsics_right
        );

        stereo_camera_ = std::make_shared<StereoCamera>(left_camera_params_, right_camera_params);
        stereo_matcher_ = std::make_shared<StereoMatcher>(stereo_camera_);

    }


    cv::Mat denseStereoReconstruction(size_t frame) const {
        cv::Mat rgb_left = getRGB(frame);

        cv::Mat rgb_right;
        CHECK_LT(frame, right_rgb_image_paths_.size());

        loadRGB(right_rgb_image_paths_.at(frame), rgb_right);
        CHECK(!rgb_right.empty());

         //this set of images are loaded as 8UC4
        CHECK_EQ(rgb_right.type(), CV_8UC4) << "Somehow the image type has changed...";
        cv::Mat depth_image;
        CHECK_NOTNULL(stereo_matcher_)->denseStereoReconstruction(rgb_left, rgb_right, depth_image);

        return depth_image;

    }



    //mapping of landmark id -> u, v coordiante
    using LandmarksMap = gtsam::FastMap<int, Keypoint>;
    using LandmarksMapPerFrame = gtsam::FastMap<FrameId, LandmarksMap>;

    void loadLeftLandmarks(LandmarksMapPerFrame& detected_landmarks) {
        std::vector<std::filesystem::path> files_in_directory = getAllFilesInDir(left_landmarks_folder_);

        for (const std::filesystem::path& file_path : files_in_directory) {

            //this should be the file name which we will then decompose into the frame id
            //e.g. .../pose/0000.txt -> filename = 0000 -> frame id = int(0)
            const std::string file_name = file_path.filename();
            const int frame =  std::stoi(file_name);

            std::ifstream fstream;
            fstream.open((std::string)file_path, std::ios::in);
            if(!fstream) {
                throw std::runtime_error("Could not open file " + (std::string)file_path + " when trying to load Cluster Slam landmarks information!");
            }

            //check we've never processed this frame before
            CHECK(!detected_landmarks.exists(frame));

            LandmarksMap detected_lmks_this_frame;

            //expected size of each line (landamrk_id u v)
            constexpr static size_t kExpectedSize = 3u;
            while (!fstream.eof()) {
                std::vector<std::string> split_line;
                if(!getLine(fstream, split_line)) continue;

                CHECK_EQ(split_line.size(), kExpectedSize) << "Line present in landmarks file does not have the right length - " << container_to_string(split_line);

                const int landmark_id = std::stoi(split_line.at(0));
                const double u = std::stod(split_line.at(1));
                const double v = std::stod(split_line.at(2));

                detected_lmks_this_frame.insert2(landmark_id, Keypoint(u, v));
            }

            detected_landmarks.insert2(frame, detected_lmks_this_frame);
        }

        VLOG(20) << "Loaded " << detected_landmarks.size() << " detected landmarks...";

    }

    //Landmark to cluster id mapping.
    //the landmark id is the key in LandmarksMap
    void loadLandMarkMapping(gtsam::FastMap<int, int>& landmark_mapping) {
        std::ifstream fstream;
        fstream.open(landmark_mapping_file_path_, std::ios::in);
        if(!fstream) {
            throw std::runtime_error("Could not open file " + landmark_mapping_file_path_ + " when trying to load Cluster Slam landmarks mapping!");
        }

        //expected size of each line (landamrk_id cluster_id)
        constexpr static size_t kExpectedSize = 2u;
        while (!fstream.eof()) {
            std::vector<std::string> split_line;
            if(!getLine(fstream, split_line)) continue;

            CHECK_EQ(split_line.size(), kExpectedSize) << "Line present in landmark mapping file does not have the right length - " << container_to_string(split_line);

            const int landmark_id = std::stoi(split_line.at(0));
            //clusters start at 0 here, but that includes background label (which is 0)
            //since the  line ordering of the trajectories files determines the label
            //the camera pose is always first (line = 0), so any cluster that is associated to an object
            //and not the camera pose, should have idx > 0
            const int cluster_id = std::stoi(split_line.at(1));

            landmark_mapping.insert2(landmark_id, cluster_id);
        }
    }

    //we're going to have to associate the detected bounding boxes
    //with the actual landmark ids so that the gt labels match with the
    //tracking labels....
    //we will have to do this by taking all the landmarks of detected features
    //and check which objects lies within the bounding box....
    //this is almost the definitions of heinous but like, whatever....
    //this function updates
    //returns the new instance mask
    //from the original mask, relabel the pixel values using the gt tracks
    void associateDetectedBBWithObject(const cv::Mat& instance_mask, FrameId frame_id, cv::Mat& relabelled_mask) const {
        const GroundTruthInputPacket& gt_packet = getGtPacket(frame_id);
        ObjectIds object_ids = vision_tools::getObjectLabels(instance_mask);
        const size_t n = object_ids.size();

        instance_mask.copyTo(relabelled_mask);

        if(n == 0) {
            return;
        }


        //sort kps map into a map of clusters -> [kps....]
        const LandmarksMap& kps_map = left_landmarks_map_.at(frame_id);
        //cluster id (track id -> all keypoints for that cluster)
        std::set<int> cluster_id_set;
        gtsam::FastMap<int, Keypoints> clustered_kps;
        for(const auto&[landmark_id, kp] : kps_map) {
            const auto cluster_id = landmark_mapping_.at(landmark_id);
            cluster_id_set.insert(cluster_id);
            if(!clustered_kps.exists(cluster_id)) {
                clustered_kps.insert2(cluster_id, Keypoints());
            }
            clustered_kps.at(cluster_id).push_back(kp);
        }

        std::vector<int> cluster_ids(cluster_id_set.begin(), cluster_id_set.end());

        //want to assign object ids to clusters
        const size_t m = cluster_ids.size();
        CHECK_EQ(m, clustered_kps.size());

        LOG(INFO) << n <<" " << m;

        Eigen::MatrixXd cost;
        cost.resize(n, m);

        for(size_t rows = 0; rows < n; rows++) {
            ObjectId object_id = object_ids.at(rows);

            // get the binary mask just for this object, we will use this too find a bounding box
            //and count how many kp's fall into this mask
            const cv::Mat object_mask = relabelled_mask == object_id;

            cv::Rect bb;
            if(!vision_tools::findObjectBoundingBox(instance_mask, object_id, bb)) {
                VLOG(10) << "No bb could be found for detected object " << object_id << " at frame " << frame_id << ". Skipping association and removing object from mask";
                relabelled_mask.setTo(cv::Scalar(0), object_mask);
                continue;
            }


            for(size_t cols = 0; cols < m; cols++) {
                int cluster_id = cluster_ids.at(cols);
                const Keypoints& kps = clustered_kps.at(cluster_id);

                //(inverse) cost for the object id with this cluster id
                //hack - make really small number so we divide by zero!!
                double object_count = 0.000001;
                for(const auto& kp : kps) {
                    cv::Point2f pt(utils::gtsamPointToCv(kp));
                    if(bb.contains(pt)) {
                        object_count++;
                    }
                }
                cost(rows, cols) = object_count;
            }
        }

        //apply scaling to the costs to turn the cost function from an argmax to an argmin problem
        //which the hungrian problem sovles
        //ie we want the assignment that maximuses the number of kp's in the object insancec
        Eigen::MatrixXd loged_costs = cost.unaryExpr([](double x) { return 1.0/x * 10; });
        Eigen::VectorXi assignment;
        internal::HungarianAlgorithm().solve(loged_costs, assignment);

        // for(size_t i = 0; i < assignment.size(); i++) {
        //     int j = assignment[i];

        //     //the i-th object is assignment to the j-th cluster
        //     ObjectId object_id = object_ids.at(i);
        //     int assigned_custer = cluster_ids.at(j);

        //     const cv::Mat object_mask = relabelled_mask == object_id;
        //     relabelled_mask.setTo(cv::Scalar(assigned_custer), object_mask);
        // }
        ObjectIds old_labels = object_ids;
        ObjectIds new_labels;
        for(size_t i = 0; i < assignment.size(); i++) {
            int j = assignment[i];

            // //the i-th object is assignment to the j-th cluster
            // ObjectId instance_id = instance_ids.at(i);
            int assigned_custer = cluster_ids.at(j);
            new_labels.push_back(assigned_custer);

            // LOG(INFO) << "Relabelled " << instance_id << " to " << assigned_object_id;

            // const cv::Mat object_mask = relabelled_mask == instance_id;
            // relabelled_mask.setTo(cv::Scalar(assigned_object_id), object_mask);
        }

        vision_tools::relabelMasks(instance_mask, relabelled_mask, old_labels, new_labels);

    }


    void setGroundTruthPacket(GroundTruthPacketMap& ground_truth_packets) {
        //load the poses
        //first line contains camera pose
        //consequative lines contain object ids which are indexed from 0, but we will index from 1
        //expect the file name to give us the frame we are working with!!
        std::vector<std::filesystem::path> files_in_directory = getAllFilesInDir(pose_folder_path_);

        gtsam::Pose3 initial_pose = gtsam::Pose3::Identity();
        bool initial_frame_set = false;

        for (const std::filesystem::path& pose_file_path : files_in_directory) {

            //this should be the file name which we will then decompose into the frame id
            //e.g. .../pose/0000.txt -> filename = 0000 -> frame id = int(0)
            const std::string file_name = pose_file_path.filename();
            const int frame =  std::stoi(file_name);
            const gtsam::Pose3Vector poses = processPoseFile(pose_file_path, ground_truth_packets);

            //check we've never processed this frame before
            CHECK(!ground_truth_packets.exists(frame));

            if(frame > 0) {
                //check we have the previous frame!! (ie.e we process in order!)
                CHECK(ground_truth_packets.exists(frame-1));
            }

            GroundTruthInputPacket gt_packet;
            gt_packet.frame_id_ = frame;
            gt_packet.timestamp_ = getTimestamp(frame);
            // Quoting the dataset home page:
            // For each trajectory file under pose/ directory, each line in each file represents
            // the pose of each cluster except for the first line - that line represents the camera pose.
            for(size_t i = 0; i < poses.size(); i++) {

                //pose is in world coordinate convention so we put into camera convention (the camera_to_world.inverse())
                // gtsam::Pose3 pose = camera_to_world.inverse() * poses.at(i);
                gtsam::Pose3 pose = poses.at(i);

                const static gtsam::Pose3 camera_to_world(gtsam::Rot3::RzRyRx(M_PI_2, 0, M_PI_2), gtsam::traits<gtsam::Point3>::Identity());

                // LOG(INFO) << pose;

                // pose = camera_to_world.inverse() * pose;

                // LOG(INFO) << pose;


                if(!initial_frame_set) {
                    //expect very first pose (first frame, and first pose within that frame) to be the first camera pose
                    initial_pose = pose;
                    initial_frame_set = true;
                     LOG(INFO) << pose;
                }

                // offset initial pose so we start at "0, 0, 0"
                pose = initial_pose.inverse() * pose;


                if(i == 0) {
                    gt_packet.X_world_ = pose;
                }
                else {
                    ObjectPoseGT object_pose_gt;
                    object_pose_gt.frame_id_ = frame;
                    object_pose_gt.object_id_ = i;
                    object_pose_gt.L_world_ = pose;
                    object_pose_gt.L_camera_ = gt_packet.X_world_.inverse() * object_pose_gt.L_world_;
                    gt_packet.object_poses_.push_back(object_pose_gt);
                }
            }

            if(frame > 0) {
                const GroundTruthInputPacket& previous_gt_packet = ground_truth_packets.at(frame - 1);
                gt_packet.calculateAndSetMotions(previous_gt_packet);
            }

            ground_truth_packets.insert2(frame, gt_packet);

        }

    }
    gtsam::Pose3Vector processPoseFile(const std::string& pose_file, GroundTruthPacketMap& ground_truth_packets) {
        std::ifstream fstream;
        fstream.open(pose_file, std::ios::in);
        if(!fstream) {
            throw std::runtime_error("Could not open file " + pose_file + " when trying to load Cluster Slam pose information!");
        }

        //expected size of each line (3 translation components + 4 for quaternion)
        constexpr static size_t kExpectedSize = 7u;

        //pose string expected to be in form x,y,z, qx, qy, qz, qw
        auto construct_pose =[](const std::vector<std::string>& pose_string) -> gtsam::Pose3 {
            const double x = std::stod(pose_string.at(0));
            const double y = std::stod(pose_string.at(1));
            const double z = std::stod(pose_string.at(2));

            const double qx = std::stod(pose_string.at(3));
            const double qy = std::stod(pose_string.at(4));
            const double qz = std::stod(pose_string.at(5));
            const double qw = std::stod(pose_string.at(6));

            return gtsam::Pose3(
                gtsam::Rot3(qw, qx, qy, qz),
                gtsam::Point3(x, y, z)
            );
        };

        gtsam::Pose3Vector poses;
        while (!fstream.eof()) {
            std::vector<std::string> split_line;
            if(!getLine(fstream, split_line)) continue;

            CHECK_EQ(split_line.size(), kExpectedSize) << "Line present in pose file does not have the right length - " << container_to_string(split_line);
            poses.push_back(construct_pose(split_line));
        }
        return poses;
    }


private:
    const std::string left_images_folder_path_;
    const std::string optical_flow_folder_path_;
    const std::string pose_folder_path_;
    const std::string left_landmarks_folder_;
    const std::string right_images_folder_path_;
    const std::string instance_masks_folder_;
    const std::string landmark_mapping_file_path_;
    const std::string intrinsics_file_path_;

    std::vector<std::string> left_rgb_image_paths_; //index from 0 to match the naming convention of the dataset
    std::vector<std::string> right_rgb_image_paths_;
    std::vector<std::string> optical_flow_image_paths_;
    std::vector<std::string> instance_masks_image_paths_;
    LandmarksMapPerFrame left_landmarks_map_;

    //instance masks per frame - we need a chace of them as we're going to change
    //the index of each object to match the gt labels!!
    gtsam::FastMap<FrameId, cv::Mat> instance_masks_;

    //Landmark to cluster id mapping
    //cluster should be the actual track
    //the landmark id is the key in LandmarksMap
    gtsam::FastMap<int, int> landmark_mapping_;

    //set by setIntrinsics()
    gtsam::Matrix33 K_cam_1;
    gtsam::Matrix33 K_cam_2;
    gtsam::Matrix34 projection_matrix_cam1_;
    gtsam::Matrix34 projection_matrix_cam2_;
    CameraParams left_camera_params_;

    StereoCamera::Ptr stereo_camera_;
    StereoMatcher::Ptr stereo_matcher_;

    GroundTruthPacketMap ground_truth_packets_;
    size_t dataset_size_; //set in setGroundTruthPacket. Reflects the number of files in the /optical_flow folder which is one per frame

};

struct ClusterSlamTimestampLoader : public TimestampBaseLoader {

    ClusterSlamAllLoader::Ptr loader_;

    ClusterSlamTimestampLoader(ClusterSlamAllLoader::Ptr loader) : loader_(CHECK_NOTNULL(loader)) {}
    std::string getFolderName() const override { return ""; }

    size_t size() const override {
        return loader_->size();
    }

    double getItem(size_t idx) override {
        return loader_->getTimestamp(idx);
    }
};



ClusterSlamDataLoader::ClusterSlamDataLoader(const fs::path& dataset_path) : ClusterSlamDatasetProvider(dataset_path)
{
    LOG(INFO) << "Starting ClusterSlamDataLoader with path" << dataset_path;

    //this would go out of scope but we capture it in the functional loaders
    auto loader = std::make_shared<ClusterSlamAllLoader>(dataset_path);
    auto timestamp_loader = std::make_shared<ClusterSlamTimestampLoader>(loader);

    left_camera_params_ = loader->getLeftCameraParams();

    CHECK(getCameraParams());

    auto rgb_loader = std::make_shared<FunctionalDataFolder<cv::Mat>>(
        [loader](size_t idx) {
            return loader->getRGB(idx);
        }
    );

    auto optical_flow_loader = std::make_shared<FunctionalDataFolder<cv::Mat>>(
        [loader](size_t idx) {
            return loader->getOpticalFlow(idx);
        }
    );

    auto depth_loader = std::make_shared<FunctionalDataFolder<cv::Mat>>(
        [loader](size_t idx) {
            return loader->getDepthImage(idx);
        }
    );

    auto instance_mask_loader = std::make_shared<FunctionalDataFolder<cv::Mat>>(
        [loader](size_t idx) {
            return loader->getInstanceMask(idx);
        }
    );


    auto gt_loader = std::make_shared<FunctionalDataFolder<GroundTruthInputPacket>>(
        [loader](size_t idx) {
            return loader->getGtPacket(idx);
        }
    );

    this->setLoaders(
        timestamp_loader,
        rgb_loader,
        optical_flow_loader,
        depth_loader,
        instance_mask_loader,
        gt_loader
    );

     auto callback = [&](size_t frame_id,
        Timestamp timestamp,
        cv::Mat rgb,
        cv::Mat optical_flow,
        cv::Mat depth,
        cv::Mat instance_mask,
        GroundTruthInputPacket gt_object_pose_gt) -> bool
    {
        CHECK_EQ(timestamp, gt_object_pose_gt.timestamp_);

        CHECK(ground_truth_packet_callback_);
        if(ground_truth_packet_callback_) ground_truth_packet_callback_(gt_object_pose_gt);

        ImageContainer::Ptr image_container = nullptr;
        image_container = ImageContainer::Create(
                timestamp,
                frame_id,
                ImageWrapper<ImageType::RGBMono>(rgb),
                ImageWrapper<ImageType::Depth>(depth),
                ImageWrapper<ImageType::OpticalFlow>(optical_flow),
                ImageWrapper<ImageType::MotionMask>(instance_mask));
        CHECK(image_container);
        CHECK(image_container_callback_);
        if(image_container_callback_) image_container_callback_(image_container);
        return true;
    };

    this->setCallback(callback);
}




}
````

## File: dataprovider/DataInterfacePipeline.cc
````
/*
 *   Copyright (c) 2023 Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include "dynosam/dataprovider/DataInterfacePipeline.hpp"

#include <glog/logging.h>

#include "dynosam/utils/Numerical.hpp"
#include "dynosam/utils/SafeCast.hpp"
#include "dynosam/utils/Statistics.hpp"

namespace dyno {

DataInterfacePipeline::DataInterfacePipeline(bool parallel_run)
    : MIMO("data-interface"), parallel_run_(parallel_run) {}

void DataInterfacePipeline::shutdownQueues() {
  packet_queue_.shutdown();
  // call the virtual shutdown method for the derived dataprovider module
  this->onShutdown();
}

FrontendInputPacketBase::ConstPtr DataInterfacePipeline::getInputPacket() {
  if (isShutdown()) {
    return nullptr;
  }

  utils::StatsCollector queue_size_stats("data-interface_queue_size #");
  queue_size_stats.AddSample(packet_queue_.size());

  bool queue_state;
  ImageContainer::Ptr packet = nullptr;

  if (parallel_run_) {
    queue_state = packet_queue_.pop(packet);
  } else {
    queue_state = packet_queue_.popBlocking(packet);
  }

  if (!queue_state) {
    //  LOG_EVERY_N(WARNING, 10)
    //     << "Module: " << MIMO::module_name_ << " - queue is down";
    return nullptr;
  }

  CHECK(packet);

  GroundTruthInputPacket::Optional ground_truth;
  if (ground_truth_packets_.find(packet->getFrameId()) !=
      ground_truth_packets_.end()) {
    VLOG(5) << "Gotten ground truth packet for frame id "
            << packet->getFrameId();
    ground_truth = ground_truth_packets_.at(packet->getFrameId());
  }

  return std::make_shared<FrontendInputPacketBase>(packet, ground_truth);
}

bool DataInterfacePipeline::hasWork() const {
  return !packet_queue_.empty() && !packet_queue_.isShutdown();
}

bool DataInterfacePipelineImu::getTimeSyncedImuMeasurements(
    const Timestamp& timestamp, ImuMeasurements* imu_meas) {
  CHECK_NOTNULL(imu_meas);
  CHECK_LT(timestamp_last_sync_, timestamp)
      << "Timestamps out of order:\n"
      << " - Last Frame Timestamp = " << timestamp_last_sync_ << '\n'
      << " - Current Timestamp = " << timestamp;

  if (imu_buffer_.size() == 0) {
    VLOG(1) << "No IMU measurements available yet, dropping this frame.";
    return false;
  }

  // Extract imu measurements between consecutive frames.
  if (dyno::fpEqual(timestamp_last_sync_, 0.0)) {
    // TODO(Toni): wouldn't it be better to get all IMU measurements up to
    // this
    // timestamp? We should add a method to the IMU buffer for that.
    VLOG(1) << "Skipping first frame, because we do not have a concept of "
               "a previous frame timestamp otherwise.";
    timestamp_last_sync_ = timestamp;
    return false;
  }

  ThreadsafeImuBuffer::QueryResult query_result =
      ThreadsafeImuBuffer::QueryResult::kDataNeverAvailable;
  bool log_error_once = true;
  while (!MIMO::isShutdown() &&
         (query_result = imu_buffer_.getImuDataInterpolatedUpperBorder(
              timestamp_last_sync_, timestamp, &imu_meas->timestamps_,
              &imu_meas->acc_gyr_)) !=
             ThreadsafeImuBuffer::QueryResult::kDataAvailable) {
    VLOG(1) << "No IMU data available. Reason:\n";
    switch (query_result) {
      case ThreadsafeImuBuffer::QueryResult::kDataNotYetAvailable: {
        if (log_error_once) {
          LOG(WARNING) << "Waiting for IMU data...";
          log_error_once = false;
        }
        continue;
      }
      case ThreadsafeImuBuffer::QueryResult::kQueueShutdown: {
        LOG(WARNING)
            << "IMU buffer was shutdown. Shutting down DataInterfacePipeline.";
        MIMO::shutdown();
        return false;
      }
      case ThreadsafeImuBuffer::QueryResult::kDataNeverAvailable: {
        LOG(WARNING)
            << "Asking for data before start of IMU stream, from timestamp: "
            << timestamp_last_sync_ << " to timestamp: " << timestamp;
        // Ignore frames that happened before the earliest imu data
        timestamp_last_sync_ = timestamp;
        return false;
      }
      case ThreadsafeImuBuffer::QueryResult::kTooFewMeasurementsAvailable: {
        LOG(WARNING) << "No IMU measurements here, and IMU data stream already "
                        "passed this time region"
                     << "from timestamp: " << timestamp_last_sync_
                     << " to timestamp: " << timestamp;
        return false;
      }
      case ThreadsafeImuBuffer::QueryResult::kDataAvailable: {
        LOG(FATAL) << "We should not be inside this while loop if IMU data is "
                      "available...";
        return false;
      }
    }
  }
  timestamp_last_sync_ = timestamp;

  VLOG(10) << "////////////////////////////////////////// Creating packet!\n"
           << "STAMPS IMU rows : \n"
           << imu_meas->timestamps_.rows() << '\n'
           << "STAMPS IMU cols : \n"
           << imu_meas->timestamps_.cols() << '\n'
           << "STAMPS IMU: \n"
           << imu_meas->timestamps_ << '\n'
           << "ACCGYR IMU rows : \n"
           << imu_meas->acc_gyr_.rows() << '\n'
           << "ACCGYR IMU cols : \n"
           << imu_meas->acc_gyr_.cols() << '\n'
           << "ACCGYR IMU: \n"
           << imu_meas->acc_gyr_;

  return true;
}

}  // namespace dyno
````

## File: dataprovider/DataProvider.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/dataprovider/DataProvider.hpp"
#include "dynosam/dataprovider/DataInterfacePipeline.hpp"

#include <functional>

#include <glog/logging.h>

namespace dyno {

DataProvider::DataProvider(DataInterfacePipeline* module) {
    CHECK_NOTNULL(module);
    registerImageContainerCallback(std::bind(&DataInterfacePipeline::fillImageContainerQueue, module, std::placeholders::_1));
    CHECK(image_container_callback_);

}

DataProvider::~DataProvider() {
    shutdown();
}

void DataProvider::shutdown() {
    LOG(INFO) << "Shutting down data provider and associated module";
    shutdown_ = true;
}


DataProvider::DataProvider(DataInterfacePipelineImu* module) : DataProvider(dynamic_cast<DataInterfacePipeline*>(module)) {
    CHECK_NOTNULL(module);
    registerImuMultiCallback(std::bind(
        static_cast<void(DataInterfacePipelineImu::*)(const ImuMeasurements&)>(&DataInterfacePipelineImu::fillImuQueue),
        module,
        std::placeholders::_1));
    registerImuSingleCallback(std::bind(
        static_cast<void(DataInterfacePipelineImu::*)(const ImuMeasurement&)>(&DataInterfacePipelineImu::fillImuQueue),
        module,
        std::placeholders::_1));

    CHECK(imu_multi_input_callback_);
    CHECK(imu_single_input_callback_);
}

} //dyno
````

## File: dataprovider/DataProviderFactory.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/dataprovider/DataProviderFactory.hpp"
#include "dynosam/dataprovider/DataProvider.hpp"

#include "dynosam/dataprovider/VirtualKittiDataProvider.hpp"
#include "dynosam/dataprovider/KittiDataProvider.hpp"
#include "dynosam/dataprovider/ClusterSlamDataProvider.hpp"
#include "dynosam/dataprovider/OMDDataProvider.hpp"
#include "dynosam/dataprovider/ProjectAriaDataProvider.hpp"

#include "dynosam/utils/YamlParser.hpp"

#include <glog/logging.h>

DEFINE_int32(starting_frame, -1, "Starting frame of the dataset. If -1 use the default which is the starting frame=0");
DEFINE_int32(ending_frame, -1, "Ending frame of the dataset. If -1 use the default which is the ending_frame=dataset_size");

namespace dyno {

DataProvider::Ptr DataProviderFactory::Create(const std::string& dataset_folder_path, const std::string& params_folder_path, DatasetType dataset_type) {
    if(dataset_type == DatasetType::KITTI) {
        LOG(INFO) << "Using KITTI dataset at path: " << dataset_folder_path;
        KittiDataLoader::Params params = KittiDataLoader::Params::fromYaml(params_folder_path);
        auto loader =  std::make_shared<KittiDataLoader>(dataset_folder_path, params);

        loader->setStartingFrame(FLAGS_starting_frame);
        loader->setEndingFrame(FLAGS_ending_frame);
        return loader;
    }
    else if(dataset_type == DatasetType::VIRTUAL_KITTI) {
        LOG(INFO) << "Using Virtual KITTI dataset at path: " << dataset_folder_path;
        VirtualKittiDataLoader::Params params = VirtualKittiDataLoader::Params::fromYaml(params_folder_path);
        auto loader = std::make_shared<VirtualKittiDataLoader>(dataset_folder_path, params);
        loader->setStartingFrame(FLAGS_starting_frame);
        loader->setEndingFrame(FLAGS_ending_frame);
        return loader;
    }
    else if(dataset_type == DatasetType::CLUSTER) {
        LOG(INFO) << "Using Cluster (SLAM) dataset at path: " << dataset_folder_path;
        auto loader = std::make_shared<ClusterSlamDataLoader>(dataset_folder_path);
        loader->setStartingFrame(FLAGS_starting_frame);
        loader->setEndingFrame(FLAGS_ending_frame);
        return loader;
    }
    else if (dataset_type == DatasetType::OMD) {
        LOG(INFO) << "Using Oxford Multi-motion Dataset dataset at path: " << dataset_folder_path;
        auto loader = std::make_shared<OMDDataLoader>(dataset_folder_path);
        loader->setStartingFrame(FLAGS_starting_frame);
        loader->setEndingFrame(FLAGS_ending_frame);
        return loader;
    }
    else if (dataset_type == DatasetType::ARIA) {
        LOG(INFO) << "Using ARIA dataset at path: " << dataset_folder_path;
        auto loader = std::make_shared<ProjectARIADataLoader>(dataset_folder_path);
        loader->setStartingFrame(FLAGS_starting_frame);
        loader->setEndingFrame(FLAGS_ending_frame);
        return loader;
    }
    else {
        throw std::runtime_error("Unable to construct Dataprovider - unknown dataset type: " + static_cast<int>(dataset_type));
    }
}


}
````

## File: dataprovider/DataProviderUtils.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/dataprovider/DataProviderUtils.hpp"
#include "dynosam/utils/OpenCVUtils.hpp"

#include "dynosam/frontend/vision/VisionTools.hpp" //for getObjectLabels
#include "dynosam/common/ImageTypes.hpp"


#include <filesystem>
#include <exception>
#include <fstream>

#include <boost/algorithm/string/split.hpp>
#include <boost/algorithm/string.hpp>

#include <glog/logging.h>

#include <opencv4/opencv2/opencv.hpp>

namespace dyno {

void throwExceptionIfPathInvalid(const std::string image_path) {
    namespace fs = std::filesystem;
    if(!fs::exists(image_path)) {
        throw std::runtime_error("Path does not exist: " + image_path);
    }
}

void loadRGB(const std::string& image_path, cv::Mat& img) {
    throwExceptionIfPathInvalid(image_path);
    img = cv::imread(image_path, cv::IMREAD_UNCHANGED);

}

void loadFlow(const std::string& image_path, cv::Mat& img) {
    throwExceptionIfPathInvalid(image_path);
    img = utils::readOpticalFlow(image_path);
}

void loadDepth(const std::string& image_path, cv::Mat& img) {
    throwExceptionIfPathInvalid(image_path);
    img = cv::imread(image_path, cv::IMREAD_UNCHANGED);
    img.convertTo(img, CV_64F);
}


void loadSemanticMask(const std::string& image_path, const cv::Size& size, cv::Mat& mask) {
    throwExceptionIfPathInvalid(image_path);
    CHECK(!size.empty());

    mask = cv::Mat(size, CV_32SC1);

    std::ifstream file_mask;
    file_mask.open(image_path.c_str());

    int count = 0;
    while (!file_mask.eof())
    {
        std::string s;
        getline(file_mask, s);
        if (!s.empty())
        {
            std::stringstream ss;
            ss << s;
            int tmp;
            for (int i = 0; i < mask.cols; ++i)
            {
                ss >> tmp;
                if (tmp != 0)
                {
                mask.at<int>(count, i) = tmp;
                }
                else
                {
                mask.at<int>(count, i) = 0;
                }
            }
            count++;
        }
    }

  file_mask.close();

}

void loadMask(const std::string& image_path, cv::Mat& mask) {
    throwExceptionIfPathInvalid(image_path);
    mask = cv::imread(image_path, cv::IMREAD_UNCHANGED);
    mask.convertTo(mask, CV_32SC1);
}


std::vector<std::filesystem::path> getAllFilesInDir(const std::string& folder_path) {
    std::vector<std::filesystem::path> files_in_directory;
    std::copy(std::filesystem::directory_iterator(folder_path), std::filesystem::directory_iterator(), std::back_inserter(files_in_directory));
    std::sort(files_in_directory.begin(), files_in_directory.end());
    return files_in_directory;

}


void loadPathsInDirectory(std::vector<std::string>& file_paths, const std::string& folder_path, const std::function<bool(const std::string&)>& condition) {
    std::function<bool(const std::string&)> impl_condition;
    if(condition) {
        impl_condition = condition;
    }
    else {
        //if no condition is provided, set condition to always return true; adding all the files found
        impl_condition = [](const std::string&) -> bool { return true; };
    }

    auto files_in_directory = getAllFilesInDir(folder_path);
    for (const std::string file_path : files_in_directory) {
        throwExceptionIfPathInvalid(file_path);

        //if condition is true, add
        if(impl_condition(file_path)) file_paths.push_back(file_path);
    }
}


void removeStaticObjectFromMask(const cv::Mat& instance_mask, cv::Mat& motion_mask, const GroundTruthInputPacket& gt_packet) {
    try {
        ImageType::SemanticMask::validate(instance_mask);
    }
    catch (const InvalidImageTypeException& e) {
        throw DynosamException("Input mask to removeStaticObjectFromMask as it"
            "did not meet the requirements of ImageType::SemanticMask. Error: " + std::string(e.what()));
    }

    ObjectIds object_ids = vision_tools::getObjectLabels(instance_mask);

    instance_mask.copyTo(motion_mask);

    const FrameId frame_id = gt_packet.frame_id_;

    //collect only moving labels
    std::vector<int> moving_labels;
    for(const ObjectPoseGT& object_pose_gt : gt_packet.object_poses_) {
        // LOG_IF(WARNING, !object_pose_gt.motion_info_)
        //     << "Object Pose GT (object " << object_pose_gt.object_id_ << ", frame " << object_pose_gt.frame_id_ << " does not have motion info set! Cannot determine of object is moving or not!";

        if(!object_pose_gt.motion_info_) {
            LOG(WARNING) << "Object Pose GT (object " << object_pose_gt.object_id_ << ", frame " << object_pose_gt.frame_id_ << " does not have motion info set! Cannot determine of object is moving or not!";
            continue;
        }

        const auto& motion_info = *object_pose_gt.motion_info_;
        const ObjectId& object_id = object_pose_gt.object_id_;
        if(motion_info.is_moving_) {
            moving_labels.push_back(object_id);
        }

        //this is just a sanity (debug) check to ensure all the labels in the image
        //match up with the ones we have already collected in the ground truth packet
        const auto& it = std::find(object_ids.begin(), object_ids.end(), object_id);
        CHECK(it != object_ids.end()) << "Object id " << object_id << " appears in gt packet but"
            "is not in mask when loading motion mask for Virtual Kitti at frame " << frame_id;

    }

    //iterate over each object and if not moving remove
    for (int i = 0; i < motion_mask.rows; i++)
    {
        for (int j = 0; j < motion_mask.cols; j++)
        {

            int label = motion_mask.at<int>(i, j);
            if(label == 0) {
                continue;
            }

            //check if label is in moving labels. if not, make zero!
            auto it = std::find(moving_labels.begin(), moving_labels.end(), label);
            if(it == moving_labels.end()) {
                motion_mask.at<int>(i, j) = 0;
            }
        }
    }

}


 std::vector<std::string> trimAndSplit(const std::string& input, const std::string& delimiter) {
    std::string trim_input = boost::algorithm::trim_right_copy(input);
    std::vector<std::string> split_line;
    boost::algorithm::split(split_line, trim_input, boost::is_any_of(delimiter));
    return split_line;
}


bool getLine(std::ifstream& fstream, std::vector<std::string>& split_lines) {
    std::string line;
    getline(fstream, line);

    split_lines.clear();

    if(line.empty()) return false;

    split_lines = trimAndSplit(line);
    return true;
}

} //dyno
````

## File: dataprovider/DatasetLoader.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/dataprovider/DatasetLoader.hpp"
#include "dynosam/dataprovider/DataProviderUtils.hpp"

namespace dyno {

std::string RGBDataFolder::getFolderName() const {
    return "image_0";
}

cv::Mat RGBDataFolder::getItem(size_t idx) {
    std::stringstream ss;
    ss << std::setfill('0') << std::setw(6) << idx;
    const std::string file_path = (std::string)getAbsolutePath()  + "/" + ss.str() + ".png";

    cv::Mat rgb;
    loadRGB(file_path, rgb);

    CHECK(!rgb.empty());
    return rgb;
}


std::string OpticalFlowDataFolder::getFolderName() const {
    return "flow";
}

cv::Mat OpticalFlowDataFolder::getItem(size_t idx) {
    std::stringstream ss;
    ss << std::setfill('0') << std::setw(6) << idx;
    const std::string file_path = (std::string)getAbsolutePath()  + "/" + ss.str() + ".flo";

    cv::Mat flow;
    loadFlow(file_path, flow);

    CHECK(!flow.empty());
    return flow;
}



cv::Mat DepthDataFolder::getItem(size_t idx) {
  std::stringstream ss;
  ss << std::setfill('0') << std::setw(6) << idx;
  const std::string file_path = (std::string)getAbsolutePath()  + "/" + ss.str() + ".png";

  cv::Mat depth;
  loadDepth(file_path, depth);

  CHECK(!depth.empty());
  return depth;
}


cv::Mat SegMaskFolder::getItem(size_t idx) {
  std::stringstream ss;
  ss << std::setfill('0') << std::setw(6) << idx;
  const std::string file_path = (std::string)getAbsolutePath()  + "/" + ss.str() + ".txt";

  const cv::Size expected_mask_size = rgb_data_folder_->getItem(idx).size();

  cv::Mat mask;
  loadSemanticMask(file_path, expected_mask_size, mask);

  CHECK(!mask.empty());
  return mask;
}


std::string TimestampFile::getFolderName() const {
    return "times.txt";
}

double TimestampFile::getItem(size_t idx) {
    return times.at(idx);
}

void TimestampFile::onPathInit() {
    std::ifstream times_stream((std::string)getAbsolutePath(), std::ios::in);

    while (!times_stream.eof())
  {
    std::string s;
    getline(times_stream, s);
    if (!s.empty())
    {
      std::stringstream ss;
      ss << s;
      double t;
      ss >> t;
      times.push_back(t);
    }
  }
  times_stream.close();


}


} //dyno
````

## File: dataprovider/DatasetProvider.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/dataprovider/DatasetProvider.hpp"

#include <glog/logging.h>

namespace dyno {


// PlaybackGui::PlaybackGui()
//     :   pause_button_rect_(20, 20, 100, 50),
//         resume_button_rect_(140, 20, 100, 50),
//         frame_(200, 300, CV_8UC3),
//         window_("Screen")
//     {
//         frame_.setTo(cv::Scalar(255, 255, 255)); // White background
//         window_.registerMouseCallback(onMouseCallback,
//                                                 (void*)this );
//         window_.registerKeyboardCallback([](const cv::viz::KeyboardEvent& event,
//                                             void* t) { LOG(INFO) << event.action; },
//                                                (void*)this );

//         // cv::setMouseCallback( "image", onMouseCallback, (void*)this );
//     }

// void PlaybackGui::draw() {
//     // drawButtons();
//     // cv::Rect rect = cv::Rect(0, 0, frame_.size().width, frame_.size().height);
//     // window_.showWidget("Screen", cv::viz::WImageOverlay(frame_, rect));
//     LOG(INFO) << "Spinning";
//     window_.spinOnce(1, true);
//     // cv::imshow("Dialog Box", frame_);
//     // cv::waitKey(0);
// }

// void PlaybackGui::onMouseCallback(const cv::viz::MouseEvent & event, void* v_gui) {
//     PlaybackGui* gui = (PlaybackGui*)v_gui;
//     CHECK_NOTNULL(gui);

//     LOG(INFO) << "Click";

//     // if (event == cv::EVENT_LBUTTONDOWN) {
//     //     if (gui->pause_button_rect_.contains(cv::Point(x, y))) {
//     //         LOG(INFO) << "PAUSED!";
//     //         // paused = true;
//     //     } else if (gui->resume_button_rect_.contains(cv::Point(x, y))) {
//     //         // paused = false;
//     //         LOG(INFO) << "RESUME!";
//     //     }
//     // }

// }


// void PlaybackGui::drawButtons() {
//      // Draw pause button
//     cv::rectangle(frame_, pause_button_rect_, cv::Scalar(0, 255, 0), -1);
//     cv::putText(frame_, "Pause", cv::Point(pause_button_rect_.x + 10, pause_button_rect_.y + 30),
//                 cv::FONT_HERSHEY_SIMPLEX, 1, cv::Scalar(0, 0, 0), 2);

//     // Draw resume button
//     cv::rectangle(frame_, resume_button_rect_, cv::Scalar(0, 0, 255), -1);
//     cv::putText(frame_, "Resume", cv::Point(resume_button_rect_.x + 10, resume_button_rect_.y + 30),
//                 cv::FONT_HERSHEY_SIMPLEX, 1, cv::Scalar(0, 0, 0), 2);
// }

} //dyno
````

## File: dataprovider/OMDDataProvider.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/dataprovider/OMDDataProvider.hpp"

#include "dynosam/utils/CsvParser.hpp"
#include "dynosam/dataprovider/DataProviderUtils.hpp"
#include "dynosam/utils/OpenCVUtils.hpp"
#include "dynosam/visualizer/ColourMap.hpp"
#include "dynosam/utils/GtsamUtils.hpp"

#include "dynosam/frontend/vision/VisionTools.hpp" //for getObjectLabels
#include "dynosam/common/Algorithms.hpp"

#include "dynosam/pipeline/ThreadSafeTemporalBuffer.hpp"

#include <glog/logging.h>
#include <filesystem>

namespace dyno {

// class OMDAllLoader {

// public:
//     DYNO_POINTER_TYPEDEFS(OMDAllLoader)

//     OMDAllLoader(const std::string& file_path)
//     :   rgbd_folder_path_(file_path + "/rgbd"),
//         instance_masks_folder_(file_path + "/instance_masks"),
//         optical_flow_folder_path_(file_path + "/optical_flow"),
//         vicon_file_path_(file_path + "/vicon.csv"),
//         // kalibr_file_path_(file_path + "/manufacturer.yaml"),
//         kalibr_file_path_(file_path + "/kalibr.yaml"),
//         vicon_calibration_file_path_(file_path + "/vicon.yaml")
//     {
//         throwExceptionIfPathInvalid(rgbd_folder_path_);
//         throwExceptionIfPathInvalid(instance_masks_folder_);
//         throwExceptionIfPathInvalid(optical_flow_folder_path_);
//         throwExceptionIfPathInvalid(vicon_file_path_);
//         throwExceptionIfPathInvalid(kalibr_file_path_);
//         throwExceptionIfPathInvalid(vicon_calibration_file_path_);

//         //first load images and size
//         //the size will be used as a refernce for all other loaders
//         //size is number of images
//         //we use flow to set the size as for this dataset, there will be one less
//         //optical flow as the flow ids index t to t+1 (which is gross, I know!!)
//         //TODO: code and comments replicated in ClusterSlamDataProvider
//         loadFlowImagesAndSize(optical_flow_image_paths_, dataset_size_);

//         //load rgb and aligned depth image file paths into rgb_image_paths_ and aligned_depth_image_paths_
//         //does some sanity checks
//         //as well as timestamps
//         loadRGBDImagesAndTime();

//         //load instance masks
//         loadPathsInDirectory(instance_masks_image_paths_, instance_masks_folder_);
//         //remove the ones up to the dataset size
//         instance_masks_image_paths_.resize(dataset_size_);

//         //want one less rgb image than optical flow
//         CHECK_EQ(optical_flow_image_paths_.size() - 1, rgb_image_paths_.size());
//         CHECK_EQ(instance_masks_image_paths_.size(), optical_flow_image_paths_.size());

//         setGroundTruthPacketFromVicon(ground_truth_packets_);

//         setIntrisicsAndTransforms();
//     }

//     size_t size() const {
//         return dataset_size_;
//     }

//     cv::Mat getRGB(size_t idx) const {
//         CHECK_LT(idx, rgb_image_paths_.size());

//         cv::Mat rgb;
//         loadRGB(rgb_image_paths_.at(idx), rgb);
//         CHECK(!rgb.empty());

//         //this set of images are loaded as 8UC4
//         // CHECK_EQ(rgb.type(), CV_8UC4) << "Somehow the image type has changed...";
//         // rgb.convertTo(rgb, CV_8UC3);

//         //debug check -> draw keypoints on image!
//         // const LandmarksMap& kps_map = left_landmarks_map_.at(idx);

//         // for(const auto&[landmark_id, kp] : kps_map) {
//         //     const auto cluster_id = landmark_mapping_.at(landmark_id);

//         //     cv::Point2f pt(utils::gtsamPointToCv(kp));
//         //     utils::drawCircleInPlace(rgb, pt, ColourMap::getObjectColour(cluster_id));
//         // }

//         return rgb;
//     }

//     cv::Mat getOpticalFlow(size_t idx) const {
//         CHECK_LT(idx,optical_flow_image_paths_.size());

//         cv::Mat flow;
//         loadFlow(optical_flow_image_paths_.at(idx), flow);
//         CHECK(!flow.empty());
//         return flow;
//     }

//     cv::Mat getInstanceMask(size_t idx) const {
//         CHECK_LT(idx, rgb_image_paths_.size());
//         CHECK_LT(idx, dataset_size_);

//         cv::Mat mask, relabelled_mask;
//         loadMask(instance_masks_image_paths_.at(idx), mask);
//         CHECK(!mask.empty());

//         associateGTWithObject(mask, getDepthImage(idx), idx, relabelled_mask);

//         // cv::Mat mask_viz = ImageType::SemanticMask::toRGB(relabelled_mask);

//         // cv::imshow("Relabelled mask", mask_viz);
//         // cv::waitKey(1);

//         return relabelled_mask;
//     }

//     cv::Mat getDepthImage(size_t idx) const {
//         CHECK_LT(idx, aligned_depth_image_paths_.size());

//         cv::Mat depth;
//         loadDepth(aligned_depth_image_paths_.at(idx), depth);
//         // loadRGB(aligned_depth_image_paths_.at(idx), depth);
//         CHECK(!depth.empty());
//         // CHECK(depth.type() == CV_16UC1);
//         // depth.convertTo(depth, CV_64F);

//         //The D435 publishes depth in "16-bit unsigned integers in millimeter resolution."
//         //TODO: paramterise and check why this is different for OMD
//         //why is dyno-sam so different that it needs the baseline scale factor as well as the scaling term
//         //is this just depth to dispartiy?
//         //imD.at<float>(i,j) = mbf/(imD.at<float>(i,j)/mDepthMapFactor);

//          const cv::Mat K = rgbd_camera_params_.getCameraMatrix();
//         //expect D to be zeros
//         const cv::Mat D = rgbd_camera_params_.getDistortionCoeffs();
//         cv::Mat new_K;
//         new_K = cv::getOptimalNewCameraMatrix(K, D, rgbd_camera_params_.imageSize(), 1);
//         cv::Mat undistorted_image = depth;
//         // cv::undistort(depth, undistorted_image, K, D, new_K);

//         for (int i = 0; i < undistorted_image.rows; i++)
//         {
//             for (int j = 0; j < undistorted_image.cols; j++)
//             {
//                 // LOG(INFO) << undistorted_image.at<double>(i, j);
//                 if (undistorted_image.at<double>(i, j) <= 0) {
//                     undistorted_image.at<double>(i, j) = 0;
//                 }
//                 else {
//                     // baseline = 50mm (0.05)
//                     //bfm = 0.05 * fx (618.3587036132812)
//                     undistorted_image.at<double>(i, j) =  (undistorted_image.at<double>(i, j) / 1000.0);
//                 }
//             }
//         }


//         // // CHECK_EQ()
//         // //check image sizes are the same!

//         // // LOG(INFO) << "Processing depth map";
//         // for (int i = 0; i < undistorted_image.rows; i++) {
//         //     for (int j = 0; j < undistorted_image.cols; j++) {
//         //         if (undistorted_image.at<double>(i,j)<0) {
//         //             undistorted_image.at<double>(i,j)=0;
//         //         }
//         //         else
//         //         {
//         //             undistorted_image.at<double>(i,j) = (undistorted_image.at<double>(i,j) / 1000.0);
//         //             // undistorted_image.at<double>(i,j) = 387.5744 /  (undistorted_image.at<double>(i,j) / 1000.0);
//         //         }

//         //         if(std::isinf(undistorted_image.at<double>(i,j))) {
//         //             undistorted_image.at<double>(i,j) = 0;
//         //         }
//         //     }
//         // }
//         // undistorted_image.convertTo(undistorted_image, CV_64F);
//         // LOG(INFO ) << "Finished processing";

//         // depth /= 1000.0;

//         // cv::Mat depth;
//         // const_disparity.copyTo(depth);
//         return undistorted_image;
//     }

//     const GroundTruthInputPacket& getGtPacket(size_t idx) const {
//         CHECK(ground_truth_packets_.exists(idx)) << " Idx not contained in gt packet: " << idx;
//         return ground_truth_packets_.at(idx);
//     }

//     const CameraParams& getLeftCameraParams() const {
//         return rgbd_camera_params_;
//     }

//     double getTimestamp(size_t idx) {
//         return static_cast<double>(times_.at(idx));
//     }


// private:
//     void loadFlowImagesAndSize(std::vector<std::string>& images_paths, size_t& dataset_size) {
//         std::vector<std::filesystem::path> files_in_directory = getAllFilesInDir(optical_flow_folder_path_);
//         dataset_size = files_in_directory.size();
//         CHECK_GT(dataset_size, 0);

//         for (const std::string file_path : files_in_directory) {
//             throwExceptionIfPathInvalid(file_path);
//             images_paths.push_back(file_path);
//         }
//    }

//    inline Timestamp toTimestamp(int time_sec, int time_nsec) const {
//     return static_cast<Timestamp>(time_sec) + static_cast<Timestamp>(time_nsec) / 1e+9;
//    }

//    void loadRGBDImagesAndTime() {
//         // from dataset contains *_aligned_depth.png and *_color.png
//         // need to construct both
//         auto image_files = getAllFilesInDir(rgbd_folder_path_);

//         //maps the image label prefix number with the two image paths - rgb (.first) and depth (.second)
//         //this enables us to ensure that each image number (which use the same prefix) has both
//         //rgb and depth images available in the dataset
//         gtsam::FastMap<int, std::pair<std::string, std::string>> image_label_to_path_pair;

//         const auto colour_substring = "color.png";
//         const auto depth_substring = "aligned_depth.png";

//         namespace fs = std::filesystem;

//         for(const std::string file_path : image_files) {
//             //extract image id
//             //this should be in the form (XXXX_aligned_depth or XXXX_color)
//             //get the filename from the file path (as the full file path contains "_" in other places
//             //e.g root/data/omm/swinging_4_unconstrained/rgbd/010707_color.png
//             //we just want the 010707_color.png componenent of the file path
//             auto split_lines = trimAndSplit(std::string(fs::path(file_path).filename()), "_");
//             int image_number = -1;

//             try {
//                 image_number = std::stoi(split_lines.at(0));
//             }
//             catch(const std::invalid_argument& ex) {
//                 //this fails on the csv file
//             }

//             //if image number extracted successfully
//             if(image_number != -1) {
//                  if(!image_label_to_path_pair.exists(image_number)) {
//                     image_label_to_path_pair.insert2(image_number, std::make_pair("", ""));
//                 }

//                 auto& file_path_pair = image_label_to_path_pair.at(image_number);

//                 if(file_path.find(colour_substring) != std::string::npos) {
//                     //is rgb image
//                     file_path_pair.first = file_path;
//                 }
//                 else if (file_path.find(depth_substring) != std::string::npos) {
//                     //is depth image
//                     file_path_pair.second = file_path;
//                 }
//             }
//             //special case - rgbd.csv file is inside this folder
//             //and gives timestamp per frame number
//             else if(file_path.find("rgbd.csv") != std::string::npos) {
//                 loadTimestampsFromRGBDCsvFile(file_path);
//             }
//             else {
//                 LOG(FATAL) << "Could not load rgbd image at path " << file_path;
//             }
//         }

//         //go through the map and add all image paths that have both image paths
//         //assume everything is ordered by FastMap
//         for(const auto& [image_num, file_path_pair] : image_label_to_path_pair) {
//             (void)image_num;
//             if(file_path_pair.first.empty() || file_path_pair.second.empty()) {
//                 continue;
//             }

//             rgb_image_paths_.push_back(file_path_pair.first);
//             aligned_depth_image_paths_.push_back(file_path_pair.second);

//             //want one less than the number of flow images
//             if(rgb_image_paths_.size() + 1 >= dataset_size_) {
//                 break;
//             }
//         }

//         CHECK_EQ(rgb_image_paths_.size(), aligned_depth_image_paths_.size());
//         CHECK_EQ(times_.size(), rgb_image_paths_.size());

//         LOG(INFO) << "Loaded " << rgb_image_paths_.size() << " rgbd images";
//    }

//    void loadTimestampsFromRGBDCsvFile(const std::string& file_path) {
//         //exepect a csv file with the header [frame_num, time_sec, time_nsec]
//         //inside the rgbd folder
//         //TODO: for now dont use a csv reader!!
//         std::ifstream file(file_path);
//         CsvReader csv_reader(file);
//         auto it = csv_reader.begin();
//         //skip header
//         ++it;

//         for(; it != csv_reader.end(); it++) {
//             const auto row = *it;
//             int frame_num = row.at<int>(0);
//             int time_sec = row.at<int>(1);
//             int time_nsec = row.at<int>(2);

//             Timestamp time = toTimestamp(time_sec, time_nsec);

//             times_.push_back(time);
//             timestamp_frame_map_.insert({frame_num, time});

//             //want one less than the number of flow images
//             if(times_.size() + 1 >= dataset_size_) {
//                 break;
//             }
//         }
//         LOG(INFO) << "Loaded " << times_.size() << " timestamps from RGBD csv file: " << file_path;

//    }

//    struct ViconPoseData {
//         Timestamp timestamp;
//         gtsam::Pose3 T_world_object; //or T_OA using the omd dataset notation, the inverse of what is given by the dataset (T_AO)
//         ObjectId object_id;
//    };


//     void setGroundTruthPacketFromVicon(GroundTruthPacketMap& ground_truth_packets) {
//         LOG(INFO) << "Loading object/camera pose gt from vicon: " << vicon_file_path_;
//         std::ifstream file(vicon_file_path_);
//         CsvReader csv_reader(file);
//         auto it = csv_reader.begin();
//         //skip header
//         ++it;

//         std::vector<ViconPoseData> vicon_pose_data;
//         gtsam::FastMap<FrameId, std::vector<ObjectPoseGT>> temp_object_poses;
//         //vicon timestamps that will replace the internal variable times_ (which are initally camera timestamps)
//         //should have the same length
//         std::vector<Timestamp> vicon_timestamps;

//         //  np.array([[1, 0, 0, 0],
//         // [0, 0, -1, 0],
//         // [0, 1, 0, 0],
//         // [0, 0, 0, 1]], dtype=np.float32)
//         // const gtsam::Pose3 T_cv_robotic = gtsam::Pose3(gtsam::Rot3::RzRyRx(M_PI/2.0, 0, M_PI/2.0), gtsam::traits<gtsam::Point3>::Identity());
//         const gtsam::Pose3 T_cv_robotic = gtsam::Pose3(
//             gtsam::Rot3(
//                 1, 0, 0,
//                 0, 0, -1,
//                 0, 1, 0),
//             gtsam::traits<gtsam::Point3>::Identity());
//         const gtsam::Pose3 T_omd_cv = gtsam::Pose3(gtsam::Rot3::RzRyRx(-M_PI/2.0, 0, 0), gtsam::traits<gtsam::Point3>::Identity()) ;


//         for(; it != csv_reader.end(); it++) {
//             const auto row = *it;
//             //Though the stereo camera, RGB-D camera, and IMU were recorded on the same machine,
//             //they are not hardware synchronized. The Vicon was recorded on a separate system with an unknown temporal offset
//             //and clock drift.
//             int time_sec = row.at<int>(0);
//             int time_nsec = row.at<int>(1);
//             Timestamp vicon_time = toTimestamp(time_sec, time_nsec);
//             //object is in the form boxX or sensor payload (the camera)
//             std::string object = row.at<std::string>(2);
//             ObjectId object_id;
//             if(object == "sensor_payload") {
//                 //camera
//                 object_id = 0;
//             }
//             else if(object.find("box") != std::string::npos) {
//                 //should be in boxX, so extract the last character as a number
//                 ObjectId box_id = object.back() - '0';
//                 CHECK_NE(box_id, 0); ///canot be 0, this is reserved for the camera
//                 object_id = box_id;
//             }
//             else {
//                 LOG(FATAL) << "Unknown object type: " << object;
//             }

//             double qx = row.at<double>(3);
//             double qy = row.at<double>(4);
//             double qz = row.at<double>(5);
//             double qw = row.at<double>(6);

//             double tx = row.at<double>(7);
//             double ty = row.at<double>(8);
//             double tz = row.at<double>(9);

//             //transformation from object to world frame where world is the vicon frame
//             //given as T_AO in dataset paper
//             gtsam::Pose3 T_object_world_robotic(gtsam::Rot3(qw, qx, qy, qz), gtsam::Point3(tx, ty, tz));
//             //world is the vicon frame but is defined in the ROBOTIC convention (but with y forward, becuase reasons...?)
//             //see figure.2. in paper where x is to the right and z is up
//             gtsam::Pose3 T_world_object_robotic = T_object_world_robotic;
//             //rotate the reference 90\deg around z to put into real robotic convention; z up , x forward, y left
//             //so apply a -90 transformation
//             // T_world_object_robotic = gtsam::Pose3(gtsam::Rot3::RzRyRx(0, 0, M_PI_2), gtsam::traits<gtsam::Point3>::Identity()) * T_world_object_robotic;

//             // //rotation that takes a transform from the robotic convention
//             //this is from actually the omd robotic convention
//             // const gtsam::Pose3 T_cv_robotic = gtsam::Pose3(
//             //     gtsam::Rot3(
//             //         1, 0, 0,
//             //         0, 0, -1,
//             //         0, 1, 0
//             //     ),
//             //     gtsam::traits<gtsam::Point3>::Identity());
//             // gtsam::Pose3 T_world_object_camera = T_cv_robotic * T_world_object_robotic * T_cv_robotic.inverse();


//             ViconPoseData vicon_data;
//             vicon_data.timestamp = vicon_time;
//             vicon_data.object_id = object_id;
//             vicon_data.T_world_object = T_object_world_robotic;
//             // vicon_data.T_world_object = T_world_object_robotic;
//             vicon_pose_data.push_back(vicon_data);

//         }

//         // //convert camera times to map for lookup
//         ThreadsafeTemporalBuffer<FrameId> camera_timestamp_buffer;
//         for(const auto& [frame_id, timestamp] : timestamp_frame_map_) {
//             camera_timestamp_buffer.addValue(timestamp, frame_id);
//         }
//         //dataset size is number of optical flow images - we want one less camera image!!
//         CHECK_EQ(camera_timestamp_buffer.size(), dataset_size_ - 1);
//         const auto earliest_camera_timestamp = camera_timestamp_buffer.getOldestTimestamp();
//         const auto latest_camera_timestamp = camera_timestamp_buffer.getNewestTimestamp();


//         //this is very slow!!!
//         for(size_t i = 0; i  < vicon_pose_data.size(); i++) {
//             const auto vicon_data = vicon_pose_data.at(i);
//         // for(const auto& vicon_data : vicon_pose_data) {
//             const ObjectId object_id = vicon_data.object_id;
//             const gtsam::Pose3& T_world_object = vicon_data.T_world_object;
//             const Timestamp& vicon_timestamp = vicon_data.timestamp;

//             //dont include if less than first or greater than last camera time
//             if (vicon_timestamp < earliest_camera_timestamp || vicon_timestamp > latest_camera_timestamp) {
//                 continue;
//             }

//             //get associated
//             //get closest camera timestamp to the vicon timestamp - this is the value we want to interpolate
//             //to as as this will be the timestamp of the frame we actually use
//             //the min delta should really be the delta between camera frames. According to the associated RA-L
//             //paper the rate of the RGBD sensor is 30Hz = 0.033
//             //the rate of the vicon system is 200Hz
//             constexpr static auto approx_rgbd_frame_rate = 0.033;
//             FrameId frame_id;
//             Timestamp camera_timestamp;
//             if(!camera_timestamp_buffer.getNearestValueToTime(vicon_timestamp, approx_rgbd_frame_rate, &frame_id, &camera_timestamp)) {
//                 continue;
//                 //TODO: throw warning?
//             }

//             // LOG(INFO) << std::setprecision(15) <<  camera_timestamp;

//             //get associated camera timestamp



//             if (fpEqual(camera_timestamp, vicon_timestamp)) {

//                 if(!ground_truth_packets.exists(frame_id)) {
//                     GroundTruthInputPacket gt_packet;
//                     gt_packet.frame_id_ = frame_id;
//                     gt_packet.timestamp_ = camera_timestamp;
//                     ground_truth_packets.insert2(frame_id, gt_packet);
//                 }

//                 if(!temp_object_poses.exists(frame_id)) {
//                     temp_object_poses.insert2(frame_id, std::vector<ObjectPoseGT>{});
//                 }

//                 GroundTruthInputPacket& gt_packet = ground_truth_packets.at(frame_id);

//                 std::vector<ObjectPoseGT>& tmp_object_vector = temp_object_poses.at(frame_id);

//                 if(object_id == 0) {
//                     //if object is sensor payload we need to apply extra transforms to get T_world_depthcam from T_world_object
//                     //in this case T_world_object = T_world_apparatus (T_OA) as the object is the apparatus frame measured from the vicon
//                     //T_world_depthcam =  T_world_apparatus * T_apparatus_leftstereo * T_leftstereo_depthcam
//                     //where T_apparatus_leftstereo is given from the vicon.yaml config
//                     //and T_leftstereo_depthcam is constructed from the kalibr config
//                     //this is actually the omd robotic convention
//                     // LOG(INFO) << T_cv_robotic;
//                     //apply frame convention change to cancel out the frame convention change that is inbuilt into T_apparatus_rgbd_
//                     //specifically this happens in the T_apparatus_left
//                     // gt_packet.X_world_ = T_world_object * T_cv_robotic * T_apparatus_rgbd_;
//                     // gt_packet.X_world_ = T_world_object * T_apparatus_rgbd_;
//                     // gt_packet.X_world_ = T_leftstereo_depthcam_.inverse() * T_apparatus_left_.inverse() * T_world_object;
//                     // gt_packet.X_world_ = T_leftstereo_depthcam_.inverse() * T_apparatus_left_.inverse() * T_world_object;

//                     //T_DO (relates rgbD camera to the origin frame O)
//                     //T_DO = T_AD^{-1} * T_AO
//                     // gt_packet.X_world_ = T_apparatus_rgbd_.inverse() * T_world_object;
//                     // gt_packet.X_world_ =  T_apparatus_rgbd_.inverse() * T_world_object;
//                     gt_packet.X_world_ =  T_world_object * T_apparatus_rgbd_;
//                     gt_packet.X_world_ = T_cv_robotic * gt_packet.X_world_ * T_cv_robotic.inverse();

//                     // gt_packet.X_world_ = T_world_object * T_apparatus_rgbd_;
//                     //  gt_packet.X_world_ = T_world_object;
//                      //now in camera convention!!!
//                     // gt_packet.X_world_ = camera_to_world.inverse() * gt_packet.X_world_;
//                 }
//                 else {
//                     ObjectPoseGT object_pose_gt;
//                     object_pose_gt.frame_id_ = frame_id;
//                     object_pose_gt.object_id_ = object_id;
//                     //if this is object, the transform is the object as seen in the vicon frame
//                     //we want it in the depth sensor frame
//                     //T_world_object -> T_vicon_object
//                     object_pose_gt.L_world_ =  T_cv_robotic * T_world_object * T_cv_robotic.inverse();
//                     // object_pose_gt.L_world_ = camera_to_world.inverse() * object_pose_gt.L_world_;
//                     //cannot set L_camera yet as we might not have the camera pose

//                     //if object is already in the vector dont add
//                     //this is 'wrong' but currently we dont interpolate the gt data
//                     auto it_this = std::find_if(tmp_object_vector.begin(), tmp_object_vector.end(),
//                         [=](const ObjectPoseGT& gt_object) { return gt_object.object_id_ == object_id; });
//                     if(it_this == tmp_object_vector.end()) {
//                         tmp_object_vector.push_back(object_pose_gt);
//                     }
//                 }
//             }
//             else {
//                 LOG(FATAL) << "Currently expecting vicon and camera tiemstamp data to have at least one synched timestamp per frame!!";
//             }

//             //it seems that everything is very fast so (maybe a hack?) we'll just take the cloestst timestamp

//         }

//         CHECK_EQ(ground_truth_packets.size(), times_.size());

//         //assumes we get one for every frame!!!!?
//         gtsam::Pose3 initial_pose = gtsam::Pose3::Identity();
//         bool set_initial_pose = false;
//         FrameId previous_frame = 0;
//         for(auto& [frame_id, gt_packet] : ground_truth_packets) {
//             vicon_timestamps.push_back(gt_packet.timestamp_);

//             if(!set_initial_pose) {
//                 initial_pose = gt_packet.X_world_;
//                 set_initial_pose = true;
//             }

//             // offset initial pose so we start at "0, 0, 0"
//             gtsam::Pose3 X_world_aligned = initial_pose.inverse() * gt_packet.X_world_;
//             //put into cv convention with Z-axis forward
//             // X_world_aligned = T_omd_cv * X_world_aligned * T_omd_cv.inverse();


//             //get object vector for the tmp list
//             auto& unprocessed_objects = temp_object_poses.at(gt_packet.frame_id_);
//             for(auto& object_pose_gt : unprocessed_objects) {

//                 // // pose of the object in the camera frame usign the vicon world frame and not our aligned one

//                 // using the original pose of the camera put into camera frame
//                 gtsam::Pose3 object_pose_camera = gt_packet.X_world_.inverse() * object_pose_gt.L_world_;
//                 gtsam::Pose3 object_pose_world_aligned = X_world_aligned * object_pose_camera;
//                 gtsam::Pose3 object_pose_camera_aligned = X_world_aligned.inverse() * object_pose_world_aligned;

//                 // gtsam::Pose3 object_pose_world_aligned = object_pose_gt.L_world_ * X_world_aligned * T_omd_cv;
//                 // gtsam::Pose3 object_pose_camera_aligned = X_world_aligned.inverse() * object_pose_world_aligned;

//                 object_pose_gt.L_camera_ = object_pose_camera_aligned;
//                 object_pose_gt.L_world_ = object_pose_world_aligned;
//                 CHECK_EQ(frame_id, object_pose_gt.frame_id_);
//             }

//             //update camera pose gt with the aligned one
//             gt_packet.X_world_ = X_world_aligned;
//             gt_packet.object_poses_ = unprocessed_objects;

//             if(frame_id > 0) {
//                 const GroundTruthInputPacket& previous_gt_packet = ground_truth_packets.at(frame_id - 1);
//                 gt_packet.calculateAndSetMotions(previous_gt_packet);
//                 CHECK_EQ(frame_id - 1, previous_frame) << "Frames are not in ascending order!!!";
//             }

//             previous_frame = frame_id;
//         }

//         CHECK_EQ(ground_truth_packets.size(), times_.size());
//         CHECK_EQ(vicon_timestamps.size(), times_.size());

//         //update times with vicon timestamps as this is what is used externally by the dataprovider
//         times_ = vicon_timestamps;
//     }

//     void setIntrisicsAndTransforms() {
//         //using manufacturer
//         //cam0 -> rgbd
//         //cam1 -> stereo left
//         //cam2 -> stereo right
//         //kalirb file gives the rigid body transforms from the sensors to the apparatus frame
//         //cam0 -> stereo left
//         //cam1 -> stereo right
//         //cam2 -> rgbd
//         //using the camera (opencv) convention, ie. with z forward
//         YamlParser yaml_parser(kalibr_file_path_);


//         // std::vector<double> v_cam2_cam1;
//         // yaml_parser.getNestedYamlParam("cam2", "T_cn_cnm1", &v_cam2_cam1);
//         // gtsam::Pose3 T_cam1_cam2 = utils::poseVectorToGtsamPose3(v_cam2_cam1).inverse();
//         // gtsam::Pose3 T_left_right = T_cam1_cam2;


//         std::vector<double> v_cam1_cam0;
//         yaml_parser.getNestedYamlParam("cam1", "T_cn_cnm1", &v_cam1_cam0);
//         gtsam::Pose3 T_cam0_cam1 = utils::poseVectorToGtsamPose3(v_cam1_cam0).inverse();
//         const gtsam::Pose3 T_left_right = T_cam0_cam1;

//         std::vector<double> v_cam2_cam1;
//         yaml_parser.getNestedYamlParam("cam2_undistort", "T_cn_cnm1", &v_cam2_cam1);
//         //transform from cam2 into camera 1 frame
//         gtsam::Pose3 T_cam1_cam2 = utils::poseVectorToGtsamPose3(v_cam2_cam1).inverse();
//         const gtsam::Pose3 T_right_rgbd = T_cam1_cam2;

//         // // //transformation from cam2 INTO cam0
//         // // //in this case camera2 is the RGBD camera and cam0 is the stereo left camera
//         // // gtsam::Pose3 T_cam0_cam2 = T_cam0_cam1 * T_cam1_cam2;
//         // // T_leftstereo_depthcam_ = T_cam0_cam2;

//         // //now load apparaturs to left camera (cam0) transform to put cam0 into the sensor frame (or A)
//         YamlParser vicon_yaml_parser(vicon_calibration_file_path_);
//         std::vector<double> v_apparatus_left;
//         vicon_yaml_parser.getYamlParam("T_apparatus_left", &v_apparatus_left);
//         //transformation from cam0 (left) into the apparatus (A) frame
//         gtsam::Pose3 T_apparatus_cam0 = utils::poseVectorToGtsamPose3(v_apparatus_left);
//         // //this contains a (partial?) frame change convention from opencv to omd's robotic convention
//         T_apparatus_left_ = T_apparatus_cam0;
//         // T_leftstereo_depthcam_ = T_left_right * T_right_rgbd;
//         // T_apparatus_rgbd_ = T_apparatus_left_;
//         T_apparatus_rgbd_ = T_apparatus_left_ * T_left_right * T_right_rgbd;



//         //NOTE: expect the rgbd camera to always be cam2
//         CameraParams::IntrinsicsCoeffs intrinsics;
//         std::vector<double> intrinsics_v;
//         yaml_parser.getNestedYamlParam("cam2_undistort", "intrinsics", &intrinsics_v);
//         // yaml_parser.getNestedYamlParam("cam0", "intrinsics", &intrinsics_v);
//         CHECK_EQ(intrinsics_v.size(), 4u);
//         intrinsics.resize(4u);
//         // Move elements from one to the other.
//         std::copy_n(std::make_move_iterator(intrinsics_v.begin()),
//                     intrinsics.size(),
//                     intrinsics.begin());

//         CameraParams::DistortionCoeffs distortion({0, 0, 0, 0});

//         std::vector<int> resolution;
//         yaml_parser.getNestedYamlParam("cam2_undistort", "resolution", &resolution);
//         CHECK_EQ(resolution.size(), 2);
//         cv::Size image_size(resolution[0], resolution[1]);

//         std::string distortion_model, camera_model;
//         yaml_parser.getNestedYamlParam("cam2_undistort", "camera_model", &camera_model);
//         auto model = CameraParams::stringToDistortion("radtan", camera_model);

//         rgbd_camera_params_ = CameraParams(
//             intrinsics,
//             distortion,
//             image_size,
//             model
//         );

//         LOG(INFO) << rgbd_camera_params_.toString();

//     }

//     void associateGTWithObject(const cv::Mat& instance_mask, const cv::Mat& depth, FrameId frame_id, cv::Mat& relabelled_mask) const {
//         const GroundTruthInputPacket& gt_packet = getGtPacket(frame_id);
//         const ObjectIds gt_object_ids = gt_packet.getObjectIds();

//         ObjectIds object_ids = vision_tools::getObjectLabels(instance_mask);
//         const size_t n = object_ids.size();

//         instance_mask.copyTo(relabelled_mask);

//         if(n == 0) {
//             return;
//         }

//         //instance id (track id -> all keypoints for that cluster)
//         std::set<ObjectId> instance_id_set;
//         gtsam::FastMap<ObjectId, Keypoints> instance_kps;
//         gtsam::FastMap<ObjectId, Landmarks> instance_lmks;

//         Camera camera(rgbd_camera_params_);

//         //sample sparsely across image and fill instance_kps
//         int step = 3;
//         for (int i = 0; i < instance_mask.rows - step; i = i + step)
//         {
//             for (int j = 0; j < instance_mask.cols - step; j = j + step)
//             {
//                 ObjectId instance_label = instance_mask.at<ObjectId>(i, j);
//                 if(instance_label == background_label) {
//                     continue;
//                 }
//                 CHECK_NE(instance_label, background_label);

//                 instance_id_set.insert(instance_label);

//                 Keypoint keypoint(j, i);

//                 if(!instance_kps.exists(instance_label)) {
//                     instance_kps.insert2(instance_label, Keypoints{});
//                 }
//                 instance_kps.at(instance_label).push_back(keypoint);

//                 //lmk in camera frame
//                 const Depth d = functional_keypoint::at<Depth>(keypoint, depth);
//                 Landmark lmk;
//                 camera.backProject(keypoint, d, &lmk);

//                 if(!instance_lmks.exists(instance_label)) {
//                     instance_lmks.insert2(instance_label, Landmarks{});
//                 }
//                 instance_lmks.at(instance_label).push_back(lmk);
//             }
//         }

//         std::vector<int> instance_ids(instance_id_set.begin(), instance_id_set.end());
//         CHECK_EQ(instance_ids.size(), n);

//         //want to assign instance_ids to gt labels
//         const size_t m = gt_object_ids.size();

//         Eigen::MatrixXd cost;
//         cost.resize(n, m);

//         for(size_t rows = 0; rows < n; rows++) {
//             int instance_id = instance_ids.at(rows);
//             // all lmks in the camera frame for the detected mask
//             auto sampled_lmks = instance_lmks.at(instance_id);
//             Landmark avg_lmk_camera = gtsam::mean(sampled_lmks);
//             Landmark avg_lmk_world = gt_packet.X_world_ * avg_lmk_camera;

//             for(size_t cols = 0; cols < m; cols++) {
//                 // LOG(INFO) << cols;
//                 ObjectId gt_object_id = gt_object_ids.at(cols);
//                 // LOG(INFO) << gt_object_id;

//                 ObjectPoseGT gt_object_pose;
//                 CHECK(gt_packet.getObject(gt_object_id, gt_object_pose));

//                 gtsam::Point3 gt_translation = gt_object_pose.L_world_.translation();

//                 //distance between the gt object pose (translation) and the cluster of detected points
//                 cost(rows, cols) = gtsam::distance3(gt_translation, avg_lmk_world);

//             }
//         }

//         Eigen::VectorXi assignment;
//         internal::HungarianAlgorithm().solve(cost, assignment);

//         // LOG(INFO) << " With " << n << " original ids and " << m << " gt ids";

//         ObjectIds old_labels = instance_ids;
//         ObjectIds new_labels;
//         for(size_t i = 0; i < assignment.size(); i++) {
//             int j = assignment[i];

//             // //the i-th object is assignment to the j-th cluster
//             // ObjectId instance_id = instance_ids.at(i);
//             int assigned_object_id = gt_object_ids.at(j);
//             new_labels.push_back(assigned_object_id);

//             // LOG(INFO) << "Relabelled " << instance_id << " to " << assigned_object_id;

//             // const cv::Mat object_mask = relabelled_mask == instance_id;
//             // relabelled_mask.setTo(cv::Scalar(assigned_object_id), object_mask);
//         }

//         vision_tools::relabelMasks(instance_mask, relabelled_mask, old_labels, new_labels);
//         // LOG(INFO) << "Done";

//     }



// private:
//     const std::string rgbd_folder_path_;
//     const std::string instance_masks_folder_;
//     const std::string optical_flow_folder_path_;
//     const std::string vicon_file_path_; //vicon measurement (.csv) file
//     const std::string kalibr_file_path_;
//     const std::string vicon_calibration_file_path_; //vicon calibration (.yaml) file giving transform between left stereo and apparatus

//     std::vector<std::string> rgb_image_paths_; //index from 0 to match the naming convention of the dataset
//     std::vector<std::string> aligned_depth_image_paths_;
//     std::vector<Timestamp> times_; //loaded from rgbd.csv file and associated with rgbd images. Should be the same length as rgb/aligned depth files
//     //timestamps between vicon (gt) and camera (from rgbd) are not synchronized!!
//     //camera frame ids to vicon frame ids
//     gtsam::FastMap<FrameId, Timestamp> timestamp_frame_map_;

//     std::vector<std::string> optical_flow_image_paths_;
//     std::vector<std::string> instance_masks_image_paths_;

//     GroundTruthPacketMap ground_truth_packets_;
//     size_t dataset_size_; //set in setGroundTruthPacket. Reflects the number of files in the /optical_flow folder which is one per frame

//     //below are set in the setIntrisicsAndTransforms
//     CameraParams rgbd_camera_params_;
//     gtsam::Pose3 T_apparatus_left_; //T_AL or the transform from the left stereo to the apparaturs frame, as given in vicon.yaml
//     gtsam::Pose3 T_leftstereo_depthcam_; //T_LD or the transfrom from the depthcam to the left stereo. Given in the kalibr.yaml
//     gtsam::Pose3 T_apparatus_rgbd_; //T_AL * T_LD = T_AD

// };


class OMDOldAllLoader {

public:
    DYNO_POINTER_TYPEDEFS(OMDOldAllLoader)

    OMDOldAllLoader(const std::string& file_path)
    {
        loadAll(file_path);
        setCameraParams(file_path);
    }

    size_t size() const {
        return rgb_file_names_.size();
    }

    double getTimestamp(size_t idx) {
        return static_cast<double>(timestamps_.at(idx));
    }

    cv::Mat getRGB(size_t idx) const {
        CHECK_LT(idx, rgb_file_names_.size());

        cv::Mat rgb;
        loadRGB(rgb_file_names_.at(idx), rgb);
        CHECK(!rgb.empty()) << "Empty at  " << rgb_file_names_.at(idx);

        img_size_ = rgb.size();
        return rgb;
    }

    cv::Mat getDepthImage(size_t idx) const {
        CHECK_LT(idx, depth_file_names_.size());

        cv::Mat disp;
        loadDepth(depth_file_names_.at(idx), disp);
        CHECK(!disp.empty());

        constexpr auto depth_type = ImageType::Depth::OpenCVType;
        cv::Mat depth_image = cv::Mat::zeros(disp.size(), depth_type);

        const auto baseline = base_line_;
        const auto fx = rgbd_camera_params_.fx();

        // Get depth from disparity
        for (int i = 0u; i < disp.rows; i++) {
            // Loop over rows
            const double* disp_ptr = disp.ptr<double>(i);
            double* depth_ptr = depth_image.ptr<double>(i);

            for (int j = 0u; j < disp.cols; j++) {
                // Loop over cols
                const double depth = (baseline * fx) / (static_cast<double>(disp_ptr[j])/ 256.0);
                *(depth_ptr + j) = depth;
            }
        }

        return depth_image;
    }

    cv::Mat getOpticalFlow(size_t idx) const {
        CHECK_LT(idx, flow_file_names_.size());

        cv::Mat rgb;
        loadFlow(flow_file_names_.at(idx), rgb);
        CHECK(!rgb.empty());

        return rgb;
    }

    cv::Mat getInstanceMask(size_t idx) const {
        CHECK_LT(idx, semantic_file_names_.size());

        cv::Mat rgb;
        loadSemanticMask(semantic_file_names_.at(idx), img_size_, rgb);
        CHECK(!rgb.empty());

        return rgb;
    }

    GroundTruthInputPacket getGtPacket(size_t idx) const {
        return ground_truths_.at(idx);
    }


    void loadAll(const std::string& path_to_sequence) {
        std::ifstream times_stream;
        std::string strPathTimeFile = path_to_sequence + "/times.txt";
        throwExceptionIfPathInvalid(strPathTimeFile);

        const gtsam::Pose3 T_cv_robotic = gtsam::Pose3(
            gtsam::Rot3(
                1, 0, 0,
                0, 0, -1,
                0, 1, 0),
            gtsam::traits<gtsam::Point3>::Identity());

        times_stream.open(strPathTimeFile.c_str());
        while (!times_stream.eof())
        {
            std::string s;
            getline(times_stream, s);
            if (!s.empty())
            {
                std::stringstream ss;
                ss << s;
                double t;
                ss >> t;
                timestamps_.push_back(t);
            }
        }
        times_stream.close();
        LOG(INFO) << "Loaded " << timestamps_.size() << " timestamps";

        // +++ image, depth, semantic and moving object tracking mask +++
        std::string strPrefixImage = path_to_sequence + "/image_0/";      // image  image_0
        std::string strPrefixDepth = path_to_sequence + "/depth/";        // depth_gt  depth  depth_mono_stereo
        std::string strPrefixSemantic = path_to_sequence + "/semantic/";  // semantic_gt  semantic
        std::string strPrefixFlow = path_to_sequence + "/flow/";          // flow_gt  flow

        // const int nTimes = timestamps_.size();
        // rgb_file_names_.resize(nTimes);
        // depth_file_names_.resize(nTimes);
        // semantic_file_names_.resize(nTimes);
        // flow_file_names_.resize(nTimes);
        loadPathsInDirectory(rgb_file_names_, strPrefixImage,
            [](const std::string& file) -> bool {
                return boost::algorithm::ends_with( &file[0], ".png");  // true
            });
        loadPathsInDirectory(depth_file_names_, strPrefixDepth,
            [](const std::string& file) -> bool {
                return boost::algorithm::ends_with( &file[0], ".png");  // true
            });
        loadPathsInDirectory(semantic_file_names_, strPrefixSemantic,
            [](const std::string& file) -> bool {
                return boost::algorithm::ends_with( &file[0], ".txt");  // true
            });
        loadPathsInDirectory(flow_file_names_, strPrefixFlow,
            [](const std::string& file) -> bool {
               return boost::algorithm::ends_with( &file[0], ".flo");  // true
            });

        const int nTimes = rgb_file_names_.size();
        LOG(INFO) << rgb_file_names_.back();


        LOG(INFO) << "Loaded " << nTimes;


        // for (int i = 0; i < nTimes; i++)
        // {
        //     std::stringstream ss;
        //     ss << std::setfill('0') << std::setw(6) << i;
        //     rgb_file_names_.push_back(strPrefixImage + ss.str() + ".png");
        //     depth_file_names_.push_back(strPrefixDepth + ss.str() + ".png");
        //     semantic_file_names_.push_back(strPrefixSemantic + ss.str() + ".txt");
        //     flow_file_names_.push_back(strPrefixFlow + ss.str() + ".flo");
        // }

        // +++ ground truth pose +++
        std::string strFilenamePose = path_to_sequence + "/pose_gt.txt";  //  pose_gt.txt  kevin_extrinsics.txt
        throwExceptionIfPathInvalid(strFilenamePose);
        // vPoseGT.resize(nTimes);
        std::ifstream fPose;
        fPose.open(strFilenamePose.c_str());
        LOG(INFO) << "OPened pose file";

        gtsam::Pose3 initial_pose;

        bool has_initial_pose = false;

        while (!fPose.eof())
        {
            std::string s;
            getline(fPose, s);
            if (!s.empty())
            {
                std::stringstream ss;
                ss << s;
                int t;
                ss >> t;
                cv::Mat Pose_tmp = cv::Mat::eye(4, 4, CV_64F);

                ss >> Pose_tmp.at<double>(0, 0) >> Pose_tmp.at<double>(0, 1) >> Pose_tmp.at<double>(0, 2) >>
                    Pose_tmp.at<double>(0, 3) >> Pose_tmp.at<double>(1, 0) >> Pose_tmp.at<double>(1, 1) >>
                    Pose_tmp.at<double>(1, 2) >> Pose_tmp.at<double>(1, 3) >> Pose_tmp.at<double>(2, 0) >>
                    Pose_tmp.at<double>(2, 1) >> Pose_tmp.at<double>(2, 2) >> Pose_tmp.at<double>(2, 3) >>
                    Pose_tmp.at<double>(3, 0) >> Pose_tmp.at<double>(3, 1) >> Pose_tmp.at<double>(3, 2) >>
                    Pose_tmp.at<double>(3, 3);

                // std::vector<double> vec(Pose_tmp.begin<double>(),Pose_tmp.end<double>());
                // vPoseGT_.push_back(parsePose(vec));
                gtsam::Pose3 pose = utils::cvMatToGtsamPose3(Pose_tmp);

                // pose = T_cv_robotic * pose * T_cv_robotic.inverse();

                if (!has_initial_pose) {
                    initial_pose = pose;
                    has_initial_pose = true;
                }

                // pose = initial_pose.inverse() * pose;

                vPoseGT_.push_back(pose);
            }
        }
        fPose.close();

        // +++ ground truth object pose +++
        std::string strFilenameObjPose = path_to_sequence + "/object_pose.txt";
        throwExceptionIfPathInvalid(strFilenameObjPose);
        std::ifstream fObjPose;
        fObjPose.open(strFilenameObjPose.c_str());
        LOG(INFO) << "Opened object pose file";

        while (!fObjPose.eof())
        {
            std::string s;
            getline(fObjPose, s);
            if (!s.empty())
            {
                std::stringstream ss;
                ss << s;

                std::vector<double> ObjPose_tmp(8, 0);
                ss >> ObjPose_tmp[0] >> ObjPose_tmp[1] >> ObjPose_tmp[2] >> ObjPose_tmp[3] >> ObjPose_tmp[4] >> ObjPose_tmp[5] >>
                    ObjPose_tmp[6] >> ObjPose_tmp[7];

                ObjectPoseGT object_pose;
                object_pose.frame_id_ = static_cast<size_t>(ObjPose_tmp[0]);
                object_pose.object_id_ = static_cast<size_t>(ObjPose_tmp[1]);

                const auto frame_id = object_pose.frame_id_;

                // assign t vector
                cv::Mat t(3, 1, CV_64F);
                t.at<double>(0) = ObjPose_tmp[2];
                t.at<double>(1) = ObjPose_tmp[3];
                t.at<double>(2) = ObjPose_tmp[4];

                // from axis-angle to Rotation Matrix
                cv::Mat R(3, 3, CV_64F);
                cv::Mat Rvec(3, 1, CV_64F);

                // assign r vector
                Rvec.at<double>(0, 0) = ObjPose_tmp[5];
                Rvec.at<double>(0, 1) = ObjPose_tmp[6];
                Rvec.at<double>(0, 2) = ObjPose_tmp[7];

                // *******************************************************************

                const double angle = std::sqrt(ObjPose_tmp[5] * ObjPose_tmp[5] + ObjPose_tmp[6] * ObjPose_tmp[6] + ObjPose_tmp[7] * ObjPose_tmp[7]);

                if (angle > 0)
                {
                    Rvec.at<double>(0, 0) = Rvec.at<double>(0, 0) / angle;
                    Rvec.at<double>(0, 1) = Rvec.at<double>(0, 1) / angle;
                    Rvec.at<double>(0, 2) = Rvec.at<double>(0, 2) / angle;
                }

                const double s = std::sin(angle);
                const double c = std::cos(angle);

                const double v = 1 - c;
                const double x = Rvec.at<double>(0, 0);
                const double y = Rvec.at<double>(0, 1);
                const double z = Rvec.at<double>(0, 2);
                const double xyv = x * y * v;
                const double yzv = y * z * v;
                const double xzv = x * z * v;

                R.at<double>(0, 0) = x * x * v + c;
                R.at<double>(0, 1) = xyv - z * s;
                R.at<double>(0, 2) = xzv + y * s;
                R.at<double>(1, 0) = xyv + z * s;
                R.at<double>(1, 1) = y * y * v + c;
                R.at<double>(1, 2) = yzv - x * s;
                R.at<double>(2, 0) = xzv - y * s;
                R.at<double>(2, 1) = yzv + x * s;
                R.at<double>(2, 2) = z * z * v + c;

                // construct 4x4 transformation matrix
                cv::Mat Pose = cv::Mat::eye(4, 4, CV_64F);
                Pose.at<double>(0, 0) = R.at<double>(0, 0);
                Pose.at<double>(0, 1) = R.at<double>(0, 1);
                Pose.at<double>(0, 2) = R.at<double>(0, 2);
                Pose.at<double>(0, 3) = t.at<double>(0);
                Pose.at<double>(1, 0) = R.at<double>(1, 0);
                Pose.at<double>(1, 1) = R.at<double>(1, 1);
                Pose.at<double>(1, 2) = R.at<double>(1, 2);
                Pose.at<double>(1, 3) = t.at<double>(1);
                Pose.at<double>(2, 0) = R.at<double>(2, 0);
                Pose.at<double>(2, 1) = R.at<double>(2, 1);
                Pose.at<double>(2, 2) = R.at<double>(2, 2);
                Pose.at<double>(2, 3) = t.at<double>(2);

                object_pose.L_world_ = utils::cvMatToGtsamPose3(Pose);
                // object_pose.L_world_ = T_cv_robotic * object_pose.L_world_ * T_cv_robotic.inverse();

                vObjPoseGT_.push_back(object_pose);
            }
        }
        fObjPose.close();
        LOG(INFO) << "Loaded object poses";

        // organise gt poses into vector of arrays
        std::vector<std::vector<size_t>> vObjPoseID(rgb_file_names_.size());
        for (size_t i = 0; i < vObjPoseGT_.size(); ++i)
        {
            size_t f_id = vObjPoseGT_[i].frame_id_;
            if (f_id >= rgb_file_names_.size())
            {
                break;
            }
            vObjPoseID[f_id].push_back(i);
        }
        LOG(INFO) << "Organised object poses";

        // now read image image and add grount truths
        for (size_t frame_id = 0; frame_id < nTimes - 1; frame_id++)
        {
            Timestamp timestamp = timestamps_[frame_id];
            GroundTruthInputPacket gt_packet;
            gt_packet.timestamp_ = timestamp;
            gt_packet.frame_id_ = frame_id;

            auto original_camera_pose = vPoseGT_[frame_id];
            auto aligned_camera_pose = initial_pose.inverse() * original_camera_pose;
            // auto aligned_camera_pose = initial_pose.inverse() * original_camera_pose;

            gt_packet.X_world_ = aligned_camera_pose;

            // add ground truths for this fid
            for (int i = 0; i < vObjPoseID[frame_id].size(); i++)
            {
                auto gt_object = vObjPoseGT_[vObjPoseID[frame_id][i]];
                auto relative_object_pose = original_camera_pose.inverse() * gt_object.L_world_;
                auto aligned_object_pose = aligned_camera_pose * relative_object_pose;

                gt_object.L_camera_ = relative_object_pose;
                gt_object.L_world_ = aligned_object_pose;

                gt_packet.object_poses_.push_back(gt_object);
                // sanity check
                CHECK_EQ(gt_packet.object_poses_[i].frame_id_, frame_id);
            }

            if (frame_id > 0) {
                auto& previous_gt = ground_truths_.at(frame_id - 1);
                gt_packet.calculateAndSetMotions(previous_gt);
            }

            ground_truths_.push_back(gt_packet);
        }
        LOG(INFO) << "Set GT";

    }

    const CameraParams& getLeftCameraParams() const {
        return rgbd_camera_params_;
    }

    void setCameraParams(const std::string& file_path) {
        std::string params_file = file_path + "oxford.yaml";

        YamlParser yaml_parser(params_file);

        std::vector<double> intrinsics_v(4);
        yaml_parser.getYamlParam<double>("Camera.fx", intrinsics_v.data());
        yaml_parser.getYamlParam<double>("Camera.fy", intrinsics_v.data() + 1);
        yaml_parser.getYamlParam<double>("Camera.cx", intrinsics_v.data() + 2);
        yaml_parser.getYamlParam<double>("Camera.cy", intrinsics_v.data() + 3);

        CameraParams::IntrinsicsCoeffs intrinsics;
        intrinsics.resize(4u);
        // Move elements from one to the other.
        std::copy_n(std::make_move_iterator(intrinsics_v.begin()),
                    intrinsics.size(),
                    intrinsics.begin());

        CameraParams::DistortionCoeffs distortion({0, 0, 0, 0});

        double width, height;
        yaml_parser.getYamlParam<double>("Camera.width", &width);
        yaml_parser.getYamlParam<double>("Camera.height", &height);
        cv::Size image_size(width, height);

        yaml_parser.getYamlParam<double>("Camera.baseline", &base_line_);

        auto model = CameraParams::stringToDistortion("radtan", "pinhole");

        rgbd_camera_params_ = CameraParams(
            intrinsics,
            distortion,
            image_size,
            model
        );

        LOG(INFO) << "Camera params " << rgbd_camera_params_.toString();
    }


private:
    std::vector<std::string> rgb_file_names_;
    std::vector<std::string> depth_file_names_;
    std::vector<std::string> flow_file_names_;
    std::vector<std::string> semantic_file_names_;
    std::vector<Timestamp> timestamps_;

    std::vector<gtsam::Pose3> vPoseGT_;
    std::vector<ObjectPoseGT> vObjPoseGT_;
    // per frame ground truth
    std::vector<GroundTruthInputPacket> ground_truths_;

    mutable cv::Size img_size_;
    CameraParams rgbd_camera_params_;
    double base_line_;

};

struct OMMTimestampLoader : public TimestampBaseLoader {

    OMDOldAllLoader::Ptr loader_;

    OMMTimestampLoader(OMDOldAllLoader::Ptr loader) : loader_(CHECK_NOTNULL(loader)) {}
    std::string getFolderName() const override { return ""; }

    size_t size() const override {
        return loader_->size();
    }

    double getItem(size_t idx) override {
        return loader_->getTimestamp(idx);
    }
};


OMDDataLoader::OMDDataLoader(const fs::path& dataset_path) : OMDDatasetProvider(dataset_path) {
    LOG(INFO) << "Starting OMDDataLoader with path" << dataset_path;

    //this would go out of scope but we capture it in the functional loaders
    auto loader = std::make_shared<OMDOldAllLoader>(dataset_path);
    auto timestamp_loader = std::make_shared<OMMTimestampLoader>(loader);

    left_camera_params_ = loader->getLeftCameraParams();

    CHECK(getCameraParams());

    auto rgb_loader = std::make_shared<FunctionalDataFolder<cv::Mat>>(
        [loader](size_t idx) {
            return loader->getRGB(idx);
        }
    );

    auto optical_flow_loader = std::make_shared<FunctionalDataFolder<cv::Mat>>(
        [loader](size_t idx) {
            return loader->getOpticalFlow(idx);
        }
    );

    auto depth_loader = std::make_shared<FunctionalDataFolder<cv::Mat>>(
        [loader](size_t idx) {
            return loader->getDepthImage(idx);
        }
    );

    auto instance_mask_loader = std::make_shared<FunctionalDataFolder<cv::Mat>>(
        [loader](size_t idx) {
            return loader->getInstanceMask(idx);
        }
    );


    auto gt_loader = std::make_shared<FunctionalDataFolder<GroundTruthInputPacket>>(
        [loader](size_t idx) {
            return loader->getGtPacket(idx);
        }
    );

    this->setLoaders(
        timestamp_loader,
        rgb_loader,
        optical_flow_loader,
        depth_loader,
        instance_mask_loader,
        gt_loader
    );

    auto callback = [&](size_t frame_id,
        Timestamp timestamp,
        cv::Mat rgb,
        cv::Mat optical_flow,
        cv::Mat depth,
        cv::Mat instance_mask,
        GroundTruthInputPacket gt_object_pose_gt) -> bool
    {
        CHECK_EQ(timestamp, gt_object_pose_gt.timestamp_);

        CHECK(ground_truth_packet_callback_);
        if(ground_truth_packet_callback_) ground_truth_packet_callback_(gt_object_pose_gt);

        ImageContainer::Ptr image_container = nullptr;
        image_container = ImageContainer::Create(
                timestamp,
                frame_id,
                ImageWrapper<ImageType::RGBMono>(rgb),
                ImageWrapper<ImageType::Depth>(depth),
                ImageWrapper<ImageType::OpticalFlow>(optical_flow),
                ImageWrapper<ImageType::MotionMask>(instance_mask));
        CHECK(image_container);
        CHECK(image_container_callback_);
        if(image_container_callback_) image_container_callback_(image_container);
        return true;
    };

    this->setCallback(callback);

    //first valid frame is 1
    // setStartingFrame(1u);
}

} //dyno
````

## File: dataprovider/ProjectAriaDataProvider.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/dataprovider/ProjectAriaDataProvider.hpp"
#include "dynosam/dataprovider/DataProviderUtils.hpp"
#include "dynosam/utils/OpenCVUtils.hpp"
#include "dynosam/utils/GtsamUtils.hpp"
#include "dynosam/utils/CsvParser.hpp"

#include <nlohmann/json.hpp>

namespace dyno {

class ProjectAriaAllLoader {

public:
    DYNO_POINTER_TYPEDEFS(ProjectAriaAllLoader)

    ProjectAriaAllLoader(const std::string& file_path)
        :   rgb_images_folder_path_(file_path + "/rgb_sync"),
            depth_images_folder_path_(file_path + "/depth_sync"),
            optical_flow_folder_path_(file_path + "/optical_flow"),
            instance_masks_folder_(file_path + "/instance_masks"),
            intrinsics_file_path_(file_path + "/calibration_undistort.json"),
            rgb_timestamps_file_path_(file_path + "/sync_timestamp.csv")
        {
            // Initialize folders and file paths
            throwExceptionIfPathInvalid(rgb_images_folder_path_);
            throwExceptionIfPathInvalid(depth_images_folder_path_);
            throwExceptionIfPathInvalid(optical_flow_folder_path_);
            throwExceptionIfPathInvalid(instance_masks_folder_);
            // throwExceptionIfPathInvalid(intrinsics_file_path_);
            // throwExceptionIfPathInvalid(rgb_timestamps_file_path_);

             //first load images and size
            //the size will be used as a refernce for all other loaders
            //size is number of images
            //we use flow to set the size as for this dataset, there will be one less
            //optical flow as the flow ids index t to t+1 (which is gross, I know!!)
            //TODO: code and comments replicated in ClusterSlamDataProvider
            loadFlowImagesAndSize(optical_flow_image_paths_, dataset_size_);

            //load rgb, depth and instance masks - they should all have the same length!!
            loadOtherImages();
            // loadTimestamps();
            loadCalibration();
        }

        size_t size() const {
            return dataset_size_;
        }

    cv::Mat getOpticalFlow(size_t idx) const {
        CHECK_LT(idx,optical_flow_image_paths_.size());

        cv::Mat flow;
        loadFlow(optical_flow_image_paths_.at(idx), flow);
        CHECK(!flow.empty());
        return flow;
    }

    cv::Mat getRGB(size_t idx) const {
        CHECK_LT(idx, rgb_image_paths_.size());

        cv::Mat rgb;
        loadRGB(rgb_image_paths_.at(idx), rgb);
        CHECK(!rgb.empty());

        return rgb;
    }

    cv::Mat getInstanceMask(size_t idx) const {
        CHECK_LT(idx, rgb_image_paths_.size());
        CHECK_LT(idx, dataset_size_);

        cv::Mat mask;
        loadMask(instance_masks_image_paths_.at(idx), mask);
        CHECK(!mask.empty());

        return mask;
    }

    cv::Mat getDepthImage(size_t idx) const {
        CHECK_LT(idx, depth_image_paths_.size());

        cv::Mat depth;
        loadDepth(depth_image_paths_.at(idx), depth);
        CHECK(!depth.empty());

        return depth;
    }


    const CameraParams& getLeftCameraParams() const {
        return rgb_camera_params_;
    }

    double getTimestamp(size_t idx) {
        CHECK_LT(idx, times_.size());
        return times_.at(idx);
    }

private:
    void loadFlowImagesAndSize(std::vector<std::string>& images_paths, size_t& dataset_size) {
        std::vector<std::filesystem::path> files_in_directory = getAllFilesInDir(optical_flow_folder_path_);
        dataset_size = files_in_directory.size();
        CHECK_GT(dataset_size, 0);

        for (const std::filesystem::path& path : files_in_directory) {
            const std::string file_path = path;
            throwExceptionIfPathInvalid(file_path);
            images_paths.push_back(file_path);

            std::string timestamp_str = path.stem();
            double timestamp_nano = std::stod(timestamp_str);
            double timestamp_seconds = timestamp_nano / 1.0e+9;
            times_.push_back(timestamp_seconds);
        }
    }

    void loadOtherImages() {
        auto rgb_image_files = getAllFilesInDir(rgb_images_folder_path_);
        auto depth_image_files = getAllFilesInDir(depth_images_folder_path_);
        auto instance_masks_image_files = getAllFilesInDir(instance_masks_folder_);

        for (auto f : rgb_image_files) { rgb_image_paths_.push_back(f); }
        for (auto f : depth_image_files) { depth_image_paths_.push_back(f); }
        for (auto f : instance_masks_image_files) { instance_masks_image_paths_.push_back(f); }

        CHECK_EQ(rgb_image_paths_.size(), depth_image_paths_.size());
        CHECK_EQ(depth_image_paths_.size(), instance_masks_image_paths_.size());

        //dataset size is number optical flow which should be one less!!
        //or equal too!!
        // CHECK_EQ(instance_masks_image_paths_.size() -1u, dataset_size_);
    }

    // void loadTimestamps() {
    //     std::ifstream file(rgb_timestamps_file_path_);
    //     CsvReader csv_reader(file);
    //     auto it = csv_reader.begin();
    //     //skip header
    //     ++it;
    //     bool is_first_timestamp = true;
    //     Timestamp time_offset = 0; //first timestamp
    //     for(; it != csv_reader.end(); it++) {
    //         const auto row = *it;

    //         double time_ms = row.at<double>(0);
    //         Timestamp time_s = time_ms / 1000.0;

    //         if(is_first_timestamp) {
    //             time_offset = time_s;
    //             is_first_timestamp = false;
    //         }

    //         //start time at zero with first timestamp offset
    //         Timestamp offset_time_s = time_s - time_offset;
    //         times_.push_back(offset_time_s);


    //     }
    //     LOG(INFO) << "Loaded " << times_.size() << " timestamps from RGBD csv file: " << rgb_timestamps_file_path_;

    // }

    void loadCalibration() {
        // using json = nlohmann::json;

        // std::ifstream file(intrinsics_file_path_);
        // json calibration_json;
        // file >> calibration_json;

        // int rgb_width = calibration_json["rgb_width"].template get<int>();
        // int rgb_height = calibration_json["rgb_height"].template get<int>();

        // std::vector<double> K = calibration_json["rgb_intrinsics"].template get<std::vector<double>>();
        // CameraParams::IntrinsicsCoeffs intrinsics({
        //     K.at(0), //fu
        //     K.at(4), //fv
        //     K.at(2), //cu
        //     K.at(5)  // cv
        //     });

        // CameraParams::IntrinsicsCoeffs intrinsics({
        //     535.288025, //fu
        //     535.288025, //fv
        //     623.312256, //cu
        //     348.522400  // cv
        //     });

        //acfr_1_moving_small
        CameraParams::IntrinsicsCoeffs intrinsics({
            267.644012, //fu
            311.656128, //fv
            267.644012, //cu
            174.261200  // cv
            });

        //TODO: Assume Image is undistorted!!!!!!
        CameraParams::DistortionCoeffs distortion({0, 0, 0, 0});

        //distortion_model not given
        const auto distortion_model = "plumb_bob";
        const auto camera_model = "pinhole";
        auto model = CameraParams::stringToDistortion(distortion_model, camera_model);

        rgb_camera_params_ = CameraParams(
            intrinsics,
            distortion,
            // cv::Size(1280, 720),
            cv::Size(640, 360),
            model
        );


    }

private:
    std::string rgb_images_folder_path_;
    std::string depth_images_folder_path_;
    std::string optical_flow_folder_path_;
    std::string instance_masks_folder_;
    std::string intrinsics_file_path_;
    std::string rgb_timestamps_file_path_;

    std::vector<std::string> rgb_image_paths_;
    std::vector<std::string> depth_image_paths_;
    std::vector<std::string> optical_flow_image_paths_;
    std::vector<std::string> instance_masks_image_paths_;

    std::vector<Timestamp> times_;

    size_t dataset_size_ = 0;
    CameraParams rgb_camera_params_;

};

struct ProjectAriaTimestampLoader : public TimestampBaseLoader {

    ProjectAriaAllLoader::Ptr loader_;

    ProjectAriaTimestampLoader(ProjectAriaAllLoader::Ptr loader) : loader_(CHECK_NOTNULL(loader)) {}
    std::string getFolderName() const override { return ""; }

    size_t size() const override {
        return loader_->size();
    }

    double getItem(size_t idx) override {
        return loader_->getTimestamp(idx);
    }
};

ProjectARIADataLoader::ProjectARIADataLoader(const fs::path& dataset_path) : ProjectAriaDatasetProvider(dataset_path)
{
    LOG(INFO) << "Starting ProjectARIADataLoader with path" << dataset_path;

    //this would go out of scope but we capture it in the functional loaders
    auto loader = std::make_shared<ProjectAriaAllLoader>(dataset_path);
    auto timestamp_loader = std::make_shared<ProjectAriaTimestampLoader>(loader);

    left_camera_params_ = loader->getLeftCameraParams();

    CHECK(getCameraParams());

    auto rgb_loader = std::make_shared<FunctionalDataFolder<cv::Mat>>(
        [loader](size_t idx) {
            return loader->getRGB(idx);
        }
    );

    auto optical_flow_loader = std::make_shared<FunctionalDataFolder<cv::Mat>>(
        [loader](size_t idx) {
            return loader->getOpticalFlow(idx);
        }
    );

    auto depth_loader = std::make_shared<FunctionalDataFolder<cv::Mat>>(
        [loader](size_t idx) {
            return loader->getDepthImage(idx);
        }
    );

    auto instance_mask_loader = std::make_shared<FunctionalDataFolder<cv::Mat>>(
        [loader](size_t idx) {
            return loader->getInstanceMask(idx);
        }
    );


    // auto gt_loader = std::make_shared<FunctionalDataFolder<GroundTruthInputPacket>>(
    //     [loader](size_t idx) {
    //         return loader->getGtPacket(idx);
    //     }
    // );

    this->setLoaders(
        timestamp_loader,
        rgb_loader,
        optical_flow_loader,
        depth_loader,
        instance_mask_loader
        // gt_loader
    );

     auto callback = [&](size_t frame_id,
        Timestamp timestamp,
        cv::Mat rgb,
        cv::Mat optical_flow,
        cv::Mat depth,
        cv::Mat instance_mask
        /**GroundTruthInputPacket gt_object_pose_gt**/) -> bool
    {
        // CHECK_EQ(timestamp, gt_object_pose_gt.timestamp_);

        // CHECK(ground_truth_packet_callback_);
        // if(ground_truth_packet_callback_) ground_truth_packet_callback_(gt_object_pose_gt);

        ImageContainer::Ptr image_container = nullptr;
        image_container = ImageContainer::Create(
                timestamp,
                frame_id,
                ImageWrapper<ImageType::RGBMono>(rgb),
                ImageWrapper<ImageType::Depth>(depth),
                ImageWrapper<ImageType::OpticalFlow>(optical_flow),
                ImageWrapper<ImageType::MotionMask>(instance_mask));
        CHECK(image_container);
        CHECK(image_container_callback_);
        if(image_container_callback_) image_container_callback_(image_container);
        return true;
    };

    this->setCallback(callback);
}



} //dyno
````

## File: dataprovider/VirtualKittidataProvider.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/dataprovider/VirtualKittiDataProvider.hpp"
#include "dynosam/dataprovider/DataProviderUtils.hpp"

#include "dynosam/frontend/vision/VisionTools.hpp" //for getObjectLabels

#include <glog/logging.h>
#include <fstream>


#include <png++/png.hpp> //libpng-dev

namespace dyno {

/**
 * @brief NOTE!!: In vitual kitti, the track id's start at 0 and then the ground truth instances in the
 * semantic instances are indexed from trackId+1 such that a pixel value of zero means not a vehicle.
 * In order to reduce confusion when we work with the semantic images and then associate with the ground truth labels,
 * we will index all tracks as trackID = virtualKittiTrackID + 1 so all tracks start from 1!
 *
 * @param split_string
 * @param idx
 * @return int
 */
inline int getTrackID(const std::vector<std::string>& split_string, int idx) {
    return std::stoi(split_string.at(idx)) + 1;
}


/**
 * @brief Loading class for the Virtual Kitti 2 dataset: https://europe.naverlabs.com/research/computer-vision/proxy-virtual-worlds-vkitti-2/
 *
 */

class GenericVirtualKittiImageLoader : public dyno::DataFolder<cv::Mat> {
public:
    GenericVirtualKittiImageLoader(const std::string& full_path) : full_path_(full_path) {}

    std::string getFolderName() const override { return ""; }
    const std::string full_path_; //! Full path as set by the full path constructor argument
};

class VirtualKittiRGBDataFolder : public GenericVirtualKittiImageLoader {

public:
    VirtualKittiRGBDataFolder(const std::string& path) : GenericVirtualKittiImageLoader(path) {}
    cv::Mat getItem(size_t idx) override {
        std::stringstream ss;
        ss << std::setfill('0') << std::setw(5) << idx;
        const std::string file_path = full_path_  + "/rgb_" + ss.str() + ".jpg";
        throwExceptionIfPathInvalid(file_path);

        cv::Mat rgb;
        loadRGB(file_path, rgb);

        return rgb;
    }
};

class VirtualKittiFlowDataFolder : public GenericVirtualKittiImageLoader  {

public:
    VirtualKittiFlowDataFolder(const std::string& path) : GenericVirtualKittiImageLoader(path) {}

    cv::Mat getItem(size_t idx) override {
        CHECK(idx > 0);
        std::stringstream ss;
        //index at frame -1 so that we get the flow from t-1 to t (where the idx is t)
        ss << std::setfill('0') << std::setw(5) << idx-1;
        const std::string file_path = full_path_  + "/flow_" + ss.str() + ".png";
        // const std::string file_path = full_path_  + "/backwardFlow_" + ss.str() + ".png"; //specific to backwards flow
        throwExceptionIfPathInvalid(file_path);
        return vKittiPngToFlow(file_path);
    }

private:
    //as per example code in https://europe.naverlabs.com/research/computer-vision/proxy-virtual-worlds-vkitti-2/
    // In R, flow along x-axis normalized by image width and quantized to [0;2^16 – 1]
    // In G, flow along x-axis normalized by image width and quantized to [0;2^16 – 1]
    // B = 0 for invalid flow (e.g., sky pixels)
    cv::Mat vKittiPngToFlow(const std::string& path) const {
        // cv::Mat bgr = cv::imread(path, cv::IMREAD_ANYCOLOR | cv::IMREAD_ANYDEPTH);
        cv::Mat bgr;
        loadRGB(path, bgr);

        CHECK(!bgr.empty());

        int h = bgr.rows;
        int w = bgr.cols;
        int c = bgr.channels();

        CHECK_EQ(bgr.type(), CV_16UC3) << "type was " << utils::cvTypeToString(bgr);
        CHECK_EQ(c, 3);

        cv::Mat bgr_channels[3];
        cv::split(bgr, bgr_channels);
        cv::Mat b_channel = bgr_channels[0];

        cv::Mat bgr_float;
        //float 32
        bgr.convertTo(bgr_float, CV_32F);

        //scaled to [0;2**16 – 1]
        cv::Mat unscaled_out_flow = 2.0 / (std::pow(2, 16) - 1.0) * bgr_float - 1;

        cv::Mat channels[2];
        //now only take g and r channels
        cv::split(unscaled_out_flow, channels);
        // g,r == flow_y,x normalized by height,width
        cv::Mat& flow_y = channels[1];
        cv::Mat& flow_x = channels[2];
        flow_x *= w -1.0;
        flow_y *= h -1.0;

        //re-order channels so x is first and then y
        cv::Mat x_y_ordered_channels[] = {flow_x, flow_y};
        cv::Mat out_flow;
        cv::merge(x_y_ordered_channels, 2, out_flow);

        //b == invalid flow flag == 0 for sky or other invalid flow
        cv::Mat invalid = (b_channel == 0);
        CHECK(!invalid.empty());
        out_flow.setTo(0,invalid);

        out_flow.convertTo(out_flow, CV_32F);

        return out_flow;

    }
};

class VirtualKittiDepthDataFolder : public GenericVirtualKittiImageLoader  {

public:
    VirtualKittiDepthDataFolder(const std::string& path) : GenericVirtualKittiImageLoader(path) {}

    cv::Mat getItem(size_t idx) override {
        std::stringstream ss;
        ss << std::setfill('0') <<  std::setw(5) << idx;
        const std::string file_path = full_path_  + "/depth_" + ss.str() + ".png";
        throwExceptionIfPathInvalid(file_path);

        //TODO: for now? When to apply preprocessing?
        cv::Mat depth;
        loadRGB(file_path, depth);
        //first convert to double as loadRGB provides 16UC1
        depth.convertTo(depth, CV_64F);
        //apply depth factor to get information into meters (from cm)
        depth /= 100.0;

        return depth;
    }
};

struct VirtualKittiTimestampLoader : public TimestampBaseLoader {

    VirtualKittiTimestampLoader(size_t total_num_frames) : total_num_frames_(total_num_frames) {}

    std::string getFolderName() const override { return ""; }
    double getItem(size_t idx) override {
        //we dont have a time? for now just make idx
        return static_cast<double>(idx);
    }

    size_t size() const override {
        //only valid after loading
        //we go up to -1 becuase the optical flow has ONE LESS image
        return total_num_frames_-1u;
    }

    const size_t total_num_frames_;

};


class VirtualKittiTextGtLoader {

public:
    DYNO_POINTER_TYPEDEFS(VirtualKittiTextGtLoader)

    /**
     * @brief Construct a new Virtual Kitti Text Gt Laoder object
     *
     * File path is full file path to the textgt folder containing bbox.txt, colors.txt, ... pose.txt
     *
     * @param file_path
     */
    VirtualKittiTextGtLoader(const std::string& file_path)
    :  bbox_path(file_path + "/bbox.txt"),
       colors_path(file_path + "/colors.txt"),
       extrinsic_path(file_path + "/extrinsic.txt"),
       info_path(file_path + "/info.txt"),
       intrinsic_path(file_path + "/intrinsic.txt"),
       pose_path(file_path + "/pose.txt")  {

        throwExceptionIfPathInvalid(bbox_path);
        throwExceptionIfPathInvalid(colors_path);
        throwExceptionIfPathInvalid(extrinsic_path);
        throwExceptionIfPathInvalid(info_path);
        throwExceptionIfPathInvalid(intrinsic_path);
        throwExceptionIfPathInvalid(pose_path);

        loadBBoxMetaData(bbox_data_);
        loadPoseTxt(object_poses_, bbox_data_);
        loadCameraPoses(camera_poses_);

        gt_packets_ = constructGTPackets(object_poses_, camera_poses_);

    }

    const GroundTruthInputPacket& getGTPacket(size_t frame) const {
        return gt_packets_.at(frame);
    }

    size_t size() const {
        return gt_packets_.size();
    }

    /**
     * @brief Queries if an object is moving in the current frame
     *
     * Unlike in Virtual kitti which indicates if movement is between frame t and t+1,
     * we need to know if the object moved between t-1 and t (as we estimate for t-1 to t)
     *
     * The frame argument is considered to be frame t and we requery the isMoving variable of the BBoxMetaData at t-1.
     *
     * If object does not exist at frame return false?
     *
     * @param frame
     * @param object_id
     * @return true
     * @return false
     */
    bool isMoving(FrameId frame, ObjectId object_id) const {
        //check current frame and prev frame
        if(checkExists(frame, object_id, bbox_data_) && checkExists(frame-1, object_id, bbox_data_)) {
            return bbox_data_.at(frame - 1).at(object_id).is_moving;
        }
        return false;
    }


private:
    /**
     * @brief Nested map mapping frame id to a map of object ids to a type T
     * Allows fast access of object per frame
     *
     * @tparam T
     */
    template<typename T>
    using FrameObjectIDMap = std::map<FrameId, std::map<ObjectId, T>>;


    template<typename T>
    bool checkExists(FrameId frame_id, ObjectId object_id, const FrameObjectIDMap<T>& map) const {
        auto it = map.find(frame_id);
        if(it == map.end()) return false;

        const auto& inner_map = it->second;
        auto it_inner = inner_map.find(object_id);

        if(it_inner == inner_map.end()) return false;

        return true;
    }

    template<typename T>
    void addToFrameObjectMap(FrameId frame_id, ObjectId object_id, T t, FrameObjectIDMap<T>& map) {
        if(map.find(frame_id) == map.end()) {
            map[frame_id] = std::map<ObjectId, T>();
        }

        map[frame_id][object_id] = t;
    }

    /// @brief Storing metadata from the bbox.txt
    struct BBoxMetaData {
        FrameId frame;
        ObjectId track_id; //!to be treated as object id
        cv::Rect bbox;
        bool is_moving; //!flag to indicate whether the object is really moving between this frame and the next one
    };


private:
    void loadBBoxMetaData(FrameObjectIDMap<BBoxMetaData>& bbox_metadata) {
        std::ifstream fstream;
        fstream.open(bbox_path, std::ios::in);
        if(!fstream) {
            throw std::runtime_error("Could not open file " + bbox_path + " when trying to load BBox metadata for Virtual Kitti dataset!");
        }

        LOG(INFO) << "Loading BBox metadta for virtual kitti datasetat path: " << bbox_path;

        static const std::map<std::string, int> header_map = {
            {"frame", 0},
            {"cameraID", 1},
            {"trackID", 2},
            {"left", 3},
            {"right", 4},
            {"top", 5},
            {"bottom", 6},
            {"number_pixels", 7},
            {"truncation_ratio", 8},
            {"occupancy_ratio", 9},
            {"isMoving", 10},
        };

        auto convert_string_bool = [](const std::string& string_bool) -> bool {
            if(string_bool == "True") return true;
            else if(string_bool == "False") return false;
            else throw std::runtime_error("Unknown string bool " + string_bool + " when attempting to covnert to bool type");
        };

        bool is_first = true; //to access header
        while (!fstream.eof()) {
            std::vector<std::string> split_line;
            if(!getLine(fstream, split_line)) continue;

            CHECK_EQ(split_line.size(), header_map.size()) << "Header map and the line present in the bbox.txt file do not have the same length - " << container_to_string(split_line);

            if(is_first) {
                //check header and validate
                for(const auto& [header_value, index] : header_map) {
                    const std::string value = split_line.at(index);
                    if(value != header_value) {
                        throw std::runtime_error("Header value mismatch - " + value + " != " + header_value);
                    }
                }
                is_first = false;
            }
            else {
                const int frame =  std::stoi(split_line.at(header_map.at("frame")));
                const int camera_id =  std::stoi(split_line.at(header_map.at("cameraID")));
                const int track_id = getTrackID(split_line, header_map.at("trackID"));
                const int left =  std::stoi(split_line.at(header_map.at("left")));
                const int right =  std::stoi(split_line.at(header_map.at("right")));
                const int top =  std::stoi(split_line.at(header_map.at("top")));
                const int bottom =  std::stoi(split_line.at(header_map.at("bottom")));
                // const int number_pixels =  std::stoi(split_line.at(header_map.at("number_pixels")));
                // const double truncation_ratio =  std::stod(split_line.at(header_map.at("truncation_ratio")));
                // const double occupancy_ratio =  std::stod(split_line.at(header_map.at("occupancy_ratio")));
                const bool is_moving =  convert_string_bool(split_line.at(header_map.at("isMoving")));

                if(camera_id != 0) {
                    continue; //hardcoded for only one camera atm
                }

                BBoxMetaData bbox_data;
                bbox_data.frame = frame;
                bbox_data.track_id = track_id;
                bbox_data.is_moving = is_moving;

                //convert to cv::Rect where image starts top right
                cv::Point tl(left, top);
                cv::Point br(right, bottom);
                bbox_data.bbox = cv::Rect(tl, br);

                addToFrameObjectMap(frame, track_id, bbox_data, bbox_metadata);
            }
        }
    }


    void loadPoseTxt(FrameObjectIDMap<ObjectPoseGT>& object_poses, const FrameObjectIDMap<BBoxMetaData>& bbox_metadata) {
        std::ifstream fstream;
        fstream.open(pose_path, std::ios::in);
        if(!fstream) {
            throw std::runtime_error("Could not open file " + pose_path + " when trying to load Object pose metadata for Virtual Kitti dataset!");
        }

        LOG(INFO) << "Loading Object pose metadta for virtual kitti datasetat path: " << pose_path;

        static const std::map<std::string, int> header_map = {
            {"frame", 0},
            {"cameraID", 1},
            {"trackID", 2},
            {"alpha", 3},
            {"width", 4},
            {"height", 5},
            {"length", 6},
            {"world_space_X", 7},
            {"world_space_Y", 8},
            {"world_space_Z", 9},
            {"rotation_world_space_y", 10},
            {"rotation_world_space_x", 11},
            {"rotation_world_space_z", 12},
            {"camera_space_X", 13},
            {"camera_space_Y", 14},
            {"camera_space_Z", 15},
            {"rotation_camera_space_y", 16},
            {"rotation_camera_space_x", 17},
            {"rotation_camera_space_z", 18},
        };

        bool is_first = true; //to access header
        while (!fstream.eof()) {
            std::vector<std::string> split_line;
            if(!getLine(fstream, split_line)) continue;


            CHECK_EQ(split_line.size(), header_map.size()) << "Header map and the line present in the pose.txt file do not have the same length";

             if(is_first) {
                //check header and validate
                for(const auto& [header_value, index] : header_map) {
                    CHECK_EQ(split_line.at(index), header_value);
                }
                is_first = false;
            }
            else {

                const int frame =  std::stoi(split_line.at(header_map.at("frame")));
                const int camera_id =  std::stoi(split_line.at(header_map.at("cameraID")));
                const int track_id = getTrackID(split_line, header_map.at("trackID"));

                if(camera_id != 0) {
                    continue; //hardcoded for only one camera atm
                }

                // const double alpha = std::stod(split_line.at(header_map.at("alpha")));
                const double width = std::stod(split_line.at(header_map.at("width")));
                const double height = std::stod(split_line.at(header_map.at("height")));
                const double length = std::stod(split_line.at(header_map.at("length")));
                // const double world_space_X =  std::stod(split_line.at(header_map.at("world_space_X")));
                // const double world_space_Y =  std::stod(split_line.at(header_map.at("world_space_Y")));
                // const double world_space_Z =  std::stod(split_line.at(header_map.at("world_space_Z")));
                // const double rotation_world_space_y =  std::stod(split_line.at(header_map.at("rotation_world_space_y")));
                // const double rotation_world_space_x =  std::stod(split_line.at(header_map.at("rotation_world_space_x")));
                // const double rotation_world_space_z =  std::stod(split_line.at(header_map.at("rotation_world_space_z")));
                const double camera_space_X =  std::stod(split_line.at(header_map.at("camera_space_X")));
                const double camera_space_Y =  std::stod(split_line.at(header_map.at("camera_space_Y")));
                const double camera_space_Z =  std::stod(split_line.at(header_map.at("camera_space_Z")));
                const double rotation_camera_space_y =  std::stod(split_line.at(header_map.at("rotation_camera_space_y")));
                const double rotation_camera_space_x =  std::stod(split_line.at(header_map.at("rotation_camera_space_x")));
                const double rotation_camera_space_z =  std::stod(split_line.at(header_map.at("rotation_camera_space_z")));

                ObjectPoseGT object_pose_gt;
                object_pose_gt.frame_id_ = frame;
                object_pose_gt.object_id_ = track_id;
                object_pose_gt.object_dimensions_ = gtsam::Vector3(width, height, length);

                {
                    // assign r vector
                    // double y = rotation_camera_space_y + (3.1415926 / 2);  // +(3.1415926/2)
                    double y = rotation_camera_space_y;  // +(3.1415926/2)
                    double x = rotation_camera_space_x;
                    double z = rotation_camera_space_z;

                    // the angles are in radians.
                    double cy = cos(y);
                    double sy = sin(y);
                    double cx = cos(x);
                    double sx = sin(x);
                    double cz = cos(z);
                    double sz = sin(z);

                    double m00, m01, m02, m10, m11, m12, m20, m21, m22;

                    m00 = cy * cz + sy * sx * sz;
                    m01 = -cy * sz + sy * sx * cz;
                    m02 = sy * cx;
                    m10 = cx * sz;
                    m11 = cx * cz;
                    m12 = -sx;
                    m20 = -sy * cz + cy * sx * sz;
                    m21 = sy * sz + cy * sx * cz;
                    m22 = cy * cx;

                    gtsam::Matrix33 rot;
                    rot(0, 0) = m00;
                    rot(0, 1) = m01;
                    rot(0, 2) = m02;
                    rot(1, 0) = m10;
                    rot(1, 1) = m11;
                    rot(1, 2) = m12;
                    rot(2, 0) = m20;
                    rot(2, 1) = m21;
                    rot(2, 2) = m22;

                    gtsam::Vector3 translation(camera_space_X, camera_space_Y, camera_space_Z);

                    object_pose_gt.L_camera_ = gtsam::Pose3(gtsam::Rot3(rot), translation);
                }

                CHECK(checkExists(frame, track_id, bbox_metadata));

                //check asociated bboxdata has the right info for this object
                const BBoxMetaData& bbox_data = bbox_metadata.at(frame).at(track_id);
                CHECK_EQ(bbox_data.frame, frame);
                CHECK_EQ(bbox_data.track_id, track_id);

                object_pose_gt.bounding_box_ = bbox_data.bbox;
                addToFrameObjectMap(frame, track_id, object_pose_gt, object_poses);

            }

        }
    }

    void loadCameraPoses(std::vector<gtsam::Pose3>& camera_poses) {
        std::ifstream fstream;
        fstream.open(extrinsic_path, std::ios::in);
        if(!fstream) {
            throw std::runtime_error("Could not open file " + extrinsic_path + " when trying to load camera exterinsics for Virtual Kitti dataset!");
        }

        LOG(INFO) << "Camera pose data for virtual kitti datasetat path: " << extrinsic_path;

        bool is_first = true;

        gtsam::Pose3 initial_pose = gtsam::Pose3::Identity();
        bool set_initial_pose = false;

        while (!fstream.eof()) {
            std::vector<std::string> split_line;
            if(!getLine(fstream, split_line)) continue;

            //frame camera_id, 3x4 camera pose (r11, r12, r13, t1....)
            //header: frame cameraID r1,1 r1,2 r1,3 t1 r2,1 r2,2 r2,3 t2 r3,1 r3,2 r3,3 t3 0 0 0 1
            constexpr static size_t kExtrinsicFileLength = 18u;
            CHECK_EQ(split_line.size(), kExtrinsicFileLength);

            if(is_first){

                is_first = false;
            }
            else {
                const int frame =  std::stoi(split_line.at(0));
                const int camera_id =  std::stoi(split_line.at(1));

                if(camera_id != 0) {
                    continue; //hardcoded for only one camera atm
                }

                //sanity check to ensure things are added in order and at the right index (frame is zero indexed)
                //check only after we check the camera id
                CHECK_EQ(frame, camera_poses.size());

                const double r11 =  std::stod(split_line.at(2));
                const double r12 =  std::stod(split_line.at(3));
                const double r13 =  std::stod(split_line.at(4));
                const double t1 =   std::stod(split_line.at(5));
                const double r21 =  std::stod(split_line.at(6));
                const double r22 =  std::stod(split_line.at(7));
                const double r23 =  std::stod(split_line.at(8));
                const double t2 =   std::stod(split_line.at(9));
                const double r31 =  std::stod(split_line.at(10));
                const double r32 =  std::stod(split_line.at(11));
                const double r33 =  std::stod(split_line.at(12));
                const double t3 =   std::stod(split_line.at(13));

                gtsam::Matrix33 rot;
                rot(0, 0) = r11;
                rot(0, 1) = r12;
                rot(0, 2) = r13;
                rot(1, 0) = r21;
                rot(1, 1) = r22;
                rot(1, 2) = r23;
                rot(2, 0) = r31;
                rot(2, 1) = r32;
                rot(2, 2) = r33;

                //last 4 values should be [0 0 0 1] for homogenous matrix form so we just check
                CHECK_EQ(std::stod(split_line.at(14)), 0);
                CHECK_EQ(std::stod(split_line.at(15)), 0);
                CHECK_EQ(std::stod(split_line.at(16)), 0);
                CHECK_EQ(std::stod(split_line.at(17)), 1);

                gtsam::Pose3 pose(gtsam::Rot3(rot), gtsam::Point3(t1, t2, t3));


                const static gtsam::Pose3 camera_to_world(gtsam::Rot3::RzRyRx(M_PI_2, 0, M_PI_2), gtsam::traits<gtsam::Point3>::Identity());
                //pose is in world coordinate convention so we put into camera convention (the camera_to_world.inverse())
                //the pose provided is actually world -> camera but we want camera -> world eg T_world_camera, so we apply the inverse transform
                pose = camera_to_world.inverse() * pose.inverse();

                if(!set_initial_pose) {
                    initial_pose = pose;
                    set_initial_pose = true;
                }

                // offset initial pose so we start at "0, 0, 0"
                pose = initial_pose.inverse() * pose;
                camera_poses.push_back(pose);
            }

        }

    }

    //only include objects that are moving - we will then use the returned vector as the input
    //to the mask loader to remove masks for stationary objects
    std::vector<GroundTruthInputPacket> constructGTPackets(
        const FrameObjectIDMap<ObjectPoseGT>& object_poses,
        const std::vector<gtsam::Pose3>& camera_poses
    ) const
    {

        std::vector<GroundTruthInputPacket> gt_packets;
        for(size_t frame_id = 0; frame_id < camera_poses.size(); frame_id++) {
            GroundTruthInputPacket gt_packet;
            gt_packet.frame_id_ = frame_id;
            gt_packet.X_world_ = camera_poses.at(frame_id);

            const auto& it = object_poses.find(frame_id);
            if(it != object_poses.end()) {
                auto& object_id_pose_map = it->second;
                for(auto& [object_id, object_pose_gt] : object_id_pose_map) {
                    //query for moving
                    // if(isMoving(frame_id, object_id)) {
                        // LOG(INFO) << "Added moving object " << frame_id << " " << object_id;
                        auto object_pose = object_pose_gt;
                        object_pose.L_world_ = gt_packet.X_world_ * object_pose.L_camera_;
                        gt_packet.object_poses_.push_back(object_pose);
                    // }
                }
            }

            //set motions
            if(frame_id > 0) {
                const GroundTruthInputPacket& previous_gt_packet = gt_packets.at(frame_id - 1);
                gt_packet.calculateAndSetMotions(previous_gt_packet);
            }

            gt_packets.push_back(gt_packet);
        }

        return gt_packets;
    }




private:
    const std::string bbox_path;
    const std::string colors_path;
    const std::string extrinsic_path;
    const std::string info_path;
    const std::string intrinsic_path;
    const std::string pose_path;

    FrameObjectIDMap<ObjectPoseGT> object_poses_; //! all object poses
    FrameObjectIDMap<BBoxMetaData> bbox_data_; //! all object poses

    std::vector<gtsam::Pose3> camera_poses_; //! camera poses in the world frame
    std::vector<GroundTruthInputPacket> gt_packets_; //!

};

//to be inherited either for MOTION segmentation or normal instance seg
class VirtualKittiGenericInstanceSegFolder : public GenericVirtualKittiImageLoader {
public:
    DYNO_POINTER_TYPEDEFS(VirtualKittiGenericInstanceSegFolder)

    VirtualKittiGenericInstanceSegFolder(const std::string& path, VirtualKittiTextGtLoader::Ptr gt_loader) : GenericVirtualKittiImageLoader(path), gt_loader_(CHECK_NOTNULL(gt_loader)) {}
    virtual ~VirtualKittiGenericInstanceSegFolder() = default;

    cv::Mat getItem(size_t idx) override {
        std::stringstream ss;
        ss << std::setfill('0') << std::setw(5) << idx;
        const std::string file_path = full_path_  + "/instancegt_" + ss.str() + ".png";
        throwExceptionIfPathInvalid(file_path);

        const GroundTruthInputPacket& gt_packet = gt_loader_->getGTPacket(idx);
        return loadImage(file_path, idx, gt_packet);
    }

protected:
    virtual cv::Mat loadImage(const std::string& file_path, FrameId frame, const GroundTruthInputPacket& gt_packet) = 0;

    /**
     * @brief Loads instance semantic mask from the file path.
     *
     * This loads the image using libpng++ since we need to load the image as an 8bit indexed PNG file (which opencv does not have functionality
     * for). Keeping consistent with the ground truth indexing we use for VirtualKitti we do not need to modify the trackID's within the image
     * since we want the trackID's to be indexed from 1
     *
     * @param file_path
     * @return cv::Mat
     */
    cv::Mat loadSemanticInstanceUnchanged(const std::string& file_path) const {
        png::image<png::index_pixel> index_image(file_path);

        cv::Size size(index_image.get_width(), index_image.get_height());
        cv::Mat semantic_image(size, ImageType::MotionMask::OpenCVType);

        for (size_t y = 0; y < index_image.get_height(); ++y)
        {
            for (size_t x = 0; x < index_image.get_width(); ++x)
            {
                const auto& pixel = index_image.get_pixel(x, y);

                //the semantic label
                int i_byte = static_cast<int>(pixel);
                semantic_image.at<int>(y, x) = i_byte;
            }
        }
        return semantic_image;
    }

protected:
    VirtualKittiTextGtLoader::Ptr gt_loader_;

};

//this one we will have to remove semgentation mask depending on whats in the gt folder
class VirtualKittiMotionSegFolder : public VirtualKittiGenericInstanceSegFolder {

public:
    VirtualKittiMotionSegFolder(const std::string& path, VirtualKittiTextGtLoader::Ptr gt_loader): VirtualKittiGenericInstanceSegFolder(path, gt_loader) {}

    cv::Mat loadImage(const std::string& file_path, FrameId frame, const GroundTruthInputPacket& gt_packet) override {
        CHECK_EQ(frame, gt_packet.frame_id_);

        //remove object in semantic mask
        cv::Mat instance_semantic_mask = loadSemanticInstanceUnchanged(file_path);
        cv::Mat motion_mask;

        //we used to use gt_loader->isMoving function which seems to be back indexed (see the function comment)
        //now we use the motion_info information in the ground truth packet (see removeStaticObjectFromMask())
        //not sure if this actually makes a difference...
        removeStaticObjectFromMask(instance_semantic_mask, motion_mask, gt_packet);
        return motion_mask;
    }


};

class VirtualKittiClassSegmentationDataFolder : public GenericVirtualKittiImageLoader {

public:
    VirtualKittiClassSegmentationDataFolder(const std::string& path) : GenericVirtualKittiImageLoader(path) {}
    cv::Mat getItem(size_t idx) override {
        std::stringstream ss;
        ss << std::setfill('0') << std::setw(5) << idx;
        const std::string file_path = full_path_  + "/classgt_" + ss.str() + ".png";
        throwExceptionIfPathInvalid(file_path);
        return loadClassSegmentation(file_path);
    }

private:
    //TODO: comments
    cv::Mat loadClassSegmentation(const std::string& file_path) const {

        //TODO: for now just hardcode in road!!!
        png::image<png::rgb_pixel> rgb_image(file_path);

        cv::Size size(rgb_image.get_width(), rgb_image.get_height());
        cv::Mat class_segmentation(size, ImageType::ClassSegmentation::OpenCVType);

        for (size_t y = 0; y < rgb_image.get_height(); ++y)
        {
            for (size_t x = 0; x < rgb_image.get_width(); ++x)
            {
                const auto& pixel = rgb_image.get_pixel(x, y);
                const unsigned char red = (unsigned char)pixel.red;
                const unsigned char green = (unsigned char)pixel.green;
                const unsigned char blue = (unsigned char)pixel.blue;

                // class_segmentation_rgb.at<cv::Vec3b>(y, x)[0] = blue;
                // class_segmentation_rgb.at<cv::Vec3b>(y, x)[1] = green;
                // class_segmentation_rgb.at<cv::Vec3b>(y, x)[2] = red;

                //TODO: hardcoded road values from VKITTI2
                if(red == 100 && green == 60 && blue == 100) {
                    class_segmentation.at<int>(y, x) = ImageType::ClassSegmentation::Labels::Road;
                }
                else {
                    class_segmentation.at<int>(y, x) = ImageType::ClassSegmentation::Labels::Undefined;
                }
            }
        }


        return class_segmentation;


    }
};


//instance segmentation for all objects regardless of motion
class VirtualKittiInstanceSegFolder : public VirtualKittiGenericInstanceSegFolder {

public:
    VirtualKittiInstanceSegFolder(const std::string& path, VirtualKittiTextGtLoader::Ptr gt_loader) : VirtualKittiGenericInstanceSegFolder(path, gt_loader) {}

    cv::Mat loadImage(const std::string& file_path, FrameId, const GroundTruthInputPacket&) override {
        return loadSemanticInstanceUnchanged(file_path);
    }

};

class VirtualKittiGTFolder : public DataFolder<GroundTruthInputPacket> {

public:
    VirtualKittiGTFolder(VirtualKittiTextGtLoader::Ptr gt_loader, VirtualKittiTimestampLoader::Ptr timestamp_loader)
    : gt_loader_(CHECK_NOTNULL(gt_loader)),  timestamp_loader_(CHECK_NOTNULL(timestamp_loader)) {}

    std::string getFolderName() const override { return ""; }

    GroundTruthInputPacket getItem(size_t idx) override {
       auto packet = gt_loader_->getGTPacket(idx);
       packet.timestamp_ = timestamp_loader_->getItem(idx);
       return packet;
    }

    VirtualKittiTextGtLoader::Ptr gt_loader_;
    VirtualKittiTimestampLoader::Ptr timestamp_loader_;

};


//everything current assumes camera0!! This includes loading things like extrinsics...
VirtualKittiDataLoader::VirtualKittiDataLoader(const fs::path& dataset_path,  const Params& params) : VirtualKittiDatasetProvider(dataset_path), params_(params) {
    const std::string& scene = params.scene;
    const std::string& scene_type = params.scene_type;
    const auto& mask_type = params.mask_type;
    const std::string path = dataset_path;

    LOG(INFO) << "Starting VirtualKittiDataLoader with path " << path << " requested scene " << scene << " and scene type " << scene_type;
    const std::string depth_folder = path + "/" + v_depth_folder + "/" + scene + "/" + scene_type + "/frames/depth/Camera_0";
    const std::string forward_flow_folder = path + "/" + v_forward_flow_folder + "/" + scene + "/" + scene_type + "/frames/forwardFlow/Camera_0";
    const std::string backward_flow_folder = path + "/" + v_backward_flow_folder + "/" + scene + "/" + scene_type + "/frames/backwardFlow/Camera_0";
    const std::string forward_scene_flow_folder = path + "/" + v_forward_scene_flow_folder + "/" + scene + "/" + scene_type + "/frames/forwardsceneFlow/Camera_0";
    const std::string class_semantics_folder = path + "/" + v_class_semantics_folder + "/" + scene + "/" + scene_type + "/frames/classSegmentation/Camera_0";
    const std::string instance_segmentation_folder = path + "/" + v_instance_segmentation_folder + "/" + scene + "/" + scene_type + "/frames/instanceSegmentation/Camera_0";
    const std::string rgb_folder = path + "/" + v_rgb_folder + "/" + scene + "/" + scene_type + "/frames/rgb/Camera_0";
    const std::string text_gt_folder = path + "/" + v_text_gt_folder + "/" + scene + "/" + scene_type;

    throwExceptionIfPathInvalid(depth_folder);
    throwExceptionIfPathInvalid(forward_flow_folder);
    throwExceptionIfPathInvalid(forward_scene_flow_folder);
    throwExceptionIfPathInvalid(class_semantics_folder);
    throwExceptionIfPathInvalid(instance_segmentation_folder);
    throwExceptionIfPathInvalid(rgb_folder);
    throwExceptionIfPathInvalid(text_gt_folder);

    auto gt_loader = std::make_shared<VirtualKittiTextGtLoader>(text_gt_folder);

    auto timestamp_folder = std::make_shared<VirtualKittiTimestampLoader>(gt_loader->size());
    auto rgb_loader = std::make_shared<VirtualKittiRGBDataFolder>(rgb_folder);
    auto optical_flow_loader = std::make_shared<VirtualKittiFlowDataFolder>(forward_flow_folder);
    auto depth_loader = std::make_shared<VirtualKittiDepthDataFolder>(depth_folder);

    VirtualKittiGenericInstanceSegFolder::Ptr motion_mask_loader = nullptr;
    if(mask_type == MaskType::MOTION) {
        LOG(INFO) << "Using MaskType::MOTION for loading mask";
        motion_mask_loader =  std::make_shared<VirtualKittiMotionSegFolder>(instance_segmentation_folder, gt_loader);
    }
    else if(mask_type == MaskType::SEMANTIC_INSTANCE) {
        LOG(INFO) << "Using MaskType::SEMANTIC_INSTANCE for loading mask";
        motion_mask_loader = std::make_shared<VirtualKittiInstanceSegFolder>(instance_segmentation_folder, gt_loader);
    }
    else {
        LOG(FATAL) << "Unknown MaskType for KittiDataLoader";
    }
    CHECK_NOTNULL(motion_mask_loader);

    auto gt_loader_folder = std::make_shared<VirtualKittiGTFolder>(gt_loader, timestamp_folder);
    auto class_semantics_loader = std::make_shared<VirtualKittiClassSegmentationDataFolder>(class_semantics_folder);

    this->setLoaders(
        timestamp_folder,
        rgb_loader,
        optical_flow_loader,
        depth_loader,
        motion_mask_loader,
        class_semantics_loader,
        gt_loader_folder
    );

    auto callback = [&](size_t frame_id,
        Timestamp timestamp,
        cv::Mat rgb,
        cv::Mat optical_flow,
        cv::Mat depth,
        cv::Mat instance_mask,
        cv::Mat class_semantics,
        GroundTruthInputPacket gt_object_pose_gt) -> bool
    {
        CHECK(timestamp == gt_object_pose_gt.timestamp_);

        CHECK(ground_truth_packet_callback_);
        if(ground_truth_packet_callback_) ground_truth_packet_callback_(gt_object_pose_gt);

        ImageContainer::Ptr image_container = nullptr;

        if(params_.mask_type == MaskType::MOTION) {
            image_container = ImageContainer::Create(
                timestamp,
                frame_id,
                ImageWrapper<ImageType::RGBMono>(rgb),
                ImageWrapper<ImageType::Depth>(depth),
                ImageWrapper<ImageType::OpticalFlow>(optical_flow),
                ImageWrapper<ImageType::MotionMask>(instance_mask),
                ImageWrapper<ImageType::ClassSegmentation>(class_semantics));
        }
        else {
            image_container = ImageContainer::Create(
                timestamp,
                frame_id,
                ImageWrapper<ImageType::RGBMono>(rgb),
                ImageWrapper<ImageType::Depth>(depth),
                ImageWrapper<ImageType::OpticalFlow>(optical_flow),
                ImageWrapper<ImageType::SemanticMask>(instance_mask));
        }


        CHECK(image_container);
        CHECK(image_container_callback_);
        if(image_container_callback_) image_container_callback_(image_container);
        return true;
    };

    this->setCallback(callback);


    //TODO: virtual function in base class to validate starting/end frames as it may be different between datasets?
    setStartingFrame(1u); //have to start at at least 1 so we can index backwards with optical flow

}


void VirtualKittiDataLoader::setCameraParams() {
    CameraParams::IntrinsicsCoeffs intrinsics({ 725.0087, 725.0087, 620.5, 187});
    CameraParams::DistortionCoeffs distortion({ 0, 0, 0, 0 });

    cv::Size image_size(1242, 375);
    camera_params_= CameraParams(intrinsics, distortion, image_size, "radial-tangential");
}



VirtualKittiDataLoader::Params VirtualKittiDataLoader::Params::fromYaml(const std::string& params_folder) {
    YamlParser yaml_parser(params_folder + "DatasetParams.yaml");

    Params params;

    std::string mask_type;
    yaml_parser.getYamlParam("mask_type", &mask_type);
    params.mask_type = maskTypeFromString(mask_type);

    yaml_parser.getYamlParam("scene", &params.scene);
    yaml_parser.getYamlParam("scene_type", &params.scene_type);
    return params;
}


// bool VirtualKittiDataLoader::spin() {
//     return false;
// }

} //dyno
````

## File: dataprovider/ZEDDataProvider.cc
````
// dynosam/dataprovider/ZED2DataProvider.cc

#include "dynosam/dataprovider/ZEDDataProvider.hpp"
#include "dynosam/common/ImageTypes.hpp"
#include "dynosam/utils/OpenCVUtils.hpp"
#include "dynosam/utils/GtsamUtils.hpp"

#include <glog/logging.h>

namespace dyno {

// Helper function moved inside the .cc or to a common utility if ZED2Config is widely used for this
common::ZEDCameraConfig ZEDDataProvider::createZEDCameraConfig(const ZEDConfig& dyno_config) {
    common::ZEDCameraConfig zed_cam_config;
    zed_cam_config.resolution = dyno_config.resolution;
    zed_cam_config.fps = dyno_config.fps;
    zed_cam_config.depth_mode = dyno_config.depth_mode;
    zed_cam_config.coordinate_units = dyno_config.coordinate_units;
    zed_cam_config.coordinate_system_3d = dyno_config.coordinate_system_3d_zed; // Use ZED specific coord system
    zed_cam_config.svo_file_path = dyno_config.svo_file_path;
    zed_cam_config.svo_real_time_mode = dyno_config.svo_real_time_mode;
    zed_cam_config.enable_imu = dyno_config.enable_imu;
    zed_cam_config.enable_right_side_measure = dyno_config.enable_right_side_measure;
    zed_cam_config.sdk_verbose = dyno_config.sdk_verbose_level;
    zed_cam_config.camera_device_id = dyno_config.camera_device_id;
    zed_cam_config.camera_disable_self_calib = dyno_config.camera_disable_self_calib;
    zed_cam_config.enable_image_enhancement = dyno_config.enable_image_enhancement;
    zed_cam_config.open_timeout_sec = dyno_config.open_timeout_sec;
    zed_cam_config.enable_image_validity_check = false;


    // Module specific params
    zed_cam_config.object_detection_model = dyno_config.object_detection_model;
    zed_cam_config.object_detection_enable_tracking = dyno_config.object_detection_enable_tracking;
    zed_cam_config.object_detection_enable_segmentation = dyno_config.object_detection_enable_segmentation;

    zed_cam_config.body_tracking_model = dyno_config.body_tracking_model;
    zed_cam_config.body_format = dyno_config.body_format;
    zed_cam_config.body_tracking_enable_fitting = dyno_config.body_tracking_enable_fitting;
    zed_cam_config.body_tracking_enable_segmentation = dyno_config.body_tracking_enable_segmentation;
    zed_cam_config.body_tracking_allow_reduced_precision_inference = dyno_config.body_tracking_allow_reduced_precision_inference;

    return zed_cam_config;
}

ZEDDataProvider::ZEDDataProvider(const ZEDConfig& config,
                                     DataInterfacePipeline* data_interface_pipeline)
    : DataProvider(data_interface_pipeline),
      config_(config) {
    zed_camera_wrapper_ = std::make_shared<common::ZEDCamera>(createZEDCameraConfig(config_));
    if (!initializeZED()) {
        LOG(FATAL) << "ZED2DataProvider: Failed to initialize ZED camera system.";
    }
}

ZEDDataProvider::ZEDDataProvider(const ZEDConfig& config,
                                   DataInterfacePipelineImu* data_interface_pipeline_imu)
    : DataProvider(data_interface_pipeline_imu),
      config_(config) {
    zed_camera_wrapper_ = std::make_shared<common::ZEDCamera>(createZEDCameraConfig(config_));
    if (!initializeZED()) {
        LOG(FATAL) << "ZED2DataProvider: Failed to initialize ZED camera system.";
    }
}

ZEDDataProvider::~ZEDDataProvider() {
    ZEDDataProvider::shutdown();
}

bool ZEDDataProvider::initializeZED() {
    CHECK_NOTNULL(zed_camera_wrapper_);
    if (!zed_camera_wrapper_->open()) {
        return false;
    }

    zed_runtime_params_.enable_depth = true; // Depth is fundamental for RGBD

    setupDynoSAMCameraParams();
    if (!camera_params_) {
        LOG(ERROR) << "ZED2DataProvider: Failed to setup dynosam CameraParams.";
        zed_camera_wrapper_->shutdown();
        return false;
    }

    if (config_.enable_object_detection) {
        if (!zed_camera_wrapper_->enableObjectDetection()) {
            LOG(ERROR) << "ZED2DataProvider: Failed to enable Object Detection module.";
            // Decide on graceful degradation or failure
        }
    }

    if (config_.enable_body_tracking) {
        if (!zed_camera_wrapper_->enableBodyTracking()) {
            LOG(ERROR) << "ZED2DataProvider: Failed to enable Body Tracking module.";
        }
    }

    LOG(INFO) << "ZED2DataProvider: ZED system initialized successfully.";
    return true;
}

void ZEDDataProvider::setupDynoSAMCameraParams() {
    CHECK_NOTNULL(zed_camera_wrapper_);
    sl::CameraInformation cam_info = zed_camera_wrapper_->getCameraInformation();
    sl::CalibrationParameters calib_params_left = cam_info.camera_configuration.calibration_parameters; // Rectified Left Cam


    sl::Resolution res = cam_info.camera_configuration.resolution;

    CameraParams::IntrinsicsCoeffs intrinsics = {
        calib_params_left.left_cam.fx,
        calib_params_left.left_cam.fy,
        calib_params_left.left_cam.cx,
        calib_params_left.left_cam.cy
    };

    // ZED usually provides rectified images, meaning distortion is handled.
    // For dynosam::CameraParams, pass zero distortion coefficients.
    CameraParams::DistortionCoeffs distortion_coeffs;
    distortion_coeffs.assign(5, 0.0); // Assuming plumb_bob/radtan with 5 zero coeffs

    // The model type still matters for some GTSAM calibrators.
    std::string distortion_model_str = "radial_tangential";
    DistortionModel distortion_model_enum = CameraParams::stringToDistortion(distortion_model_str, "pinhole");

    gtsam::Pose3 T_BS = gtsam::Pose3::Identity();
    if (config_.enable_imu && cam_info.sensors_configuration.isSensorAvailable(sl::SENSOR_TYPE::ACCELEROMETER)) {
        // T_BS is transform from body (IMU) to sensor (Left Camera Optical Frame)
        // ZED SDK provides camera_imu_transform: T_LeftCamera_ImuCenter
        sl::Transform zed_T_camL_imu = cam_info.sensors_configuration.camera_imu_transform;
        gtsam::Rot3 R_camL_imu(zed_T_camL_imu.getRotationMatrix().r00,
                               zed_T_camL_imu.getRotationMatrix().r01,
                               zed_T_camL_imu.getRotationMatrix().r02,
                               zed_T_camL_imu.getRotationMatrix().r10,
                               zed_T_camL_imu.getRotationMatrix().r11,
                               zed_T_camL_imu.getRotationMatrix().r12,
                               zed_T_camL_imu.getRotationMatrix().r20,
                               zed_T_camL_imu.getRotationMatrix().r21,
                               zed_T_camL_imu.getRotationMatrix().r22);
        gtsam::Point3 T_camL_imu(zed_T_camL_imu.getTranslation().x,
                                         zed_T_camL_imu.getTranslation().y,
                                         zed_T_camL_imu.getTranslation().z);
        T_BS = gtsam::Pose3(R_camL_imu, T_camL_imu).inverse();
    }

    camera_params_ = CameraParams(intrinsics,
                                  distortion_coeffs,
                                  cv::Size(static_cast<int>(res.width), static_cast<int>(res.height)),
                                  distortion_model_enum,
                                  T_BS);
}

cv::Mat ZEDDataProvider::slMat_to_cvMat(const sl::Mat& sl_mat) {
    // Implementation remains the same as it's a generic utility.
    if (sl_mat.getMemoryType() == sl::MEM::GPU) {
         LOG(FATAL) << "GPU sl::Mat to cv::Mat conversion not directly supported without CPU copy.";
    }
    int cv_type = -1;
    switch (sl_mat.getDataType()) {
        case sl::MAT_TYPE::F32_C1: cv_type = CV_32FC1; break;
        case sl::MAT_TYPE::F32_C2: cv_type = CV_32FC2; break;
        case sl::MAT_TYPE::F32_C3: cv_type = CV_32FC3; break;
        case sl::MAT_TYPE::F32_C4: cv_type = CV_32FC4; break;
        case sl::MAT_TYPE::U8_C1:  cv_type = CV_8UC1;  break;
        case sl::MAT_TYPE::U8_C2:  cv_type = CV_8UC2;  break;
        case sl::MAT_TYPE::U8_C3:  cv_type = CV_8UC3;  break;
        case sl::MAT_TYPE::U8_C4:  cv_type = CV_8UC4;  break;
        default: LOG(FATAL) << "Unsupported sl::MAT_TYPE for cv::Mat conversion: " << sl_mat.getDataType();
    }
    return cv::Mat(sl_mat.getHeight(), sl_mat.getWidth(), cv_type,
                   sl_mat.getPtr<sl::uchar1>(sl::MEM::CPU), sl_mat.getStepBytes(sl::MEM::CPU));
}

ImageContainer::Ptr ZEDDataProvider::createImageContainer(
    Timestamp timestamp,
    FrameId frame_id,
    const sl::Mat& left_sl,
    const sl::Mat& depth_sl,
    const std::optional<sl::Objects>& objects,
    const std::optional<sl::Bodies>& bodies) const {
    // Implementation remains largely the same, focuses on data conversion and ImageContainer creation.
    cv::Mat left_cv_native = slMat_to_cvMat(left_sl);
    cv::Mat left_cv_dynosam;

    if (config_.output_rgb) {
        if (left_cv_native.channels() == 4) cv::cvtColor(left_cv_native, left_cv_dynosam, cv::COLOR_BGRA2BGR);
        else if (left_cv_native.channels() == 3) left_cv_dynosam = left_cv_native.clone();
        else if (left_cv_native.channels() == 1) cv::cvtColor(left_cv_native, left_cv_dynosam, cv::COLOR_GRAY2BGR);
        else LOG(FATAL) << "Unsupported channel count for RGB: " << left_cv_native.channels();
    } else {
        if (left_cv_native.channels() == 4) cv::cvtColor(left_cv_native, left_cv_dynosam, cv::COLOR_BGRA2GRAY);
        else if (left_cv_native.channels() == 3) cv::cvtColor(left_cv_native, left_cv_dynosam, cv::COLOR_BGR2GRAY);
        else if (left_cv_native.channels() == 1) left_cv_dynosam = left_cv_native.clone();
        else LOG(FATAL) << "Unsupported channel count for Grayscale: " << left_cv_native.channels();
    }
    ImageWrapper<ImageType::RGBMono> rgb_mono_wrapper(left_cv_dynosam);

    cv::Mat depth_cv_native = slMat_to_cvMat(depth_sl);
    cv::Mat depth_cv_dynosam;
    if (depth_cv_native.type() == CV_32FC1) {
        depth_cv_native.convertTo(depth_cv_dynosam, CV_64F);
    } else {
        LOG(ERROR) << "ZED Depth map is not CV_32FC1: " << utils::cvTypeToString(depth_cv_native.type());
        depth_cv_native.convertTo(depth_cv_dynosam, CV_64F);
    }
    ImageWrapper<ImageType::Depth> depth_wrapper(depth_cv_dynosam);

    ImageWrapper<ImageType::OpticalFlow> optical_flow_wrapper; // ZED doesn't provide this directly; DynoSAM calculates it.
    ImageWrapper<ImageType::MotionMask> motion_mask_wrapper;
    ImageWrapper<ImageType::ClassSegmentation> class_segmentation_wrapper; // Not directly from ZED, unless using custom models.

    if (config_.enable_object_detection || config_.enable_body_tracking) {
        ImageType::MotionMask dummy_type_indicator;
        cv::Mat generated_mask = generateMaskFromZEDDetections(objects, bodies, rgb_mono_wrapper.image.size(), dummy_type_indicator);
        if (!generated_mask.empty()) {
            motion_mask_wrapper = ImageWrapper<ImageType::MotionMask>(generated_mask);
        }
    }

    return ImageContainer::Create(timestamp, frame_id, rgb_mono_wrapper, depth_wrapper,
                                  optical_flow_wrapper, motion_mask_wrapper, class_segmentation_wrapper);
}

cv::Mat ZEDDataProvider::generateMaskFromZEDDetections(
    const std::optional<sl::Objects>& zed_objects,
    const std::optional<sl::Bodies>& zed_bodies,
    const cv::Size& image_size,
    ImageType::MotionMask& /*mask_type_indicator*/) const {
    // Implementation remains the same.
    cv::Mat combined_mask = cv::Mat::zeros(image_size, CV_32SC1); // DynoSAM expects CV_32SC1 for masks

    if (config_.enable_object_detection && zed_objects.has_value()) {
        for (const auto& obj : zed_objects->object_list) {
            if (config_.object_detection_enable_segmentation && obj.mask.isInit() && !obj.mask.getInfos().empty()) {
                cv::Mat obj_cv_mask = slMat_to_cvMat(obj.mask);
                for (int r = 0; r < obj_cv_mask.rows; ++r) {
                    for (int c = 0; c < obj_cv_mask.cols; ++c) {
                        if (obj_cv_mask.at<unsigned char>(r,c) > 0) {
                             if (r < combined_mask.rows && c < combined_mask.cols) {
                                combined_mask.at<int32_t>(r,c) = static_cast<int32_t>(obj.id);
                            }
                        }
                    }
                }
            } else { // Fallback to bounding box
                int min_x = static_cast<int>(std::min({obj.bounding_box_2d[0].x, obj.bounding_box_2d[3].x}));
                int max_x = static_cast<int>(std::max({obj.bounding_box_2d[1].x, obj.bounding_box_2d[2].x}));
                int min_y = static_cast<int>(std::min({obj.bounding_box_2d[0].y, obj.bounding_box_2d[1].y}));
                int max_y = static_cast<int>(std::max({obj.bounding_box_2d[2].y, obj.bounding_box_2d[3].y}));
                cv::Rect roi(cv::Point(min_x, min_y), cv::Point(max_x, max_y));
                roi &= cv::Rect(0,0, image_size.width, image_size.height); // Intersect with image bounds
                if(roi.area() > 0) combined_mask(roi).setTo(static_cast<int32_t>(obj.id));
            }
        }
    }

    if (config_.enable_body_tracking && zed_bodies.has_value()) {
        for (const auto& body : zed_bodies->body_list) {
            ObjectId body_mask_id = static_cast<ObjectId>(body.id);
            if (config_.enable_object_detection) body_mask_id += 1000; // Offset

            if (config_.body_tracking_enable_segmentation && body.mask.isInit() && !body.mask.getInfos().empty()){
                 cv::Mat body_cv_mask = slMat_to_cvMat(body.mask);
                 for (int r = 0; r < body_cv_mask.rows; ++r) {
                    for (int c = 0; c < body_cv_mask.cols; ++c) {
                        if (body_cv_mask.at<unsigned char>(r,c) > 0) {
                             if (r < combined_mask.rows && c < combined_mask.cols) {
                                combined_mask.at<int32_t>(r,c) = body_mask_id;
                            }
                        }
                    }
                }
            } else { // Fallback to bounding box for bodies
                int min_x = static_cast<int>(std::min({body.bounding_box_2d[0].x, body.bounding_box_2d[3].x}));
                int max_x = static_cast<int>(std::max({body.bounding_box_2d[1].x, body.bounding_box_2d[2].x}));
                int min_y = static_cast<int>(std::min({body.bounding_box_2d[0].y, body.bounding_box_2d[1].y}));
                int max_y = static_cast<int>(std::max({body.bounding_box_2d[2].y, body.bounding_box_2d[3].y}));
                cv::Rect roi(cv::Point(min_x, min_y), cv::Point(max_x, max_y));
                roi &= cv::Rect(0,0, image_size.width, image_size.height); // Intersect
                if(roi.area() > 0) combined_mask(roi).setTo(body_mask_id);
            }
        }
    }
    return combined_mask;
}

bool ZEDDataProvider::spin() {
    if (is_shutting_down_.load() || !zed_camera_wrapper_ || !zed_camera_wrapper_->isOpened()) {
        return false;
    }

    sl::ERROR_CODE err = zed_camera_wrapper_->grab(zed_runtime_params_);
    if (err == sl::ERROR_CODE::SUCCESS) {
        sl::Mat left_image_sl, depth_map_sl;
        // Retrieve left image (BGRA typically)
        zed_camera_wrapper_->retrieveRawImage(sl::VIEW::LEFT, left_image_sl, sl::MEM::CPU);
        // Retrieve depth map (F32_C1)
        zed_camera_wrapper_->retrieveRawDepth(depth_map_sl, sl::MEM::CPU);

        Timestamp timestamp_ns = left_image_sl.timestamp.getNanoseconds();
        Timestamp timestamp_s = static_cast<double>(timestamp_ns) * 1e-9;


        std::optional<sl::Objects> zed_objects_opt = std::nullopt;
        if (config_.enable_object_detection && zed_camera_wrapper_->isObjectDetectionEnabled()) {
            sl::Objects objects_data;
            sl::ObjectDetectionRuntimeParameters od_rt_params;
            od_rt_params.detection_confidence_threshold = config_.detection_confidence_threshold_od;
            if (zed_camera_wrapper_->retrieveObjects(objects_data, od_rt_params) == sl::ERROR_CODE::SUCCESS) {
                zed_objects_opt = objects_data;
            }
        }

        std::optional<sl::Bodies> zed_bodies_opt = std::nullopt;
        if (config_.enable_body_tracking && zed_camera_wrapper_->isBodyTrackingEnabled()) {
            sl::Bodies bodies_data;
            sl::BodyTrackingRuntimeParameters bt_rt_params;
            bt_rt_params.detection_confidence_threshold = config_.detection_confidence_threshold_bt;
            if (zed_camera_wrapper_->retrieveBodies(bodies_data, bt_rt_params) == sl::ERROR_CODE::SUCCESS) {
                zed_bodies_opt = bodies_data;
            }
        }

        ImageContainer::Ptr image_container = createImageContainer(
            timestamp_s, frame_id_counter_, left_image_sl, depth_map_sl, zed_objects_opt, zed_bodies_opt);

        CHECK(image_container_callback_);
        image_container_callback_(image_container);

        if (config_.enable_imu && zed_camera_wrapper_->isImuEnabled()) {
            if (imu_single_input_callback_) {
                sl::SensorsData sensors_data;
                if (zed_camera_wrapper_->retrieveRawSensorsData(sensors_data, sl::TIME_REFERENCE::IMAGE) == sl::ERROR_CODE::SUCCESS) {
                    ImuMeasurement imu_measurement;
                    imu_measurement.timestamp_ = static_cast<double>(sensors_data.imu.timestamp.getNanoseconds()) * 1e-9;
                    // ZED IMU (NED): X Fwd, Y Right, Z Down
                    // DynoSAM (FLU assumed): X Fwd, Y Left, Z Up
                    imu_measurement.acc_gyr_ << sensors_data.imu.linear_acceleration.x,    // Acc X
                                               -sensors_data.imu.linear_acceleration.y,   // Acc Y
                                               -sensors_data.imu.linear_acceleration.z,   // Acc Z
                                                sensors_data.imu.angular_velocity.x,     // Gyro X (rad/s)
                                               -sensors_data.imu.angular_velocity.y,     // Gyro Y (rad/s)
                                               -sensors_data.imu.angular_velocity.z;     // Gyro Z (rad/s)
                    imu_single_input_callback_(imu_measurement);
                }
            } else {
                LOG_EVERY_N(WARNING, 100) << "IMU processing requested, but DataInterfacePipeline does not support IMU callbacks.";
            }
        }

        frame_id_counter_++;
        return true;
    } else if (err == sl::ERROR_CODE::END_OF_SVOFILE_REACHED) {
        LOG(INFO) << "ZED2DataProvider: End of SVO file reached.";
        shutdown();
        return false;
    } else {
        LOG(ERROR) << "ZED2DataProvider: ZEDCamera::grab() failed: " << sl::toString(err);
        shutdown();
        return false;
    }
}

void ZEDDataProvider::shutdown() {
    if (is_shutting_down_.exchange(true) == false) {
        LOG(INFO) << "ZED2DataProvider: Shutting down...";
        if (zed_camera_wrapper_) {
            zed_camera_wrapper_->shutdown();
        }
        DataProvider::shutdown();
    }
}

CameraParams::Optional ZEDDataProvider::getCameraParams() const {
    return camera_params_;
}

int ZEDDataProvider::datasetSize() const {
    if (zed_camera_wrapper_ && zed_camera_wrapper_->isSVOMode()) {
        return zed_camera_wrapper_->getSVONumberOfFrames();
    }
    return -1; // Live camera indication
}

} // namespace dyno
````

## File: factors/LandmarkMotionPoseFactor.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/factors/LandmarkMotionPoseFactor.hpp"

namespace dyno {

LandmarkMotionPoseFactor::LandmarkMotionPoseFactor(gtsam::Key previousPointKey, gtsam::Key currentPointKey, gtsam::Key previousPoseKey, gtsam::Key currentPoseKey, gtsam::SharedNoiseModel model)
    :   Base(model, previousPointKey, currentPointKey, previousPoseKey, currentPoseKey) {}


gtsam::Vector LandmarkMotionPoseFactor::evaluateError(const gtsam::Point3& previousPoint, const gtsam::Point3& currentPoint,
                              const gtsam::Pose3& previousPose, const gtsam::Pose3& currentPose,
                              boost::optional<gtsam::Matrix&> J1,
                              boost::optional<gtsam::Matrix&> J2,
                              boost::optional<gtsam::Matrix&> J3,
                              boost::optional<gtsam::Matrix&> J4) const
{
    if(J1) {
        // error w.r.t to previous point in world
        Eigen::Matrix<double, 3, 3> df_dp_k_1 =
            gtsam::numericalDerivative41<gtsam::Vector3, gtsam::Point3, gtsam::Point3, gtsam::Pose3, gtsam::Pose3>(
                std::bind(&LandmarkMotionPoseFactor::residual, std::placeholders::_1,
                    std::placeholders::_2, std::placeholders::_3,  std::placeholders::_4),
            previousPoint, currentPoint, previousPose, currentPose);
        *J1 = df_dp_k_1;
    }

    if(J2) {
        // error w.r.t to current point in world
        Eigen::Matrix<double, 3, 3> df_dp_k =
            gtsam::numericalDerivative42<gtsam::Vector3, gtsam::Point3, gtsam::Point3, gtsam::Pose3, gtsam::Pose3>(
                std::bind(&LandmarkMotionPoseFactor::residual, std::placeholders::_1,
                    std::placeholders::_2, std::placeholders::_3,  std::placeholders::_4),
            previousPoint, currentPoint, previousPose, currentPose);
        *J2 = df_dp_k;
    }

    if(J3) {
        // error w.r.t to previous pose in world
        Eigen::Matrix<double, 3, 6> df_dL_k_1 =
            gtsam::numericalDerivative43<gtsam::Vector3, gtsam::Point3, gtsam::Point3, gtsam::Pose3, gtsam::Pose3>(
                std::bind(&LandmarkMotionPoseFactor::residual, std::placeholders::_1,
                    std::placeholders::_2, std::placeholders::_3,  std::placeholders::_4),
            previousPoint, currentPoint, previousPose, currentPose);
        *J3 = df_dL_k_1;
    }

    if(J4) {
        // error w.r.t to current pose in world
        Eigen::Matrix<double, 3, 6> df_dL_k =
            gtsam::numericalDerivative44<gtsam::Vector3, gtsam::Point3, gtsam::Point3, gtsam::Pose3, gtsam::Pose3>(
                std::bind(&LandmarkMotionPoseFactor::residual, std::placeholders::_1,
                    std::placeholders::_2, std::placeholders::_3,  std::placeholders::_4),
            previousPoint, currentPoint, previousPose, currentPose);
        *J4 = df_dL_k;
    }

    return residual(previousPoint, currentPoint, previousPose, currentPose);

}

gtsam::Vector LandmarkMotionPoseFactor::residual(const gtsam::Point3& previousPoint, const gtsam::Point3& currentPoint,
                        const gtsam::Pose3& previousPose, const gtsam::Pose3& currentPose)
{
    return gtsam::Vector3(
        currentPoint - (currentPose * previousPose.inverse() * previousPoint)
    );
}


} //dyno
````

## File: factors/LandmarkMotionTernaryFactor.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/factors/LandmarkMotionTernaryFactor.hpp"


namespace dyno {

LandmarkMotionTernaryFactor::LandmarkMotionTernaryFactor(gtsam::Key previousPointKey, gtsam::Key currentPointKey,
                                                         gtsam::Key motionKey,
                                                         gtsam::SharedNoiseModel model)
  : gtsam::NoiseModelFactor3<gtsam::Point3, gtsam::Point3, gtsam::Pose3>(model, previousPointKey, currentPointKey,
                                                                         motionKey)
{
}

gtsam::Vector LandmarkMotionTernaryFactor::evaluateError(const gtsam::Point3& previousPoint,
                                                         const gtsam::Point3& currentPoint, const gtsam::Pose3& H,
                                                         boost::optional<gtsam::Matrix&> J1,
                                                         boost::optional<gtsam::Matrix&> J2,
                                                         boost::optional<gtsam::Matrix&> J3) const
{
  gtsam::Vector3 l2H = H.inverse() * currentPoint;
  gtsam::Vector3 expected = previousPoint - l2H;

  if (J1)
  {
    *J1 = (gtsam::Matrix33() << 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0).finished();
  }

  if (J2)
  {
    *J2 = -H.inverse().rotation().matrix();
  }

  if (J3)
  {
    Eigen::Matrix<double, 3, 6, Eigen::ColMajor> J;
    J.fill(0);
    J.block<3, 3>(0, 3) = gtsam::Matrix33::Identity();
    gtsam::Vector3 invHl2 = H.inverse() * currentPoint;
    J(0, 1) = invHl2(2);
    J(0, 2) = -invHl2(1);
    J(1, 0) = -invHl2(2);
    J(1, 2) = invHl2(0);
    J(2, 0) = invHl2(1);
    J(2, 1) = -invHl2(0);
    *J3 = J;
  }

  // return error vector
  return expected;
}



}
````

## File: factors/LandmarkPoseSmoothingFactor.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/factors/LandmarkPoseSmoothingFactor.hpp"
#include <gtsam/geometry/Pose3.h>

namespace dyno {

 gtsam::Vector LandmarkPoseSmoothingFactor::evaluateError(const gtsam::Pose3& pose_k_2,
                              const gtsam::Pose3& pose_k_1,
                              const gtsam::Pose3& pose_k,
                              boost::optional<gtsam::Matrix&> J1,
                              boost::optional<gtsam::Matrix&> J2,
                              boost::optional<gtsam::Matrix&> J3) const
{
    //TODO: do analytically!!
    if(J1) {
        // error.w.r.t pose at k-2
        Eigen::Matrix<double, 6, 6> df_dp_k_2 =
            gtsam::numericalDerivative31<gtsam::Vector6, gtsam::Pose3, gtsam::Pose3, gtsam::Pose3>(
                std::bind(&LandmarkPoseSmoothingFactor::residual, std::placeholders::_1,
                    std::placeholders::_2, std::placeholders::_3),
                pose_k_2, pose_k_1, pose_k
            );
        *J1 = df_dp_k_2;
    }

    if(J2) {
        // error.w.r.t pose at k-1
        Eigen::Matrix<double, 6, 6> df_dp_k_1 =
            gtsam::numericalDerivative32<gtsam::Vector6, gtsam::Pose3, gtsam::Pose3, gtsam::Pose3>(
                std::bind(&LandmarkPoseSmoothingFactor::residual, std::placeholders::_1,
                    std::placeholders::_2, std::placeholders::_3),
                pose_k_2, pose_k_1, pose_k
            );
        *J2 = df_dp_k_1;
    }

    if(J3) {
        // error.w.r.t pose at k
        Eigen::Matrix<double, 6, 6> df_dp_k =
            gtsam::numericalDerivative33<gtsam::Vector6, gtsam::Pose3, gtsam::Pose3, gtsam::Pose3>(
                std::bind(&LandmarkPoseSmoothingFactor::residual, std::placeholders::_1,
                    std::placeholders::_2, std::placeholders::_3),
                pose_k_2, pose_k_1, pose_k
            );
        *J3 = df_dp_k;
    }

    return residual(pose_k_2, pose_k_1, pose_k);

}

gtsam::Vector LandmarkPoseSmoothingFactor::residual(const gtsam::Pose3& pose_k_2, const gtsam::Pose3& pose_k_1, const gtsam::Pose3& pose_k) {
    const gtsam::Pose3 k_2_H_k_1 = pose_k_1 * pose_k_2.inverse();
    const gtsam::Pose3 k_1_H_k = pose_k * pose_k_1.inverse();

    gtsam::Pose3 hx = gtsam::traits<gtsam::Pose3>::Between(k_2_H_k_1, k_1_H_k, boost::none, boost::none); // h(x)
    static const gtsam::Pose3 Z = gtsam::Pose3::Identity();


    return gtsam::traits<gtsam::Pose3>::Local(Z, hx);
}



}
````

## File: factors/ObjectKinematicFactor.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/factors/ObjectKinematicFactor.hpp"
#include <gtsam/base/numericalDerivative.h>

namespace dyno {

ObjectKinematicFactor::ObjectKinematicFactor(gtsam::Key motionKey, gtsam::Key previousObjectPoseKey,
                                           gtsam::Key currentObjectPoseKey, gtsam::SharedNoiseModel model)
  : gtsam::NoiseModelFactor3<gtsam::Pose3, gtsam::Pose3, gtsam::Pose3>(model, motionKey, previousObjectPoseKey,
                                                                       currentObjectPoseKey)
{
}

gtsam::Vector6 ObjectKinematicFactor::calculateResidual(const gtsam::Pose3& H_w, const gtsam::Pose3& L_previous_world,
                                                       const gtsam::Pose3& L_current_world)
{
  gtsam::Pose3 propogated = H_w * L_previous_world;
  gtsam::Pose3 Hx = L_current_world.between(propogated);
  return gtsam::Pose3::Identity().localCoordinates(Hx);
}

gtsam::Vector ObjectKinematicFactor::evaluateError(const gtsam::Pose3& H_w, const gtsam::Pose3& L_previous_world,
                                                  const gtsam::Pose3& L_current_world,
                                                  boost::optional<gtsam::Matrix&> J1,
                                                  boost::optional<gtsam::Matrix&> J2,
                                                  boost::optional<gtsam::Matrix&> J3) const
{
  if (J1)
  {
    *J1 = gtsam::numericalDerivative31<gtsam::Vector6, gtsam::Pose3, gtsam::Pose3, gtsam::Pose3>(
        std::bind(&ObjectKinematicFactor::calculateResidual, std::placeholders::_1, std::placeholders::_2,
                  std::placeholders::_3),
        H_w, L_previous_world, L_current_world);
  }

  if (J2)
  {
    *J2 = gtsam::numericalDerivative32<gtsam::Vector6, gtsam::Pose3, gtsam::Pose3, gtsam::Pose3>(
        std::bind(&ObjectKinematicFactor::calculateResidual, std::placeholders::_1, std::placeholders::_2,
                  std::placeholders::_3),
        H_w, L_previous_world, L_current_world);
  }

  if (J3)
  {
    *J3 = gtsam::numericalDerivative33<gtsam::Vector6, gtsam::Pose3, gtsam::Pose3, gtsam::Pose3>(
        std::bind(&ObjectKinematicFactor::calculateResidual, std::placeholders::_1, std::placeholders::_2,
                  std::placeholders::_3),
        H_w, L_previous_world, L_current_world);
  }

  return calculateResidual(H_w, L_previous_world, L_current_world);
}

}
````

## File: factors/Pose3FlowProjectionFactor.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/factors/Pose3FlowProjectionFactor.h"

namespace dyno {}
````

## File: frontend/anms/anms.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

/**
MIT License

Copyright (c) 2018 Oleksandr Bailo

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

#include "dynosam/frontend/anms/anms/anms.h"

#include <stdlib.h>
#include <iostream>

#include <opencv2/opencv.hpp>

#include "dynosam/frontend/anms/anms/nanoflann.hpp"
#include "dynosam/frontend/anms/anms/range-tree/ranget.h"

namespace anms {

std::vector<cv::KeyPoint> TopN(const std::vector<cv::KeyPoint>& keyPoints,
                               int numRetPoints) {
  if (static_cast<size_t>(numRetPoints) > keyPoints.size()) {
    return keyPoints;
  }

  std::vector<cv::KeyPoint> kp;
  for (int i = 0; i < numRetPoints; i++)
    kp.push_back(keyPoints[i]);  // simply extracting numRetPoints keyPoints

  return kp;
}

std::vector<cv::KeyPoint> BrownANMS(const std::vector<cv::KeyPoint>& keyPoints,
                                    int numRetPoints) {
  if (static_cast<size_t>(numRetPoints) > keyPoints.size()) {
    return keyPoints;
  }

  std::vector<pair<float, int> > results;
  results.push_back(make_pair(FLT_MAX, 0));
  for (unsigned int i = 1; i < keyPoints.size();
       ++i) {  // for every keyPoint we get the min distance to the previously
               // visited keyPoints
    float minDist = FLT_MAX;
    for (unsigned int j = 0; j < i; ++j) {
      float exp1 = (keyPoints[j].pt.x - keyPoints[i].pt.x);
      float exp2 = (keyPoints[j].pt.y - keyPoints[i].pt.y);
      float curDist = sqrt(exp1 * exp1 + exp2 * exp2);
      minDist = min(curDist, minDist);
    }
    results.push_back(make_pair(minDist, i));
  }
  sort(results.begin(), results.end(), sort_pred());  // sorting by radius
  std::vector<cv::KeyPoint> kp;
  for (int i = 0; i < numRetPoints; ++i)
    kp.push_back(
        keyPoints[results[i].second]);  // extracting numRetPoints keyPoints

  return kp;
}

std::vector<cv::KeyPoint> Sdc(const std::vector<cv::KeyPoint>& keyPoints,
                              int numRetPoints,
                              float tolerance,
                              int cols,
                              int rows) {
  double eps_var = 0.25;  // this parameter is chosen to be the most optimal in
                          // the original paper

  int low = 1;
  int high = cols;  // binary search range initialization
  int radius;
  int prevradius = -1;

  std::vector<int> ResultVec;
  bool complete = false;
  unsigned int K = numRetPoints;
  unsigned int Kmin = round(K - (K * tolerance));
  unsigned int Kmax = round(K + (K * tolerance));

  std::vector<int> result;
  result.reserve(keyPoints.size());
  while (!complete) {
    radius = low + (high - low) / 2;
    if (radius == prevradius ||
        low >
            high) {  // needed to reassure the same radius is not repeated again
      ResultVec = result;  // return the keypoints from the previous iteration
      break;
    }
    result.clear();
    double c = eps_var * radius / sqrt(2);  // initializing Grid
    int numCellCols = floor(cols / c);
    int numCellRows = floor(rows / c);
    std::vector<std::vector<bool> > coveredVec(
        numCellRows + 1, std::vector<bool>(numCellCols + 1, false));

    for (unsigned int i = 0; i < keyPoints.size(); ++i) {
      int row =
          floor(keyPoints[i].pt.y /
                c);  // get position of the cell current point is located at
      int col = floor(keyPoints[i].pt.x / c);
      if (coveredVec[row][col] == false) {  // if the cell is not covered
        result.push_back(i);
        int rowMin = ((row - floor(radius / c)) >= 0)
                         ? (row - floor(radius / c))
                         : 0;  // get range which current radius is covering
        int rowMax = ((row + floor(radius / c)) <= numCellRows)
                         ? (row + floor(radius / c))
                         : numCellRows;
        int colMin =
            ((col - floor(radius / c)) >= 0) ? (col - floor(radius / c)) : 0;
        int colMax = ((col + floor(radius / c)) <= numCellCols)
                         ? (col + floor(radius / c))
                         : numCellCols;
        for (int rowToCov = rowMin; rowToCov <= rowMax; ++rowToCov) {
          for (int colToCov = colMin; colToCov <= colMax; ++colToCov) {
            double dist = sqrt((rowToCov - row) * (rowToCov - row) +
                               (colToCov - col) * (colToCov - col));
            if (dist <= ((double)radius) / c)
              coveredVec[rowToCov][colToCov] =
                  true;  // check the distance to every cell
          }
        }
      }
    }
    if (result.size() >= Kmin && result.size() <= Kmax) {  // solution found
      ResultVec = result;
      complete = true;
    } else if (result.size() < Kmin)
      high = radius - 1;  // update binary search range
    else
      low = radius + 1;
  }
  // retrieve final keypoints
  std::vector<cv::KeyPoint> kp;
  for (unsigned int i = 0; i < ResultVec.size(); i++)
    kp.push_back(keyPoints[ResultVec[i]]);

  return kp;
}

std::vector<cv::KeyPoint> KdTree(const std::vector<cv::KeyPoint>& keyPoints,
                                 int numRetPoints,
                                 float tolerance,
                                 int cols,
                                 int rows) {
  // several temp expression variables to simplify solution equation
  int exp1 = rows + cols + 2 * numRetPoints;
  long long exp2 =
      ((long long)4 * cols + (long long)4 * numRetPoints +
       (long long)4 * rows * numRetPoints + (long long)rows * rows +
       (long long)cols * cols - (long long)2 * rows * cols +
       (long long)4 * rows * cols * numRetPoints);
  double exp3 = sqrt(exp2);
  double exp4 = numRetPoints - 1;

  double sol1 = -round((exp1 + exp3) / exp4);  // first solution
  double sol2 = -round((exp1 - exp3) / exp4);  // second solution

  int high =
      (sol1 > sol2)
          ? sol1
          : sol2;  // binary search range initialization with positive solution
  int low = floor(sqrt((double)keyPoints.size() / numRetPoints));

  PointCloud<int> cloud;  // creating k-d tree with keypoints
  generatePointCloud(cloud, keyPoints);
  typedef nanoflann::KDTreeSingleIndexAdaptor<
      nanoflann::L2_Simple_Adaptor<int, PointCloud<int> >,
      PointCloud<int>,
      2>
      my_kd_tree_t;
  my_kd_tree_t index(
      2, cloud, nanoflann::KDTreeSingleIndexAdaptorParams(25 /* max leaf */));
  index.buildIndex();

  bool complete = false;
  unsigned int K = numRetPoints;
  unsigned int Kmin = round(K - (K * tolerance));
  unsigned int Kmax = round(K + (K * tolerance));
  std::vector<int> ResultVec;
  int radius;
  int prevradius = -1;

  std::vector<int> result;
  result.reserve(keyPoints.size());
  while (!complete) {
    std::vector<bool> Included(keyPoints.size(), true);
    radius = low + (high - low) / 2;
    if (radius == prevradius ||
        low >
            high) {  // needed to reassure the same radius is not repeated again
      ResultVec = result;  // return the keypoints from the previous iteration
      break;
    }
    result.clear();

    for (unsigned int i = 0; i < keyPoints.size(); ++i) {
      if (Included[i] == true) {
        Included[i] = false;
        result.push_back(i);
        const int search_radius = static_cast<int>(radius * radius);
        std::vector<pair<size_t, int> > ret_matches;
        nanoflann::SearchParams params;
        const int query_pt[2] = {(int)keyPoints[i].pt.x,
                                 (int)keyPoints[i].pt.y};
        const size_t nMatches = index.radiusSearch(
            &query_pt[0], search_radius, ret_matches, params);

        for (size_t nmIdx = 0; nmIdx < nMatches; nmIdx++) {
          if (Included[ret_matches[nmIdx].first])
            Included[ret_matches[nmIdx].first] = false;
        }
      }
    }

    if (result.size() >= Kmin && result.size() <= Kmax) {  // solution found
      ResultVec = result;
      complete = true;
    } else if (result.size() < Kmin)
      high = radius - 1;  // update binary search range
    else
      low = radius + 1;

    prevradius = radius;
  }

  // retrieve final keypoints
  std::vector<cv::KeyPoint> kp;
  for (unsigned int i = 0; i < ResultVec.size(); i++)
    kp.push_back(keyPoints[ResultVec[i]]);

  return kp;
}

std::vector<cv::KeyPoint> RangeTree(const std::vector<cv::KeyPoint>& keyPoints,
                                    int numRetPoints,
                                    float tolerance,
                                    int cols,
                                    int rows) {
  // several temp expression variables to simplify solution equation
  int exp1 = rows + cols + 2 * numRetPoints;
  long long exp2 =
      ((long long)4 * cols + (long long)4 * numRetPoints +
       (long long)4 * rows * numRetPoints + (long long)rows * rows +
       (long long)cols * cols - (long long)2 * rows * cols +
       (long long)4 * rows * cols * numRetPoints);
  double exp3 = sqrt(exp2);
  double exp4 = numRetPoints - 1;

  double sol1 = -round((exp1 + exp3) / exp4);  // first solution
  double sol2 = -round((exp1 - exp3) / exp4);  // second solution

  int high =
      (sol1 > sol2)
          ? sol1
          : sol2;  // binary search range initialization with positive solution
  int low = floor(sqrt((double)keyPoints.size() / numRetPoints));

  rangetree<u16, u16> treeANMS(
      keyPoints.size(),
      keyPoints.size());  // creating range tree with keypoints
  for (unsigned int i = 0; i < keyPoints.size(); i++)
    treeANMS.add(keyPoints[i].pt.x, keyPoints[i].pt.y, (u16*)(intptr_t)i);
  treeANMS.finalize();

  bool complete = false;
  unsigned int K = numRetPoints;
  unsigned int Kmin = round(K - (K * tolerance));
  unsigned int Kmax = round(K + (K * tolerance));
  std::vector<int> ResultVec;
  int width;
  int prevwidth = -1;

  std::vector<int> result;
  result.reserve(keyPoints.size());
  while (!complete) {
    std::vector<bool> Included(keyPoints.size(), true);
    width = low + (high - low) / 2;
    if (width == prevwidth ||
        low >
            high) {  // needed to reassure the same width is not repeated again
      ResultVec = result;  // return the keypoints from the previous iteration
      break;
    }
    result.clear();

    for (unsigned int i = 0; i < keyPoints.size(); ++i) {
      if (Included[i] == true) {
        Included[i] = false;
        result.push_back(i);
        int minx = keyPoints[i].pt.x - width;
        int maxx = keyPoints[i].pt.x +
                   width;  // defining square boundaries around the point
        int miny = keyPoints[i].pt.y - width;
        int maxy = keyPoints[i].pt.y + width;
        if (minx < 0) minx = 0;
        if (miny < 0) miny = 0;

        std::vector<u16*>* he = treeANMS.search(minx, maxx, miny, maxy);
        for (unsigned int j = 0; j < he->size(); j++)
          if (Included[(u64)(*he)[j]]) Included[(u64)(*he)[j]] = false;
        delete he;
        he = NULL;
      }
    }
    if (result.size() >= Kmin && result.size() <= Kmax) {  // solution found
      ResultVec = result;
      complete = true;
    } else if (result.size() < Kmin)
      high = width - 1;  // update binary search range
    else
      low = width + 1;
    prevwidth = width;
  }
  // retrieve final keypoints
  std::vector<cv::KeyPoint> kp;
  for (unsigned int i = 0; i < ResultVec.size(); i++)
    kp.push_back(keyPoints[ResultVec[i]]);

  return kp;
}

std::vector<cv::KeyPoint> Ssc(const std::vector<cv::KeyPoint>& keyPoints,
                              int numRetPoints,
                              float tolerance,
                              int cols,
                              int rows) {
  // several temp expression variables to simplify solution equation
  int exp1 = rows + cols + 2 * numRetPoints;
  long long exp2 =
      ((long long)4 * cols + (long long)4 * numRetPoints +
       (long long)4 * rows * numRetPoints + (long long)rows * rows +
       (long long)cols * cols - (long long)2 * rows * cols +
       (long long)4 * rows * cols * numRetPoints);
  double exp3 = sqrt(exp2);
  double exp4 = numRetPoints - 1;

  double sol1 = -round((exp1 + exp3) / exp4);  // first solution
  double sol2 = -round((exp1 - exp3) / exp4);  // second solution

  int high =
      (sol1 > sol2)
          ? sol1
          : sol2;  // binary search range initialization with positive solution
  int low = floor(sqrt((double)keyPoints.size() / numRetPoints));

  int width;
  int prevWidth = -1;

  std::vector<int> ResultVec;
  bool complete = false;
  unsigned int K = numRetPoints;
  unsigned int Kmin = round(K - (K * tolerance));
  unsigned int Kmax = round(K + (K * tolerance));

  std::vector<int> result;
  result.reserve(keyPoints.size());
  while (!complete) {
    width = low + (high - low) / 2;
    if (width == prevWidth ||
        low >
            high) {  // needed to reassure the same radius is not repeated again
      ResultVec = result;  // return the keypoints from the previous iteration
      break;
    }
    result.clear();
    double c = width / 2;  // initializing Grid
    int numCellCols = floor(cols / c);
    int numCellRows = floor(rows / c);
    std::vector<std::vector<bool> > coveredVec(
        numCellRows + 1, std::vector<bool>(numCellCols + 1, false));

    for (unsigned int i = 0; i < keyPoints.size(); ++i) {
      int row =
          floor(keyPoints[i].pt.y /
                c);  // get position of the cell current point is located at
      int col = floor(keyPoints[i].pt.x / c);
      if (coveredVec[row][col] == false) {  // if the cell is not covered
        result.push_back(i);
        int rowMin = ((row - floor(width / c)) >= 0)
                         ? (row - floor(width / c))
                         : 0;  // get range which current radius is covering
        int rowMax = ((row + floor(width / c)) <= numCellRows)
                         ? (row + floor(width / c))
                         : numCellRows;
        int colMin =
            ((col - floor(width / c)) >= 0) ? (col - floor(width / c)) : 0;
        int colMax = ((col + floor(width / c)) <= numCellCols)
                         ? (col + floor(width / c))
                         : numCellCols;
        for (int rowToCov = rowMin; rowToCov <= rowMax; ++rowToCov) {
          for (int colToCov = colMin; colToCov <= colMax; ++colToCov) {
            if (!coveredVec[rowToCov][colToCov])
              coveredVec[rowToCov][colToCov] =
                  true;  // cover cells within the square bounding box with
                         // width w
          }
        }
      }
    }

    if (result.size() >= Kmin && result.size() <= Kmax) {  // solution found
      ResultVec = result;
      complete = true;
    } else if (result.size() < Kmin)
      high = width - 1;  // update binary search range
    else
      low = width + 1;
    prevWidth = width;
  }
  // retrieve final keypoints
  std::vector<cv::KeyPoint> kp;
  for (unsigned int i = 0; i < ResultVec.size(); i++)
    kp.push_back(keyPoints[ResultVec[i]]);

  return kp;
}

void VisualizeAll(cv::Mat Image,
                  std::vector<cv::KeyPoint> keyPoints,
                  string figureTitle) {
  cv::Mat resultImg;
  cv::drawKeypoints(
      Image, keyPoints, resultImg, cv::Scalar(94.0, 206.0, 165.0, 0.0));
  cv::namedWindow(figureTitle, cv::WINDOW_AUTOSIZE);
  cv::imshow(figureTitle, resultImg);
  return;
}

}  // namespace anms
````

## File: frontend/anms/NonMaximumSupression.cc
````
/* ----------------------------------------------------------------------------
 * Copyright 2017, Massachusetts Institute of Technology,
 * Cambridge, MA 02139
 * All Rights Reserved
 * Authors: Luca Carlone, et al. (see THANKS for the full author list)
 * See LICENSE for the license information
 * -------------------------------------------------------------------------- */

/**
 * @file   NonMaximumSuppression.cpp
 * @brief  Base class for non maximum suppresion interface
 * @author Antoni Rosinol
 * @author Luca Carlone
 */

#include "dynosam/frontend/anms/NonMaximumSuppression.h"

#include <glog/logging.h>

#include <numeric>
#include <opencv2/opencv.hpp>
#include <vector>

#include <Eigen/Dense>

#include "dynosam/frontend/anms/anms/anms.h"
#include "dynosam/utils/TimingStats.hpp"

namespace dyno {

AdaptiveNonMaximumSuppression::AdaptiveNonMaximumSuppression(
    const AnmsAlgorithmType& anms_algorithm_type)
    : NonMaximumSuppression(), anms_algorithm_type_(anms_algorithm_type){};

std::vector<cv::KeyPoint> AdaptiveNonMaximumSuppression::suppressNonMax(
    const std::vector<cv::KeyPoint>& keyPoints,
    const int& numRetPoints,
    const float& tolerance,
    const int& cols,
    const int& rows,
    const int& nr_horizontal_bins,
    const int& nr_vertical_bins,
    const Eigen::MatrixXd& binning_mask) {

  utils::TimingStatsCollector timer("anms_timer");

  if (keyPoints.size() == 0) {
    std::cout << "No keypoints for non-max suppression..." <<std::endl;
    return std::vector<cv::KeyPoint>();
  }

  // Sorting keypoints by deacreasing order of strength
//   VLOG(5) << "Sorting keypoints in decreasing order of strength.";
  std::vector<int> responseVector;
  for (unsigned int i = 0; i < keyPoints.size(); i++) {
    responseVector.push_back(keyPoints[i].response);
  }
  std::vector<int> Indx(responseVector.size());
  std::iota(std::begin(Indx), std::end(Indx), 0);
  cv::sortIdx(responseVector, Indx, cv::SortFlags::SORT_DESCENDING);
  std::vector<cv::KeyPoint> keyPointsSorted;
  for (unsigned int i = 0; i < keyPoints.size(); i++) {
    keyPointsSorted.push_back(keyPoints[Indx[i]]);
  }

  std::vector<cv::KeyPoint> keypoints;
//   VLOG(5) << "Starting Adaptive Non-Maximum Suppression.";
  switch (anms_algorithm_type_) {
    case AnmsAlgorithmType::TopN: {
    //   VLOG(1) << "Running TopN: " << VIO::to_underlying(anms_algorithm_type_);
      keypoints = anms::TopN(keyPoints, numRetPoints);
      break;
    };
    case AnmsAlgorithmType::BrownANMS: {
    //   VLOG(1) << "Running BrownANMS: "
            //   << VIO::to_underlying(anms_algorithm_type_);
      keypoints = anms::BrownANMS(keyPoints, numRetPoints);
      break;
    };
    case AnmsAlgorithmType::SDC: {
    //   VLOG(1) << "Running SDC: " << VIO::to_underlying(anms_algorithm_type_);
      keypoints =
          anms::Sdc(keyPointsSorted, numRetPoints, tolerance, cols, rows);
      break;
    };
    case AnmsAlgorithmType::KdTree: {
    //   VLOG(1) << "Running KdTree: " << VIO::to_underlying(anms_algorithm_type_);
      keypoints =
          anms::KdTree(keyPointsSorted, numRetPoints, tolerance, cols, rows);
      break;
    };
    case AnmsAlgorithmType::RangeTree: {
        std::cout << "Running RangeTree " << keyPointsSorted.size() << std::endl;
        std::cout << numRetPoints << std::endl;
    //   VLOG(1) << "Running RangeTree: "
            //   << VIO::to_underlying(anms_algorithm_type_);
      keypoints =
          anms::RangeTree(keyPointsSorted, numRetPoints, tolerance, cols, rows);
      break;
    };
    case AnmsAlgorithmType::Ssc: {
    //   VLOG(1) << "Running SSC: " << VIO::to_underlying(anms_algorithm_type_);
      keypoints =
          anms::Ssc(keyPointsSorted, numRetPoints, tolerance, cols, rows);
      break;
    };
    case AnmsAlgorithmType::Binning: {
    //   VLOG(1) << "Running Binning: "
            //   << VIO::to_underlying(anms_algorithm_type_);
      keypoints = binning(keyPointsSorted,
                          numRetPoints,
                          cols,
                          rows,
                          nr_horizontal_bins,
                          nr_vertical_bins,
                          binning_mask);
      break;
    };
    default: {
    //   VLOG(1) << "Unknown ANMS algorithm requested: "
            //   << VIO::to_underlying(anms_algorithm_type_);
      break;
    };
  }
//   VLOG(1) << "Non Maximum Suppression Timing [ms]: "
//           << utils::Timer::toc(tic).count();
  return keypoints;
}

// ---------------------------------------------------------------------------------
std::vector<cv::KeyPoint> AdaptiveNonMaximumSuppression::binning(
    const std::vector<cv::KeyPoint>& keyPoints,
    const int& numKptsToRetain,
    const int& imgCols,
    const int& imgRows,
    const int& nr_horizontal_bins,
    const int& nr_vertical_bins,
    const Eigen::MatrixXd& binning_mask) {
  if (static_cast<size_t>(numKptsToRetain) > keyPoints.size()) {
    return keyPoints;
  }

  float binRowSize = float(imgRows) / float(nr_vertical_bins);
  float binColSize = float(imgCols) / float(nr_horizontal_bins);

  // Note: features should be already sorted by score at this point from detect

  // 0. count the number of valid bins (as specified by the user in the yaml
  float nrActiveBins = binning_mask.sum();  // sum of 1's in binary mask

  // 1. compute how many features we want to retain in each bin
  // numRetPointsPerBin
  const int numRetPointsPerBin =
      std::round(float(numKptsToRetain) / float(nrActiveBins));

  // 2. assign keypoints to bins and retain top numRetPointsPerBin for each bin
  std::vector<cv::KeyPoint> binnedKpts;  // binned keypoints we want to output
  Eigen::MatrixXd nrKptsInBin = Eigen::MatrixXd::Zero(
      nr_vertical_bins,
      nr_horizontal_bins);  // store number of kpts for each bin
  for (size_t i = 0; i < keyPoints.size(); i++) {
    const size_t binRowInd =
        static_cast<size_t>(keyPoints[i].pt.y / binRowSize);
    const size_t binColInd =
        static_cast<size_t>(keyPoints[i].pt.x / binColSize);
    // if bin is active and needs more keypoints
    if (binning_mask(binRowInd, binColInd) == 1 &&
        nrKptsInBin(binRowInd, binColInd) <
            numRetPointsPerBin) {  // if we need more kpts in that bin
      binnedKpts.push_back(keyPoints[i]);
      nrKptsInBin(binRowInd, binColInd) += 1;
    }
  }
  return binnedKpts;
}

}  // namespace dyno
````

## File: frontend/imu/ThreadSafeImuBuffer.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/frontend/imu/ThreadSafeImuBuffer.hpp"

/*******************************************************************************
 Copyright 2017 Autonomous Systems Lab, ETH Zurich, Switzerland

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
*******************************************************************************/

/* ----------------------------------------------------------------------------
 * Copyright 2017, Massachusetts Institute of Technology,
 * Cambridge, MA 02139
 * All Rights Reserved
 * Authors: Luca Carlone, et al. (see THANKS for the full author list)
 * See LICENSE for the license information
 * -------------------------------------------------------------------------- */

/**
 * @file   ThreadsafeImuBuffer.cpp
 * @brief  Threadsafe Imu Buffer with timestamp lookup.
 * @author Antoni Rosinol
 */

#include <chrono>
#include <iostream>

#include <algorithm>

#include <gflags/gflags.h>
#include <glog/logging.h>

#include "dynosam/frontend/imu/ThreadSafeImuBuffer.hpp"
#include "dynosam/utils/Timing.hpp"

namespace dyno {


template <template <typename, typename> class Container, typename Type>
using Aligned = Container<Type, Eigen::aligned_allocator<Type>>;

ThreadsafeImuBuffer::QueryResult ThreadsafeImuBuffer::isDataAvailableUpToImpl(
    const Timestamp& timestamp_ns_from,
    const Timestamp& timestamp_ns_to) const {
  CHECK_LT(timestamp_ns_from, timestamp_ns_to);

  if (shutdown_) {
    return QueryResult::kQueueShutdown;
  }

  if (buffer_.empty()) {
    return QueryResult::kDataNotYetAvailable;
  }

  ImuMeasurement value;
  if (buffer_.getNewestValue(&value) && value.timestamp_ < timestamp_ns_to) {
    // This is triggered if the timestamp_ns_to requested exceeds the newest
    // IMU measurement, meaning that there is data still to arrive to reach the
    // requested point in time.
    return QueryResult::kDataNotYetAvailable;
  }

  if (buffer_.getOldestValue(&value) && timestamp_ns_from < value.timestamp_) {
    // This is triggered if the user requests data previous to the oldest IMU
    // measurement present in the buffer, meaning that there is missing data
    // from the timestamp_ns_from requested to the oldest stored timestamp.
    return QueryResult::kDataNeverAvailable;
  }
  return QueryResult::kDataAvailable;
}

void ThreadsafeImuBuffer::linearInterpolate(const Timestamp& t0,
                                            const ImuAccGyr& y0,
                                            const Timestamp& t1,
                                            const ImuAccGyr& y1,
                                            const Timestamp& t,
                                            ImuAccGyr* y) {
  CHECK_NOTNULL(y);
  CHECK_LE(t0, t);
  CHECK_LE(t, t1);
  *y = t0 == t1 ? y0 :  // y0 if t0 == t1, interpolate otherwise:
           y0 + (y1 - y0) * static_cast<double>(t - t0) /
                    static_cast<double>(t1 - t0);
}

ThreadsafeImuBuffer::QueryResult ThreadsafeImuBuffer::getImuDataBtwTimestamps(
    const Timestamp& timestamp_ns_from,
    const Timestamp& timestamp_ns_to,
    Timestamps* imu_timestamps,
    ImuAccGyrs* imu_measurements,
    bool get_lower_bound) {
  CHECK_NOTNULL(imu_timestamps);
  CHECK_NOTNULL(imu_measurements);
  DCHECK_LT(timestamp_ns_from, timestamp_ns_to);
  QueryResult query_result =
      isDataAvailableUpToImpl(timestamp_ns_from, timestamp_ns_to);
  if (query_result != QueryResult::kDataAvailable) {
    imu_timestamps->resize(Eigen::NoChange, 0);
    imu_measurements->resize(Eigen::NoChange, 0);
    return query_result;
  }

  // TODO can we avoid copying and have the temporal buffer store pairs of
  // timestamps/accgyr data?
  // Copy the data with timestamp_up_to <= timestamps_buffer from the buffer.
  Aligned<std::vector, ImuMeasurement> between_values;
  CHECK(buffer_.getValuesBetweenTimes(timestamp_ns_from, timestamp_ns_to,
                                      &between_values, get_lower_bound));

  if (between_values.empty()) {
    LOG(WARNING) << "No IMU measurements available strictly between time "
                 << timestamp_ns_from << "[ns] and " << timestamp_ns_to
                 << "[ns].";
    imu_timestamps->resize(Eigen::NoChange, 0);
    imu_measurements->resize(Eigen::NoChange, 0);
    return QueryResult::kTooFewMeasurementsAvailable;
  }

  const size_t num_measurements = between_values.size();
  imu_timestamps->resize(Eigen::NoChange, num_measurements);
  imu_measurements->resize(Eigen::NoChange, num_measurements);

  for (size_t idx = 0u; idx < num_measurements; ++idx) {
    (*imu_timestamps)(idx) = between_values[idx].timestamp_;
    (*imu_measurements).col(idx) = between_values[idx].acc_gyr_;
  }

  return query_result;
}

ThreadsafeImuBuffer::QueryResult
ThreadsafeImuBuffer::getImuDataInterpolatedUpperBorder(
    const Timestamp& timestamp_ns_from,
    const Timestamp& timestamp_ns_to,
    Timestamps* imu_timestamps,
    ImuAccGyrs* imu_measurements) {
  CHECK_NOTNULL(imu_timestamps);
  CHECK_NOTNULL(imu_measurements);
  DCHECK_LT(timestamp_ns_from, timestamp_ns_to);
  // Get data.
  QueryResult query_result = getImuDataBtwTimestamps(
      timestamp_ns_from, timestamp_ns_to, imu_timestamps, imu_measurements,
      true);  // Get lower bound.
  // Early exit if there is no data.
  if (query_result != QueryResult::kDataAvailable) {
    imu_timestamps->resize(Eigen::NoChange, 0);
    imu_measurements->resize(Eigen::NoChange, 0);
    return query_result;
  }

  // Interpolate upper border.
  ImuAccGyr interpolated_upper_border;
  interpolateValueAtTimestamp(timestamp_ns_to, &interpolated_upper_border);

  DCHECK_EQ(imu_timestamps->rows(), 1);
  DCHECK_EQ(imu_measurements->rows(), 6);
  // The last measurement will correspond to the interpolated data.
  const size_t num_measurements = imu_timestamps->cols() + 1u;
  imu_timestamps->conservativeResize(Eigen::NoChange, num_measurements);
  imu_measurements->conservativeResize(Eigen::NoChange, num_measurements);
  // Append upper border.
  imu_timestamps->rightCols<1>()(0) = timestamp_ns_to;
  imu_measurements->rightCols<1>() = interpolated_upper_border;

  return query_result;
}

ThreadsafeImuBuffer::QueryResult
ThreadsafeImuBuffer::getImuDataInterpolatedBorders(
    const Timestamp& timestamp_ns_from,
    const Timestamp& timestamp_ns_to,
    Timestamps* imu_timestamps,
    ImuAccGyrs* imu_measurements) {
  CHECK_NOTNULL(imu_timestamps);
  CHECK_NOTNULL(imu_measurements);
  // Get data.
  Timestamps imu_timestamps_tmp;
  ImuAccGyrs imu_measurements_tmp;
  QueryResult query_result =
      getImuDataBtwTimestamps(timestamp_ns_from, timestamp_ns_to,
                              &imu_timestamps_tmp, &imu_measurements_tmp);

  // Early exit if there is no data.
  if (query_result != QueryResult::kDataAvailable) {
    imu_timestamps->resize(Eigen::NoChange, 0);
    imu_measurements->resize(Eigen::NoChange, 0);
    return query_result;
  }

  // Interpolate lower border.
  ImuAccGyr interpolated_lower_border;
  interpolateValueAtTimestamp(timestamp_ns_from, &interpolated_lower_border);
  // Interpolate upper border.
  ImuAccGyr interpolated_upper_border;
  interpolateValueAtTimestamp(timestamp_ns_to, &interpolated_upper_border);

  DCHECK_EQ(imu_timestamps->rows(), 1);
  DCHECK_EQ(imu_measurements->rows(), 6);
  // The first and last measurements will correspond to the interpolated data.
  const size_t num_measurements = imu_timestamps_tmp.cols() + 2u;
  imu_timestamps->resize(Eigen::NoChange, num_measurements);
  imu_measurements->resize(Eigen::NoChange, num_measurements);
  // Prepend lower border.
  imu_timestamps->leftCols<1>()(0) = timestamp_ns_from;
  imu_measurements->leftCols<1>() = interpolated_lower_border;
  // Add measurements.
  imu_timestamps->middleCols(1, imu_timestamps_tmp.cols()) = imu_timestamps_tmp;
  imu_measurements->middleCols(1, imu_measurements_tmp.cols()) =
      imu_measurements_tmp;
  // Append upper border.
  imu_timestamps->rightCols<1>()(0) = timestamp_ns_to;
  imu_measurements->rightCols<1>() = interpolated_upper_border;

  return query_result;
}

void ThreadsafeImuBuffer::interpolateValueAtTimestamp(
    const Timestamp& timestamp_ns,
    ImuAccGyr* interpolated_imu_measurement) {
  CHECK_NOTNULL(interpolated_imu_measurement);
  Timestamp pre_border_timestamp, post_border_timestamp;
  ImuMeasurement pre_border_value, post_border_value;
  CHECK(buffer_.getValueAtOrBeforeTime(timestamp_ns, &pre_border_timestamp,
                                       &pre_border_value))
      << "The IMU buffer seems not to contain measurements at or before time: "
      << timestamp_ns;
  CHECK_EQ(pre_border_timestamp, pre_border_value.timestamp_);
  CHECK(buffer_.getValueAtOrAfterTime(timestamp_ns, &post_border_timestamp,
                                      &post_border_value))
      << "The IMU buffer seems not to contain measurements at or after time: "
      << timestamp_ns;
  CHECK_EQ(post_border_timestamp, post_border_value.timestamp_);
  linearInterpolate(pre_border_value.timestamp_,
                    pre_border_value.acc_gyr_,
                    post_border_value.timestamp_,
                    post_border_value.acc_gyr_,
                    timestamp_ns,
                    interpolated_imu_measurement);
}

ThreadsafeImuBuffer::QueryResult
ThreadsafeImuBuffer::getImuDataInterpolatedBordersBlocking(
    const Timestamp& timestamp_ns_from,
    const Timestamp& timestamp_ns_to,
    long int wait_timeout_nanoseconds,
    Timestamps* imu_timestamps,
    ImuAccGyrs* imu_measurements) {
  CHECK_NOTNULL(imu_timestamps);
  CHECK_NOTNULL(imu_measurements);

  // Wait for the IMU buffer to contain the required measurements within a
  // timeout.
  auto tic = utils::Timer::tic();
  QueryResult query_result;
  {
    std::unique_lock<std::mutex> lock(m_buffer_);
    while ((query_result =
                isDataAvailableUpToImpl(timestamp_ns_from, timestamp_ns_to)) !=
           QueryResult::kDataAvailable) {
      cv_new_measurement_.wait_for(
          lock, std::chrono::nanoseconds(wait_timeout_nanoseconds));

      if (shutdown_) {
        imu_timestamps->resize(Eigen::NoChange, 0);
        imu_measurements->resize(Eigen::NoChange, 0);
        return QueryResult::kQueueShutdown;
      }

      // Check if we hit the max. time allowed to wait for the required data.
      auto toc = utils::Timer::toc<std::chrono::nanoseconds>(tic);
      if (toc.count() >= wait_timeout_nanoseconds) {
        imu_timestamps->resize(Eigen::NoChange, 0);
        imu_measurements->resize(Eigen::NoChange, 0);
        LOG(WARNING) << "Timeout reached while trying to get the requested "
                     << "IMU data. Requested range: " << timestamp_ns_from
                     << " to " << timestamp_ns_to << ".";
        if (query_result == QueryResult::kDataNotYetAvailable) {
          LOG(WARNING) << "The relevant IMU data is not yet available.";
        } else if (query_result == QueryResult::kDataNeverAvailable) {
          LOG(WARNING) << "The relevant IMU data will never be available. "
                       << "Either the buffer is too small or a sync issue "
                       << "occurred.";
        } else {
          LOG(FATAL) << "Unknown query result error.";
        }
        return query_result;
      }
    }
  }
  return getImuDataInterpolatedBorders(timestamp_ns_from, timestamp_ns_to,
                                       imu_timestamps, imu_measurements);
}

}  // namespace dyno
````

## File: frontend/vision/Feature.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/frontend/vision/Feature.hpp"

#include <glog/logging.h>

namespace dyno {

FeatureContainer::FeatureContainer() : feature_map_() {}

FeatureContainer::FeatureContainer(const FeaturePtrs feature_vector) {
    for(size_t i = 0; i < feature_vector.size(); i++) {
        add(feature_vector.at(i));
    }
}

void FeatureContainer::add(const Feature& feature) {
    auto feature_ptr = std::make_shared<Feature>(feature);
    add(feature_ptr);
}

void FeatureContainer::add(Feature::Ptr feature) {
    CHECK(!exists(feature->trackletId())) << "Feailure in FeatureContainer::add - Tracklet Id " << feature->trackletId() << " already exists";
    feature_map_[feature->trackletId()] = feature;
}

 //TODO: test
 //this will mess up any iterator that currently has a reference to any feature_map_ (so any of the filters)
void FeatureContainer::remove(TrackletId tracklet_id) {
    if(!exists(tracklet_id)) {
        throw std::runtime_error("Cannot remove feature with tracklet id " + std::to_string(tracklet_id) + " as feature does not exist!");
    }

    feature_map_.erase(tracklet_id);
}

void FeatureContainer::removeByObjectId(ObjectId object_id) {
    //collect all tracklets
    auto itr = FilterIterator(*this, [object_id](const Feature::Ptr& f) -> bool {
        return f->objectId() == object_id;
    });

    TrackletIds tracklets_to_remove;
    for (const auto& feature : itr) {
        CHECK_EQ(feature->objectId(), object_id);
        tracklets_to_remove.push_back(feature->trackletId());
    }

    for(const auto tracklet_id : tracklets_to_remove) {
        this->remove(tracklet_id);
    }

}

void FeatureContainer::clear() {
    feature_map_.clear();
}

TrackletIds FeatureContainer::collectTracklets(bool only_usable) const {
    TrackletIds tracklets;
    for(const auto& feature : *this) {
        if(only_usable && feature->usable()) {
            tracklets.push_back(feature->trackletId());
        }
        else {
            tracklets.push_back(feature->trackletId());
        }
    }

    return tracklets;
}


 void FeatureContainer::markOutliers(const TrackletIds& outliers) {
    for(TrackletId tracklet_id : outliers) {
        CHECK(exists(tracklet_id));

        getByTrackletId(tracklet_id)->markOutlier();
    }
 }

size_t FeatureContainer::size() const {
    return feature_map_.size();
}

Feature::Ptr FeatureContainer::getByTrackletId(TrackletId tracklet_id) const {
    if(!exists(tracklet_id)) {
        return nullptr;
    }
    return feature_map_.at(tracklet_id);
}

bool FeatureContainer::exists(TrackletId tracklet_id) const {
    return feature_map_.find(tracklet_id) != feature_map_.end();
}


FeatureContainer::FilterIterator FeatureContainer::beginUsable() {
    return FilterIterator(*this, [](const Feature::Ptr& f) -> bool {
        return Feature::IsUsable(f);
    });
}

FeatureContainer::FilterIterator FeatureContainer::beginUsable() const {
    FeatureContainer& t = const_cast<FeatureContainer&>(*this);
    return FilterIterator(t, [](const Feature::Ptr& f) -> bool {
        return Feature::IsUsable(f);
    });
}

std::vector<cv::Point2f> FeatureContainer::toOpenCV(TrackletIds* tracklet_ids) const {
    if(tracklet_ids) tracklet_ids->clear();

    std::vector<cv::Point2f> keypoints_cv;
    for(const auto& feature : *this) {
        const Keypoint& kp = feature->keypoint();

        float x = static_cast<float>(kp(0));
        float y = static_cast<float>(kp(1));

        keypoints_cv.push_back(cv::Point2f(x, y));

        if(tracklet_ids) tracklet_ids->push_back(feature->trackletId());
    }
    return keypoints_cv;
}


} //dyno
````

## File: frontend/vision/FeatureDetector.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/frontend/vision/FeatureDetector.hpp"
#include "dynosam/frontend/vision/ORBextractor.hpp"

#include <opencv2/imgproc.hpp>
#include <opencv2/features2d.hpp>
#include <opencv2/video/tracking.hpp>
#include <opencv2/opencv.hpp>

#include <glog/logging.h>

namespace dyno {

template<>
FunctionalDetector::Ptr FunctionalDetector::Create<cv::GFTTDetector>(const TrackerParams& tracker_params) {
    LOG(INFO) << "Creating cv::GFTTDetector";

    auto feature_detector_ = cv::GFTTDetector::create(
          tracker_params.max_nr_keypoints_before_anms,
          tracker_params.gfft_params.quality_level,
          tracker_params.min_distance_btw_tracked_and_detected_features,
          tracker_params.gfft_params.block_size,
          tracker_params.gfft_params.use_harris_corner_detector,
          tracker_params.gfft_params.k);
    auto functional_detector = [=](const cv::Mat& img, KeypointsCV& keypoints, const cv::Mat& mask) -> void {
        CHECK_NOTNULL(feature_detector_)->detect(img, keypoints, mask);
    };

    return std::make_shared<FunctionalDetector>(functional_detector);
}

template<>
FunctionalDetector::Ptr FunctionalDetector::Create<ORBextractor>(const TrackerParams& tracker_params) {
    LOG(INFO) << "Creating dyno::ORBextractor";

    auto orb_detector_ = std::make_shared<ORBextractor>(
        tracker_params.max_nr_keypoints_before_anms,
        tracker_params.orb_params.scale_factor,
        tracker_params.orb_params.n_levels,
        tracker_params.orb_params.init_threshold_fast,
        tracker_params.orb_params.min_threshold_fast);

    //NOTE that the mask is not used in this implementation
    auto functional_detector = [=](const cv::Mat& img, KeypointsCV& keypoints, const cv::Mat&) -> void {
        CHECK_NOTNULL(orb_detector_);
        //mask and descriptors are empty
        orb_detector_->operator()(img, cv::Mat(), keypoints, cv::Mat());
    };

    return std::make_shared<FunctionalDetector>(functional_detector);
}


FunctionalDetector::Ptr FunctionalDetector::FactoryCreate(const TrackerParams& tracker_params) {
    using FDT = TrackerParams::FeatureDetectorType;
    switch (tracker_params.feature_detector_type)
    {
    case FDT::GFTT:
        return FunctionalDetector::Create<cv::GFTTDetector>(tracker_params);
    case FDT::ORB_SLAM_ORB:
        return FunctionalDetector::Create<ORBextractor>(tracker_params);
    default:
        LOG(ERROR) << "Unknown Feature detection type!";
        return nullptr;
        break;
    }
}


SparseFeatureDetector::SparseFeatureDetector(const TrackerParams& tracker_params, const FeatureDetector::Ptr& feature_detector)
    : tracker_params_(tracker_params),
      feature_detector_(CHECK_NOTNULL(feature_detector)),
      clahe_(nullptr),
      non_maximum_supression_(nullptr)
{
    if(tracker_params_.use_clahe_filter)
        clahe_ = cv::createCLAHE(2.0, cv::Size(8, 8)); //TODO: make params

    if(tracker_params.use_anms)
        non_maximum_supression_ = std::make_unique<AdaptiveNonMaximumSuppression>(tracker_params.anms_params.non_max_suppression_type);
}

void SparseFeatureDetector::detect(const cv::Mat& image, KeypointsCV& keypoints, int number_tracked, const cv::Mat& detection_mask) {
    cv::Mat processed_image = image.clone();

    //pre-process image if required
    if(clahe_) clahe_->apply(processed_image, processed_image);

    std::vector<cv::KeyPoint> raw_keypoints;
    feature_detector_->detect(processed_image, raw_keypoints, detection_mask);

    std::vector<cv::KeyPoint>& max_keypoints = raw_keypoints;
    if(tracker_params_.use_anms) {

        //calculate number of corners needed
        int nr_corners_needed = std::max(
            tracker_params_.max_features_per_frame - number_tracked, 0);

        static constexpr float tolerance = 0.1;

        const auto& anms_params = tracker_params_.anms_params;
        Eigen::MatrixXd binning_mask = anms_params.binning_mask;

        max_keypoints = non_maximum_supression_->suppressNonMax(
            raw_keypoints,
            nr_corners_needed,
            tolerance,
            processed_image.cols,
            processed_image.rows,
            anms_params.nr_horizontal_bins,
            anms_params.nr_vertical_bins,
            binning_mask);

    }

    if(tracker_params_.use_subpixel_corner_refinement && max_keypoints.size() > 0u) {
        // Convert keypoints to points
        std::vector<cv::Point2f> points;
        cv::KeyPoint::convert(max_keypoints, points);

        const auto& subpixel_corner_refinement_params = tracker_params_.subpixel_corner_refinement_params;
        cv::cornerSubPix(
                processed_image,
                points,
                subpixel_corner_refinement_params.window_size,
                subpixel_corner_refinement_params.zero_zone,
                subpixel_corner_refinement_params.criteria);

        cv::KeyPoint::convert(points, max_keypoints);
    }

    keypoints = max_keypoints;
}




} //namespace dyno
````

## File: frontend/vision/FeatureTracker.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include "dynosam/frontend/vision/FeatureTracker.hpp"

#include <glog/logging.h>

#include <opencv4/opencv2/opencv.hpp>

#include "dynosam/common/Types.hpp"
#include "dynosam/frontend/vision/VisionTools.hpp"
#include "dynosam/utils/GtsamUtils.hpp"
#include "dynosam/utils/OpenCVUtils.hpp"
#include "dynosam/utils/TimingStats.hpp"

namespace dyno {

FeatureTracker::FeatureTracker(const FrontendParams& params, Camera::Ptr camera,
                               ImageDisplayQueue* display_queue)
    : FeatureTrackerBase(params.tracker_params, camera, display_queue),
      frontend_params_(params) {
  static_feature_tracker_ = std::make_unique<KltFeatureTracker>(
      params.tracker_params, camera, display_queue);
  CHECK(!img_size_.empty());
}

Frame::Ptr FeatureTracker::track(FrameId frame_id, Timestamp timestamp,
                                 const ImageContainer& image_container) {
  // take "copy" of tracking_images which is then given to the frame
  // this will mean that the tracking images (input) are not necessarily the
  // same as the ones inside the returned frame
  ImageContainer input_images = image_container;
  VLOG(5) << "Beginning feature track on frame " << frame_id;

  info_ = FeatureTrackerInfo();  // clear the info
  info_.frame_id = frame_id;
  info_.timestamp = timestamp;

  if (initial_computation_) {
    // intitial computation
    const cv::Size& other_size = input_images.get<ImageType::RGBMono>().size();
    CHECK(!previous_frame_);
    CHECK_EQ(img_size_.width, other_size.width);
    CHECK_EQ(img_size_.height, other_size.height);
    initial_computation_ = false;
  } else {
    if (params_.use_propogate_mask) {
      propogateMask(input_images);
    }
    CHECK(previous_frame_);
    CHECK_EQ(previous_frame_->frame_id_, frame_id - 1u)
        << "Incoming frame id must be consequative";
  }

  // TODO: figure out some better way of scaling this as it scales with the size
  // of the image...
  // TODO: and make parameter...
  static constexpr auto kDetectionBoaderThickness = 10;  // in pixels
  // create detection mask around the boarder of each dynamic object with some
  // thickness this prevents static and dynamic points being detected around the
  // edge of the dynamic object as there are lots of inconsistencies here the
  // detection mask is in the opencv mask form: CV_8UC1 where white pixels (255)
  // are valid and black pixels (0) should not be detected on
  cv::Mat boarder_detection_mask;
  vision_tools::computeObjectMaskBoundaryMask(
      input_images.get<ImageType::MotionMask>(), boarder_detection_mask,
      kDetectionBoaderThickness, true);

  // VLOG(5) << "Beginning static feature track on frame " << frame_id;
  FeatureContainer static_features;
  {
    utils::TimingStatsCollector static_track_timer("static_feature_track");
    static_features = static_feature_tracker_->trackStatic(
        previous_frame_, input_images, info_, boarder_detection_mask);
  }

  // VLOG(5) << "Beginning dynamic feature track on frame " << frame_id;
  FeatureContainer dynamic_features;
  {
    utils::TimingStatsCollector dynamic_track_timer("dynamic_feature_track");
    trackDynamic(frame_id, input_images, dynamic_features,
                 boarder_detection_mask);
  }

  previous_tracked_frame_ = previous_frame_;  // Update previous frame (previous
                                              // to the newly created frame)

  auto new_frame =
      std::make_shared<Frame>(frame_id, timestamp, camera_, input_images,
                              static_features, dynamic_features, info_);

  // update depth threshold information
  new_frame->setMaxBackgroundDepth(frontend_params_.max_background_depth);
  new_frame->setMaxObjectDepth(frontend_params_.max_object_depth);

  LOG(INFO) << "Tracked on frame " << frame_id
            << " t= " << std::setprecision(15) << timestamp << ", object ids "
            << container_to_string(new_frame->getObjectIds());
  previous_frame_ = new_frame;

  return new_frame;
}

void FeatureTracker::trackDynamic(FrameId frame_id,
                                  const ImageContainer& image_container,
                                  FeatureContainer& dynamic_features,
                                  const cv::Mat& detection_mask) {
  // first dectect dynamic points
  const cv::Mat& rgb = image_container.get<ImageType::RGBMono>();
  // flow is going to take us from THIS frame to the next frame (which does not
  // make sense for a realtime system)
  const cv::Mat& flow = image_container.get<ImageType::OpticalFlow>();
  const cv::Mat& motion_mask = image_container.get<ImageType::MotionMask>();

  TrackletIdManager& tracked_id_manager = TrackletIdManager::instance();

  ObjectIds instance_labels;
  dynamic_features.clear();

  const auto step_size = params_.semantic_mask_step_size;

  OccupandyGrid2D grid(
      step_size, std::ceil(static_cast<double>(img_size_.width) / step_size),
      std::ceil(static_cast<double>(img_size_.height) / step_size));

  if (previous_frame_) {
    const cv::Mat& previous_motion_mask =
        previous_frame_->image_container_.get<ImageType::MotionMask>();
    utils::TimingStatsCollector tracked_dynamic_features(
        "tracked_dynamic_features");
    for (Feature::Ptr previous_dynamic_feature :
         previous_frame_->usableDynamicFeaturesBegin()) {
      const TrackletId tracklet_id = previous_dynamic_feature->trackletId();
      const size_t age = previous_dynamic_feature->age();

      const Keypoint kp = previous_dynamic_feature->predictedKeypoint();
      const int x = functional_keypoint::u(kp);
      const int y = functional_keypoint::v(kp);
      // ObjectId predicted_label = functional_keypoint::at<ObjectId>(kp,
      // motion_mask);
      const ObjectId predicted_label = motion_mask.at<ObjectId>(y, x);

      if (!detection_mask.empty()) {
        const unsigned char valid_detection =
            detection_mask.at<unsigned char>(y, x);
        if (valid_detection == 0) {
          continue;
        }
      }

      // const Keypoint previous_kp = previous_dynamic_feature->keypoint();
      // ObjectId previous_label =
      // functional_keypoint::at<ObjectId>(previous_kp, previous_motion_mask);
      ObjectId previous_label = previous_dynamic_feature->objectId();
      CHECK_NE(previous_label, background_label);
      CHECK_GT(previous_label, 0);

      PerObjectStatus& object_tracking_info =
          info_.getObjectStatus(predicted_label);
      object_tracking_info.num_previous_track++;

      // true if predicted label not on the background
      const bool is_predicted_object_label =
          predicted_label != background_label;
      // true if predicted label the same as the previous label of the tracked
      // point
      const bool is_precited_same_as_previous =
          predicted_label == previous_label;

      // update stats
      if (!is_predicted_object_label)
        object_tracking_info.num_tracked_with_background_label++;
      if (!is_precited_same_as_previous)
        object_tracking_info.num_tracked_with_different_label++;

      // only include point if it is contained, it is not static and the
      // previous label is the same as the predicted label
      if (camera_->isKeypointContained(kp) && is_predicted_object_label &&
          is_precited_same_as_previous) {
        size_t new_age = age + 1;
        double flow_xe = static_cast<double>(flow.at<cv::Vec2f>(y, x)[0]);
        double flow_ye = static_cast<double>(flow.at<cv::Vec2f>(y, x)[1]);

        OpticalFlow flow(flow_xe, flow_ye);
        const Keypoint predicted_kp =
            Feature::CalculatePredictedKeypoint(kp, flow);

        if (!isWithinShrunkenImage(predicted_kp)) {
          object_tracking_info.num_outside_shrunken_image++;
          continue;
        }

        if (flow_xe == 0 || flow_ye == 0) {
          object_tracking_info.num_zero_flow++;
          continue;
        }

        // limit point tracking of a certain age
        TrackletId tracklet_to_use = tracklet_id;
        if (new_age > 25u) {
          tracklet_to_use = tracked_id_manager.getTrackletIdCount();
          tracked_id_manager.incrementTrackletIdCount();
          new_age = 0;
        }

        // // save correspondences
        Feature::Ptr feature = std::make_shared<Feature>();
        (*feature)
            .objectId(predicted_label)
            .frameId(frame_id)
            .keypointType(KeyPointType::DYNAMIC)
            .age(new_age)
            .trackletId(tracklet_to_use)
            .keypoint(kp)
            .measuredFlow(flow)
            .predictedKeypoint(predicted_kp);

        dynamic_features.add(feature);
        instance_labels.push_back(feature->objectId());

        object_tracking_info.num_track++;

        const size_t cell_idx = grid.getCellIndex(kp);
        grid.occupancy_[cell_idx] = true;
      }
    }
  }

  int step = params_.semantic_mask_step_size;
  for (int i = 0; i < rgb.rows - step; i = i + step) {
    for (int j = 0; j < rgb.cols - step; j = j + step) {
      if (!detection_mask.empty()) {
        const unsigned char valid_detection =
            detection_mask.at<unsigned char>(i, j);
        // //marked invalid by the detection mask
        if (valid_detection == 0) {
          continue;
        }
      }

      const ObjectId label = motion_mask.at<ObjectId>(i, j);

      if (label == background_label) {
        continue;
      }

      PerObjectStatus& object_tracking_info = info_.getObjectStatus(label);

      double flow_xe = static_cast<double>(flow.at<cv::Vec2f>(i, j)[0]);
      double flow_ye = static_cast<double>(flow.at<cv::Vec2f>(i, j)[1]);

      // TODO: close to zero?
      if (flow_xe == 0 || flow_ye == 0) {
        object_tracking_info.num_zero_flow++;
        continue;
      }

      OpticalFlow flow(flow_xe, flow_ye);
      Keypoint keypoint(j, i);
      const Keypoint predicted_kp =
          Feature::CalculatePredictedKeypoint(keypoint, flow);
      const size_t cell_idx = grid.getCellIndex(keypoint);

      // //TODO: this is a problem for the omd dataset?
      // if(grid.isOccupied(cell_idx)) {continue;}

      // if ((predicted_kp(0) < rgb.cols && predicted_kp(0) > 0 &&
      // predicted_kp(1) < rgb.rows && predicted_kp(1) > 0))
      if (isWithinShrunkenImage(keypoint) && !grid.isOccupied(cell_idx)) {
        // save correspondences
        auto tracklet_id = tracked_id_manager.getTrackletIdCount();
        tracked_id_manager.incrementTrackletIdCount();
        Feature::Ptr feature = std::make_shared<Feature>();
        (*feature)
            .objectId(label)
            .frameId(frame_id)
            .keypointType(KeyPointType::DYNAMIC)
            .age(0)
            .trackletId(tracklet_id)
            .keypoint(keypoint)
            .measuredFlow(flow)
            .predictedKeypoint(predicted_kp);

        dynamic_features.add(feature);
        instance_labels.push_back(feature->objectId());

        object_tracking_info.num_sampled++;
      } else {
        object_tracking_info.num_outside_shrunken_image++;
      }
    }
  }

  // bit of a hack -> make sure we have enough features on each object (around
  // 100) for no reason other than there is weird behaviour when a car
  // disappears (or is disappearing) usually becuase the depth is so bad and so
  // the object jumps of course this number depends on the size of the frame etc
  // etc this is just a tempory fix!!
  //  ObjectIds objects_removed;
  //  for(const auto& [object_id, status ]:info_.dynamic_track) {
  //    //cannot jus use the num tracked as on the first frame this will be zero
  //    //should track object age??
  //    if(status.num_track + status.num_sampled < 100) {
  //      VLOG(15) << "Removing object id " << object_id << " from tracks as not
  //      enough features! (" << (status.num_track + status.num_sampled) << " <
  //      100)"; dynamic_features.removeByObjectId(object_id);
  //      objects_removed.push_back(object_id);
  //    }
  //  }

  // //remove from info?
  // for (auto object_id : objects_removed) {
  //   info_.dynamic_track.erase(object_id);
  // }
}

void FeatureTracker::propogateMask(ImageContainer& image_container) {
  if (!previous_frame_) return;

  const cv::Mat& previous_rgb =
      previous_frame_->image_container_.get<ImageType::RGBMono>();
  const cv::Mat& previous_mask =
      previous_frame_->image_container_.get<ImageType::MotionMask>();
  const cv::Mat& previous_flow =
      previous_frame_->image_container_.get<ImageType::OpticalFlow>();

  // note reference
  cv::Mat& current_mask = image_container.get<ImageType::MotionMask>();

  ObjectIds instance_labels;
  for (const Feature::Ptr& dynamic_feature :
       previous_frame_->usableDynamicFeaturesBegin()) {
    CHECK(dynamic_feature->objectId() != background_label);
    instance_labels.push_back(dynamic_feature->objectId());
  }

  CHECK_EQ(instance_labels.size(), previous_frame_->numDynamicUsableFeatures());
  std::sort(instance_labels.begin(), instance_labels.end());
  instance_labels.erase(
      std::unique(instance_labels.begin(), instance_labels.end()),
      instance_labels.end());
  // each row is correlated with a specific instance label and each column is
  // the tracklet id associated with that label
  std::vector<TrackletIds> object_features(instance_labels.size());

  // collect the predicted labels and semantic labels in vector

  // TODO: inliers?
  for (const Feature::Ptr& dynamic_feature :
       previous_frame_->usableDynamicFeaturesBegin()) {
    CHECK(Feature::IsNotNull(dynamic_feature));
    for (size_t j = 0; j < instance_labels.size(); j++) {
      // save object label for object j with feature i
      if (dynamic_feature->objectId() == instance_labels[j]) {
        object_features[j].push_back(dynamic_feature->trackletId());
        CHECK(dynamic_feature->objectId() != background_label);
        break;
      }
    }
  }

  // check each object label distribution in the coming frame
  for (size_t i = 0; i < object_features.size(); i++) {
    // labels at the current mask using the predicted keypoint from the previous
    // frame each iteration is per label so temp_label should correspond to
    // features within the same object
    ObjectIds temp_label;
    for (size_t j = 0; j < object_features[i].size(); j++) {
      // feature at k-1
      Feature::Ptr feature = previous_frame_->dynamic_features_.getByTrackletId(
          object_features[i][j]);
      CHECK(Feature::IsNotNull(feature));
      // kp at k
      const Keypoint& predicted_kp = feature->predictedKeypoint();
      const int u = functional_keypoint::u(predicted_kp);
      const int v = functional_keypoint::v(predicted_kp);
      // ensure u and v are sitll inside the CURRENT frame
      if (u < previous_rgb.cols && u > 0 && v < previous_rgb.rows && v > 0) {
        // add instance label at predicted keypoint
        temp_label.push_back(current_mask.at<ObjectId>(v, u));
      }
    }

    // this is a lovely magic number inherited from some old code :)
    if (temp_label.size() < 150) {
      LOG(WARNING) << "not enoug points to track object " << instance_labels[i]
                   << " points size - " << temp_label.size();
      // TODO:mark has static!!???
      continue;
    }

    // find label that appears most in LabTmp()
    // (1) count duplicates
    std::map<int, int> label_duplicates;
    // k is object label
    for (int k : temp_label) {
      if (label_duplicates.find(k) == label_duplicates.end()) {
        label_duplicates.insert({k, 0});
      } else {
        label_duplicates.at(k)++;
      }
    }
    // (2) and sort them by descending order by number of times an object
    // appeared (ie. by pair.second)
    std::vector<std::pair<int, int>> sorted;
    for (auto k : label_duplicates) {
      sorted.push_back(std::make_pair(k.first, k.second));
    }

    auto sort_pair_int = [](const std::pair<int, int>& a,
                            const std::pair<int, int>& b) -> bool {
      return (a.second > b.second);
    };
    std::sort(sorted.begin(), sorted.end(), sort_pair_int);

    // recover the missing mask (time consuming!)
    // LOG(INFO) << sorted[0].first << " " << sorted[0].second << " " <<
    // instance_labels[i];
    //  if (sorted[0].second < 30)
    // {
    //   LOG(WARNING) << "not enoug points to track object " <<
    //   instance_labels[i] << " points size - "
    //                << sorted[0].second;
    //   //TODO:mark has static!!
    //   continue;
    // }
    if (sorted[0].first == 0)  //?
    // if (sorted[0].first == instance_labels[i])  //?
    {
      for (int j = 0; j < previous_rgb.rows; j++) {
        for (int k = 0; k < previous_rgb.cols; k++) {
          if (previous_mask.at<ObjectId>(j, k) == instance_labels[i]) {
            const double flow_xe =
                static_cast<double>(previous_flow.at<cv::Vec2f>(j, k)[0]);
            const double flow_ye =
                static_cast<double>(previous_flow.at<cv::Vec2f>(j, k)[1]);

            if (flow_xe == 0 || flow_ye == 0) {
              continue;
            }

            OpticalFlow flow(flow_xe, flow_ye);
            // x, y
            Keypoint kp(k, j);
            const Keypoint predicted_kp =
                Feature::CalculatePredictedKeypoint(kp, flow);

            if (!isWithinShrunkenImage(predicted_kp)) {
              continue;
            }

            if ((predicted_kp(0) < previous_rgb.cols && predicted_kp(0) > 0 &&
                 predicted_kp(1) < previous_rgb.rows && predicted_kp(1) > 0)) {
              current_mask.at<ObjectId>(functional_keypoint::v(predicted_kp),
                                        functional_keypoint::u(predicted_kp)) =
                  instance_labels[i];
              //  current_rgb
              // updated_mask_points++;
            }
          }
        }
      }
    }
  }
}

}  // namespace dyno
````

## File: frontend/vision/FeatureTrackerBase.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include "dynosam/frontend/vision/FeatureTrackerBase.hpp"

#include "dynosam/utils/GtsamUtils.hpp"
#include "dynosam/utils/OpenCVUtils.hpp"
#include "dynosam/visualizer/ColourMap.hpp"

namespace dyno {

decltype(TrackletIdManager::instance_) TrackletIdManager::instance_;

FeatureTrackerBase::FeatureTrackerBase(const TrackerParams& params,
                                       Camera::Ptr camera,
                                       ImageDisplayQueue* display_queue)
    : params_(params),
      img_size_(camera->getParams().imageSize()),
      camera_(camera),
      display_queue_(display_queue) {}

// doesnt make any sense for this function to be here?
// Debug could be part of a global config singleton?
cv::Mat FeatureTrackerBase::computeImageTracks(
    const Frame& previous_frame, const Frame& current_frame,
    const ImageTracksParams& config) const {
  cv::Mat img_rgb;

  const cv::Mat& rgb = current_frame.image_container_.get<ImageType::RGBMono>();
  rgb.copyTo(img_rgb);

  const bool debug = config.is_debug;
  const bool show_frame_info = debug && config.show_frame_info;
  const bool show_intermediate_tracking =
      debug && config.show_intermediate_tracking;

  static const cv::Scalar red(Color::red().bgra());
  static const cv::Scalar green(Color::green().bgra());
  static const cv::Scalar blue(Color::blue().bgra());

  constexpr static int kFeatureThicknessDebug = 5;
  constexpr static int kFeatureThickness = 4;
  // constexpr static int kFeatureThickness = 7;
  int static_point_thickness =
      debug ? kFeatureThicknessDebug : kFeatureThickness;

  int num_static_tracks = 0;
  // Add all keypoints in cur_frame with the tracks.
  for (const Feature::Ptr& feature : current_frame.static_features_) {
    const Keypoint& px_cur = feature->keypoint();
    const auto pc_cur = utils::gtsamPointToCv(px_cur);
    if (!feature->usable() &&
        show_intermediate_tracking) {  // Untracked landmarks are red.
      cv::circle(img_rgb, pc_cur, static_point_thickness, red, 2);
    } else {
      const Feature::Ptr& prev_feature =
          previous_frame.static_features_.getByTrackletId(
              feature->trackletId());
      if (prev_feature) {
        // If feature was in previous frame, display tracked feature with
        // green circle/line:
        cv::circle(img_rgb, pc_cur, static_point_thickness, green, 1);

        // draw the optical flow arrow
        const auto pc_prev = utils::gtsamPointToCv(prev_feature->keypoint());
        cv::arrowedLine(img_rgb, pc_prev, pc_cur, green, 1);

        num_static_tracks++;

      } else if (debug &&
                 show_intermediate_tracking) {  // New feature tracks are blue.
        cv::circle(img_rgb, pc_cur, 6, blue, 1);
      }
    }
  }

  for (const Feature::Ptr& feature : current_frame.dynamic_features_) {
    const Keypoint& px_cur = feature->keypoint();
    if (!feature->usable()) {  // Untracked landmarks are red.
      // cv::circle(img_rgb,  utils::gtsamPointToCv(px_cur), 1, red, 2);
    } else {
      const Feature::Ptr& prev_feature =
          previous_frame.dynamic_features_.getByTrackletId(
              feature->trackletId());
      if (prev_feature) {
        // If feature was in previous frame, display tracked feature with
        // green circle/line:
        // cv::circle(img_rgb,  utils::gtsamPointToCv(px_cur), 6, green, 1);
        const Keypoint& px_prev = prev_feature->keypoint();
        const cv::Scalar colour = Color::uniqueId(feature->objectId()).bgra();
        cv::arrowedLine(img_rgb, utils::gtsamPointToCv(px_prev),
                        utils::gtsamPointToCv(px_cur), colour, 1);
      } else {  // New feature tracks are blue.
        // cv::circle(img_rgb, utils::gtsamPointToCv(px_cur), 1, blue, 1);
      }
    }
  }

  std::vector<ObjectId> objects_to_print;
  for (const auto& object_observation_pair :
       current_frame.object_observations_) {
    const ObjectId object_id = object_observation_pair.first;
    const cv::Rect& bb = object_observation_pair.second.bounding_box_;

    // TODO: if its marked as moving!!
    if (bb.empty()) {
      continue;
    }

    objects_to_print.push_back(object_id);
    const cv::Scalar colour = Color::uniqueId(object_id).bgra();

    const std::string label = "object " + std::to_string(object_id);
    utils::drawLabeledBoundingBox(img_rgb, label, colour, bb);
  }

  // draw text info
  std::stringstream ss;
  ss << "Frame ID: " << current_frame.getFrameId() << " | ";
  ss << "VO tracks: " << num_static_tracks << " | ";
  ss << "Objects: ";

  if (objects_to_print.empty()) {
    ss << "None";
  } else {
    ss << "[";
    for (size_t i = 0; i < objects_to_print.size(); ++i) {
      ss << objects_to_print[i];
      if (i != objects_to_print.size() - 1) {
        ss << ", ";  // Add comma between elements
      }
    }
    ss << "]";
  }

  constexpr static double kFontScale = 0.6;
  constexpr static int kFontFace = cv::FONT_HERSHEY_SIMPLEX;
  constexpr static int kThickness = 1;

  if (debug && show_frame_info) {
    // taken from ORB-SLAM2 ;)
    int base_line;
    cv::Size text_size = cv::getTextSize(ss.str(), kFontFace, kFontScale,
                                         kThickness, &base_line);
    cv::Mat image_text = cv::Mat(img_rgb.rows + text_size.height + 10,
                                 img_rgb.cols, img_rgb.type());
    img_rgb.copyTo(
        image_text.rowRange(0, img_rgb.rows).colRange(0, img_rgb.cols));
    image_text.rowRange(img_rgb.rows, image_text.rows) =
        cv::Mat::zeros(text_size.height + 10, img_rgb.cols, img_rgb.type());
    cv::putText(image_text, ss.str(), cv::Point(5, image_text.rows - 5),
                kFontFace, kFontScale, cv::Scalar(255, 255, 255), kThickness);
    return image_text;
  } else {
    return img_rgb;
  }
}

bool FeatureTrackerBase::isWithinShrunkenImage(const Keypoint& kp) const {
  const auto shrunken_row = params_.shrink_row;
  const auto shrunken_col = params_.shrink_col;

  const int predicted_col = functional_keypoint::u(kp);
  const int predicted_row = functional_keypoint::v(kp);

  const auto image_rows = img_size_.height;
  const auto image_cols = img_size_.width;
  return (predicted_row > shrunken_row &&
          predicted_row < (image_rows - shrunken_row) &&
          predicted_col > shrunken_col &&
          predicted_col < (image_cols - shrunken_col));
}

}  // namespace dyno
````

## File: frontend/vision/Frame.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/common/Types.hpp"
#include "dynosam/frontend/vision/Frame.hpp"
#include "dynosam/frontend/vision/VisionTools.hpp"
#include "dynosam/visualizer/ColourMap.hpp"
#include "dynosam/utils/TimingStats.hpp"

namespace dyno {

ObjectId Frame::global_object_id{1};

Frame::Frame(
        FrameId frame_id,
        Timestamp timestamp,
        Camera::Ptr camera,
        const ImageContainer& image_container,
        const FeatureContainer& static_features,
        const FeatureContainer& dynamic_features,
        std::optional<FeatureTrackerInfo> tracking_info)
        :   frame_id_(frame_id),
            timestamp_(timestamp),
            camera_(camera),
            image_container_(image_container),
            static_features_(static_features),
            dynamic_features_(dynamic_features),
            tracking_info_(tracking_info)
        {
            constructDynamicObservations();

            // NOTE: no rectification, use camera matrix as P for cv::undistortPoints
            // see https://stackoverflow.com/questions/22027419/bad-results-when-undistorting-points-using-opencv-in-python
            //i mean, this could just be shared between frames?
            const CameraParams& cam_params = camera->getParams();
            cv::Mat P = cam_params.getCameraMatrix();
            cv::Mat R = cv::Mat::eye(3,3,CV_32FC1);
            undistorter_ = std::make_shared<UndistorterRectifier>(P, cam_params, R);
        }

bool Frame::exists(TrackletId tracklet_id) const {
    const bool result = static_features_.exists(tracklet_id) || dynamic_features_.exists(tracklet_id);

    //debug checking -> should only be in one feature container
    if(result) {
        CHECK(!(static_features_.exists(tracklet_id) &&  dynamic_features_.exists(tracklet_id)))
            << "Tracklet Id " <<  tracklet_id << " exists in both static and dynamic feature sets. Should be unique!";
    }
    return result;
}

Feature::Ptr Frame::at(TrackletId tracklet_id) const {
    if(!exists(tracklet_id)) {
        return nullptr;
    }

    if(static_features_.exists(tracklet_id)) {
        CHECK(!dynamic_features_.exists(tracklet_id));
        return static_features_.getByTrackletId(tracklet_id);
    }
    else {
        CHECK(dynamic_features_.exists(tracklet_id));
        return dynamic_features_.getByTrackletId(tracklet_id);
    }
}


bool Frame::isFeatureUsable(TrackletId tracklet_id) const {
    const auto& feature = at(tracklet_id);
    if(!feature) {
        throw std::runtime_error("Failed to check feature usability - tracklet id " + std::to_string(tracklet_id) + " does not exist");
    }

    return feature->usable();
}


FeaturePtrs Frame::collectFeatures(TrackletIds tracklet_ids) const {
    FeaturePtrs features;
    for(const auto tracklet_id : tracklet_ids) {
        Feature::Ptr feature = at(tracklet_id);
        if(!feature) {
            throw std::runtime_error("Failed to collectFeatures - tracklet id " + std::to_string(tracklet_id) + " does not exist");
        }

        features.push_back(feature);

    }

    return features;
}


Landmark Frame::backProjectToCamera(TrackletId tracklet_id) const {
    Feature::Ptr feature = at(tracklet_id);
    if(!feature) {
        throw std::runtime_error("Failed to backProjectToCamera - tracklet id " + std::to_string(tracklet_id) + " does not exist");
    }

    //if no depth, project to unitsphere?
    CHECK(feature->hasDepth());

    // Landmark lmk;
    return getLandmarkFromCache(landmark_in_camera_cache_, feature, gtsam::Pose3::Identity());
    // return lmk;
}

Landmark Frame::backProjectToWorld(TrackletId tracklet_id) const {
    Feature::Ptr feature = at(tracklet_id);
    if(!feature) {
        throw std::runtime_error("Failed to backProjectToWorld - tracklet id " + std::to_string(tracklet_id) + " does not exist");
    }

    //if no depth, project to unitsphere?
    CHECK(feature->hasDepth());

    // Landmark lmk;
    // camera_->backProject(feature->keypoint_, feature->depth_, &lmk, T_world_camera_);
    return getLandmarkFromCache(landmark_in_world_cache_, feature, T_world_camera_);
}


Camera::CameraImpl Frame::getFrameCamera() const {
    const CameraParams& camera_params = camera_->getParams();
    return Camera::CameraImpl(T_world_camera_, camera_params.constructGtsamCalibration<Camera::CalibrationType>());
}

bool Frame::updateDepths() {
    if(!image_container_.hasDepth()) {
        return false;
    }

    const ImageWrapper<ImageType::Depth>& depth = image_container_.getImageWrapper<ImageType::Depth>();
    updateDepthsFeatureContainer(static_features_, depth, max_background_threshold_);
    updateDepthsFeatureContainer(dynamic_features_, depth, max_object_threshold_);
    return true;
}

Frame& Frame::setMaxBackgroundDepth(double thresh) {
    CHECK_GT(thresh, 0);
    max_background_threshold_ = thresh;
    return *this;
}

Frame& Frame::setMaxObjectDepth(double thresh) {
    CHECK_GT(thresh, 0);
    max_object_threshold_ = thresh;
    return *this;
}


cv::Mat Frame::drawDetectedObjectBoxes() const {
    cv::Mat rgb_objects;
    image_container_.cloneImage<ImageType::RGBMono>(rgb_objects);

    for(auto& object_observation_pair : object_observations_) {
        const ObjectId object_id = object_observation_pair.first;
        const cv::Rect& bb = object_observation_pair.second.bounding_box_;

        if(bb.empty()) { continue; }

        const cv::Scalar colour = Color::uniqueId(object_id).bgra();
        const std::string label = "Obj " + std::to_string(object_id);
        utils::drawLabeledBoundingBox(rgb_objects, label, colour, bb);

    }

    return rgb_objects;
}



bool Frame::getCorrespondences(FeaturePairs& correspondences, const Frame& previous_frame, KeyPointType kp_type) const {
    if(kp_type == KeyPointType::STATIC) {
        return getStaticCorrespondences(correspondences, previous_frame);
    }
    else {
        return getDynamicCorrespondences(correspondences, previous_frame);
    }
}

Frame::ConstructCorrespondanceFunc<Landmark, Keypoint> Frame::landmarkWorldKeypointCorrespondance() const {
    auto func = [&](const Frame& previous_frame, const Feature::Ptr& previous_feature, const Feature::Ptr& current_feature) {
        if(!previous_feature->hasDepth()) {
            throw std::runtime_error("Error in constructing Landmark (w) -> keypoint correspondences - previous feature does not have depth!");
        }

        //eventuall map?
        Landmark lmk_w = previous_frame.backProjectToWorld(previous_feature->trackletId());
        return TrackletCorrespondance(previous_feature->trackletId(), lmk_w, current_feature->keypoint());
    };

    return std::bind(func, std::placeholders::_1, std::placeholders::_2, std::placeholders::_3);
}


Frame::ConstructCorrespondanceFunc<Keypoint, Keypoint> Frame::imageKeypointCorrespondance() const {
    auto func = [&](const Frame&, const Feature::Ptr& previous_feature, const Feature::Ptr& current_feature) {
        return TrackletCorrespondance(previous_feature->trackletId(), previous_feature->keypoint(), current_feature->keypoint());
    };

    return std::bind(func, std::placeholders::_1, std::placeholders::_2, std::placeholders::_3);

}

Frame::ConstructCorrespondanceFunc<Landmark, gtsam::Vector3> Frame::landmarkWorldProjectedBearingCorrespondance() const {
    auto func = [&](const Frame& previous_frame, const Feature::Ptr& previous_feature, const Feature::Ptr& current_feature) {
        if(!previous_feature->hasDepth()) {
            throw std::runtime_error("Error in constructing Landmark (w) -> keypoint correspondences - previous feature does not have depth!");
        }

        //eventuall map?
        Landmark lmk_w = previous_frame.backProjectToWorld(previous_feature->trackletId());

        //TODO: what if image is already undistorted
        gtsam::Vector3 projected_versor = undistorter_->undistortKeypointAndGetProjectedVersor(current_feature->keypoint());
        return TrackletCorrespondance(previous_feature->trackletId(), lmk_w, projected_versor);
    };

    return std::bind(func, std::placeholders::_1, std::placeholders::_2, std::placeholders::_3);
}

Frame::ConstructCorrespondanceFunc<Landmark, Landmark> Frame::landmarkWorldPointCloudCorrespondance() const {
     auto func = [&](const Frame& previous_frame, const Feature::Ptr& previous_feature, const Feature::Ptr& current_feature) {
        if(!previous_feature->hasDepth()) {
            throw std::runtime_error("Error in constructing Landmark (w) -> keypoint correspondences - previous feature does not have depth!");
        }

        //eventuall map?
        Landmark lmk_w_k_1 = previous_frame.backProjectToWorld(previous_feature->trackletId());
        //eventuall map?
        Landmark lmk_w_k = backProjectToWorld(current_feature->trackletId());

        return TrackletCorrespondance(previous_feature->trackletId(), lmk_w_k_1, lmk_w_k);
    };

    return std::bind(func, std::placeholders::_1, std::placeholders::_2, std::placeholders::_3);
}


bool Frame::getDynamicCorrespondences(FeaturePairs& correspondences, const Frame& previous_frame, ObjectId object_id) const {

    if(object_observations_.find(object_id) == object_observations_.end()) {
        LOG(WARNING) << "Object object instance id " << object_id << " not found for frame " << frame_id_;
        return false;
    }

    // const DynamicObjectObservation& observation = object_observations_.at(object_id);
    // // TODO: need to put back on - if we have motion mask, we should just mark all objects as moving CHECK(observation.marked_as_moving_);
    // const TrackletIds& tracklets = observation.object_features_;

    // FeatureContainer feature_container;
    // for(const TrackletId tracklet : tracklets) {
    //     if(isFeatureUsable(tracklet)) {
    //         feature_container.add(this->at(tracklet));
    //     }
    // }

    auto current_dynamic_features_iterator = FeatureFilterIterator(
        const_cast<FeatureContainer&>(this->dynamic_features_),
        [object_id](const Feature::Ptr& f) -> bool {
            return Feature::IsUsable(f) && f->objectId() == object_id;
        });


    //make iterator for the previous dynamic features that ensure each feature is usable and has a matching instance label
    auto previous_dynamic_features_iterator = FeatureFilterIterator(
        const_cast<FeatureContainer&>(previous_frame.dynamic_features_),
        [object_id](const Feature::Ptr& f) -> bool {
            return Feature::IsUsable(f) && f->objectId() == object_id;
        });

    //get the correspondences from these two iterators
    //we iterate over the current feature container which should only contain features on the object
    //and compare against the container
    vision_tools::getCorrespondences(
        correspondences,
        previous_dynamic_features_iterator,
        //we iterate over the current feature container which should only contain features on the object
        current_dynamic_features_iterator
    );

    // LOG(INFO) << "Found " << correspondences.size() << " correspondences for object instance " << object_id << " " << (correspondences.size() > 0u);

    return correspondences.size() > 0u;
}


bool Frame::getStaticCorrespondences(FeaturePairs& correspondences, const Frame& previous_frame) const {
    vision_tools::getCorrespondences(
        correspondences,
        previous_frame.static_features_.beginUsable(),
        static_features_.beginUsable()
    );

    return correspondences.size() > 0u;
}

bool Frame::getDynamicCorrespondences(FeaturePairs& correspondences, const Frame& previous_frame) const {
    vision_tools::getCorrespondences(
        correspondences,
        previous_frame.dynamic_features_.beginUsable(),
        dynamic_features_.beginUsable()
    );
    return correspondences.size() > 0u;
}


void Frame::updateDepthsFeatureContainer(FeatureContainer& container, const ImageWrapper<ImageType::Depth>& depth, double max_depth) {
    // auto iter = container.beginUsable();
    // auto iter = container.begin();

    int count = 0;

    //iterate over all features
    for(Feature::Ptr feature : container) {
        // CHECK(feature->usable());
        // const Feature::Ptr& feature = *iter;
        // const int x = functional_keypoint::u(feature->keypoint_);
        // const int y = functional_keypoint::v(feature->keypoint_);
        // const Depth d = depth_mat.at<Depth>(y, x);
        const Depth d = functional_keypoint::at<Depth>(feature->keypoint(), depth);

        if(d > max_depth || d <= 0) {
            feature->markInvalid();
            feature->depth(Feature::invalid_depth);
            count++;
        }
        else {
            feature->depth(d);
        }

        //  //if now invalid or happens to be invalid from a previous frame, make depth invalid too
        // if(!feature->usable()) {
        //     feature->depth_ = Feature::invalid_depth;
        // }
        // else {
        //     feature->depth_ = d;
        // }
    }

    // LOG(INFO) << count << " features marked invalud due to depth out of " << container.size() << " with max depth " << max_depth;

}


void Frame::constructDynamicObservations() {
    object_observations_.clear();
    //assumes that the mask gets updated with the tracking label
    const ObjectIds instance_labels = vision_tools::getObjectLabels(image_container_.get<ImageType::MotionMask>());

    auto inlier_iterator = dynamic_features_.beginUsable();
    for(const Feature::Ptr& dynamic_feature : inlier_iterator) {
        CHECK(!dynamic_feature->isStatic());
        CHECK(dynamic_feature->usable());

        const ObjectId instance_label = dynamic_feature->objectId();
        //this check is just for sanity!
        CHECK(std::find(instance_labels.begin(), instance_labels.end(), instance_label) != instance_labels.end())
            << "Missing " << instance_label << " in " << container_to_string(instance_labels);

        if(object_observations_.find(instance_label) == object_observations_.end()) {
            DynamicObjectObservation observation;
            observation.tracking_label_ = -1;
            observation.instance_label_ = instance_label;
            object_observations_[instance_label] = observation;
        }

        object_observations_[instance_label].object_features_.push_back(dynamic_feature->trackletId());
    }

    // now construct image masks from tracking mask
    // For each tracked object, find its id in the mask
    // and draw it.
    // We apply some eroding/dilation on it to make the resulting submask smoother
    // so that we can more easily fit an rectangle to it
    const cv::Mat& mask = image_container_.get<ImageType::MotionMask>();
    for(auto& object_observation_pair : object_observations_) {
        const ObjectId object_id = object_observation_pair.first;
        DynamicObjectObservation& obs = object_observation_pair.second;
        vision_tools::findObjectBoundingBox(mask, object_id, obs.bounding_box_);
    }


}

void Frame::moveObjectToStatic(ObjectId instance_label) {
    auto it = object_observations_.find(instance_label);
    CHECK(it != object_observations_.end());


    DynamicObjectObservation& observation = it->second;
    observation.marked_as_moving_ = false;
    CHECK(observation.instance_label_ == instance_label);
    //go through all features, move them to from dynamic structure and add them to static
    for(TrackletId tracklet_id : observation.object_features_) {
        CHECK(dynamic_features_.exists(tracklet_id));
        Feature::Ptr dynamic_feature = dynamic_features_.getByTrackletId(tracklet_id);

        if(!dynamic_feature->usable()) {continue;}

        CHECK(!dynamic_feature->isStatic());
        CHECK_EQ(dynamic_feature->trackletId(), tracklet_id);
        CHECK_EQ(dynamic_feature->objectId(), instance_label);
        dynamic_feature->keypointType(KeyPointType::STATIC);
        dynamic_feature->objectId(background_label);
        // dynamic_feature->tracking_label_ = background_label;

        dynamic_features_.remove(tracklet_id);
        //Jesse: no, do not move points (these are dense) to static - instrad we need to mark the AREA
        //around the object as static and then retrack all points in there!!
        // static_features_.add(dynamic_feature);
    }

    object_observations_.erase(it);

}

void Frame::updateObjectTrackingLabel(const DynamicObjectObservation& observation, ObjectId new_tracking_label) {
    auto it = object_observations_.find(observation.instance_label_);
    CHECK(it != object_observations_.end());

    auto& obs = it->second;
    obs.tracking_label_ = new_tracking_label;
    //update all features
    for(TrackletId tracklet_id : obs.object_features_) {
        Feature::Ptr feature = dynamic_features_.getByTrackletId(tracklet_id);
        CHECK(feature);
        feature->objectId(new_tracking_label);
    }
}


FeatureFilterIterator Frame::usableStaticFeaturesBegin() {
    return static_features_.beginUsable();
}

FeatureFilterIterator Frame::usableStaticFeaturesBegin() const {
    return static_features_.beginUsable();
}

FeatureFilterIterator Frame::usableDynamicFeaturesBegin() {
    return dynamic_features_.beginUsable();
}

FeatureFilterIterator Frame::usableDynamicFeaturesBegin() const {
    return dynamic_features_.beginUsable();
}



Landmark Frame::getLandmarkFromCache(LandmarkMap& cache, Feature::Ptr feature, const gtsam::Pose3& X_world) const {
    //TODO: dont cache as we now update the optical flow and the depth in the frontend and cacheing it will not use the right values!!!
    // const auto& it = cache.find(feature->tracklet_id_);
    // if(it != cache.end()) {
    //     return it->second;
    // }

    Landmark lmk;
    camera_->backProject(feature->keypoint(), feature->depth(), &lmk, X_world);
    // cache.insert({feature->tracklet_id_, lmk});
    return lmk;
}

// Frame::FeatureFilterIterator Frame::dynamicUsableBegin() {
//     return FeatureFilterIterator(dynamic_features_, [&](const Feature::Ptr& f) -> bool
//         {
//             return f->usable();
//         }
//     );
// }

} //dyno
````

## File: frontend/vision/MotionSolver.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include "dynosam/frontend/vision/MotionSolver.hpp"

#include <config_utilities/config_utilities.h>
#include <glog/logging.h>
#include <gtsam/nonlinear/LevenbergMarquardtOptimizer.h>  //for now? //TODO: clean
#include <gtsam/nonlinear/NonlinearFactorGraph.h>
#include <gtsam/nonlinear/Values.h>

#include <eigen3/Eigen/Dense>
#include <opencv4/opencv2/core.hpp>
#include <opencv4/opencv2/core/eigen.hpp>
#include <opengv/types.hpp>

#include "dynosam/backend/BackendDefinitions.hpp"
#include "dynosam/backend/FactorGraphTools.hpp"  //TODO: clean
#include "dynosam/common/Flags.hpp"
#include "dynosam/common/Types.hpp"
#include "dynosam/factors/LandmarkMotionTernaryFactor.hpp"
#include "dynosam/factors/Pose3FlowProjectionFactor.h"
#include "dynosam/frontend/vision/VisionTools.hpp"
#include "dynosam/utils/Accumulator.hpp"
#include "dynosam/utils/GtsamUtils.hpp"
#include "dynosam/utils/Numerical.hpp"
#include "dynosam/utils/TimingStats.hpp"

namespace dyno {

void declare_config(OpticalFlowAndPoseOptimizer::Params& config) {
  using namespace config;

  name("OpticalFlowAndPoseOptimizerParams");
  field(config.flow_sigma, "flow_sigma");
  field(config.flow_prior_sigma, "flow_prior_sigma");
  field(config.k_huber, "k_huber");
  field(config.outlier_reject, "outlier_reject");
  field(config.flow_is_future, "flow_is_future");
}

void declare_config(MotionOnlyRefinementOptimizer::Params& config) {
  using namespace config;

  name("MotionOnlyRefinementOptimizerParams");
  field(config.landmark_motion_sigma, "landmark_motion_sigma");
  field(config.projection_sigma, "projection_sigma");
  field(config.k_huber, "k_huber");
  field(config.outlier_reject, "outlier_reject");
}

void declare_config(EgoMotionSolver::Params& config) {
  using namespace config;

  name("EgoMotionSolver::Params");
  field(config.ransac_randomize, "ransac_randomize");
  field(config.ransac_use_2point_mono, "ransac_use_2point_mono");
  field(config.optimize_2d2d_pose_from_inliers,
        "optimize_2d2d_pose_from_inliers");
  field(config.ransac_threshold_pnp, "ransac_threshold_pnp");
  field(config.optimize_3d2d_pose_from_inliers,
        "optimize_3d2d_pose_from_inliers");
  field(config.ransac_threshold_stereo, "ransac_threshold_stereo");
  field(config.optimize_3d3d_pose_from_inliers,
        "optimize_3d3d_pose_from_inliers");
  field(config.ransac_iterations, "ransac_iterations");
  field(config.ransac_probability, "ransac_probability");
}
void declare_config(ObjectMotionSovler::Params& config) {
  using namespace config;
  name("ObjectMotionSovler::Params");

  base<EgoMotionSolver::Params>(config);
  field(config.refine_motion_with_joint_of, "refine_motion_with_joint_of");
  field(config.refine_motion_with_3d, "refine_motion_with_3d");
  field(config.joint_of_params, "joint_optical_flow");
  field(config.object_motion_refinement_params, "object_motion_3d_refinement");
}

EgoMotionSolver::EgoMotionSolver(const Params& params,
                                 const CameraParams& camera_params)
    : params_(params), camera_params_(camera_params) {}

Pose3SolverResult EgoMotionSolver::geometricOutlierRejection2d2d(
    Frame::Ptr frame_k_1, Frame::Ptr frame_k,
    std::optional<gtsam::Rot3> R_curr_ref) {
  // get correspondences
  RelativePoseCorrespondences correspondences;
  // this does not create proper bearing vectors (at leas tnot for 3d-2d pnp
  // solve) bearing vectors are also not undistorted atm!!
  {
    utils::TimingStatsCollector track_dynamic_timer(
        "mono_frame_correspondences");
    frame_k->getCorrespondences(correspondences, *frame_k_1,
                                KeyPointType::STATIC,
                                frame_k->imageKeypointCorrespondance());
  }

  Pose3SolverResult result;

  const size_t& n_matches = correspondences.size();

  if (n_matches < 5u) {
    result.status = TrackingStatus::FEW_MATCHES;
    return result;
  }

  gtsam::Matrix K = camera_params_.getCameraMatrixEigen();
  K = K.inverse();

  TrackletIds tracklets;
  // NOTE: currently without distortion! the correspondences should be made into
  // bearing vector elsewhere!
  BearingVectors ref_bearing_vectors, cur_bearing_vectors;
  for (size_t i = 0u; i < n_matches; i++) {
    const auto& corres = correspondences.at(i);
    const Keypoint& ref_kp = corres.ref_;
    const Keypoint& cur_kp = corres.cur_;

    gtsam::Vector3 ref_versor = (K * gtsam::Vector3(ref_kp(0), ref_kp(1), 1.0));
    gtsam::Vector3 cur_versor = (K * gtsam::Vector3(cur_kp(0), cur_kp(1), 1.0));

    ref_versor = ref_versor.normalized();
    cur_versor = cur_versor.normalized();

    ref_bearing_vectors.push_back(ref_versor);
    cur_bearing_vectors.push_back(cur_versor);

    tracklets.push_back(corres.tracklet_id_);
  }

  RelativePoseAdaptor adapter(ref_bearing_vectors, cur_bearing_vectors);

  const bool use_2point_mono = params_.ransac_use_2point_mono && R_curr_ref;
  if (use_2point_mono) {
    adapter.setR12((*R_curr_ref).matrix());
  }

  gtsam::Pose3 best_result;
  std::vector<int> ransac_inliers;
  bool success = false;
  if (use_2point_mono) {
    success = runRansac<RelativePoseProblemGivenRot>(
        std::make_shared<RelativePoseProblemGivenRot>(adapter,
                                                      params_.ransac_randomize),
        params_.ransac_threshold_mono, params_.ransac_iterations,
        params_.ransac_probability, params_.optimize_2d2d_pose_from_inliers,
        best_result, ransac_inliers);
  } else {
    success = runRansac<RelativePoseProblem>(
        std::make_shared<RelativePoseProblem>(
            adapter, RelativePoseProblem::NISTER, params_.ransac_randomize),
        params_.ransac_threshold_mono, params_.ransac_iterations,
        params_.ransac_probability, params_.optimize_2d2d_pose_from_inliers,
        best_result, ransac_inliers);
  }

  if (!success) {
    result.status = TrackingStatus::INVALID;
  } else {
    constructTrackletInliers(result.inliers, result.outliers, correspondences,
                             ransac_inliers, tracklets);
    // NOTE: 2-point always returns the identity rotation, hence we have to
    // substitute it:
    if (use_2point_mono) {
      CHECK(R_curr_ref->equals(best_result.rotation()));
    }
    result.status = TrackingStatus::VALID;
    result.best_result = best_result;
  }

  return result;
}

Pose3SolverResult EgoMotionSolver::geometricOutlierRejection3d2d(
    Frame::Ptr frame_k_1, Frame::Ptr frame_k,
    std::optional<gtsam::Rot3> R_curr_ref) {
  AbsolutePoseCorrespondences correspondences;
  // this does not create proper bearing vectors (at leas tnot for 3d-2d pnp
  // solve) bearing vectors are also not undistorted atm!!
  // TODO: change to use landmarkWorldProjectedBearingCorrespondance and then
  // change motion solver to take already projected bearing vectors
  {
    frame_k->getCorrespondences(correspondences, *frame_k_1,
                                KeyPointType::STATIC,
                                frame_k->landmarkWorldKeypointCorrespondance());
  }

  return geometricOutlierRejection3d2d(correspondences, R_curr_ref);
}

Pose3SolverResult EgoMotionSolver::geometricOutlierRejection3d2d(
    const AbsolutePoseCorrespondences& correspondences,
    std::optional<gtsam::Rot3> R_curr_ref) {
  utils::TimingStatsCollector timer("motion_solver.solve_3d2d");
  Pose3SolverResult result;
  const size_t& n_matches = correspondences.size();

  if (n_matches < 5u) {
    result.status = TrackingStatus::FEW_MATCHES;
    VLOG(5) << "3D2D tracking failed as there are to few matches" << n_matches;
    return result;
  }

  gtsam::Matrix K = camera_params_.getCameraMatrixEigen();
  K = K.inverse();

  TrackletIds tracklets, inliers, outliers;
  // NOTE: currently without distortion! the correspondences should be made into
  // bearing vector elsewhere!
  BearingVectors bearing_vectors;
  Landmarks points;
  for (size_t i = 0u; i < n_matches; i++) {
    const AbsolutePoseCorrespondence& corres = correspondences.at(i);
    const Keypoint& kp = corres.cur_;
    // make Bearing vector
    gtsam::Vector3 versor = (K * gtsam::Vector3(kp(0), kp(1), 1.0));
    versor = versor.normalized();
    bearing_vectors.push_back(versor);

    points.push_back(corres.ref_);
    tracklets.push_back(corres.tracklet_id_);
  }

  const double reprojection_error = params_.ransac_threshold_pnp;
  const double avg_focal_length =
      0.5 * static_cast<double>(camera_params_.fx() + camera_params_.fy());
  const double threshold =
      1.0 - std::cos(std::atan(std::sqrt(2.0) * reprojection_error /
                               avg_focal_length));
  // const double threshold = params_.ransac_threshold_pnp;

  AbsolutePoseAdaptor adapter(bearing_vectors, points);

  gtsam::Pose3 best_result;
  std::vector<int> ransac_inliers;

  bool success = runRansac<AbsolutePoseProblem>(
      std::make_shared<AbsolutePoseProblem>(adapter,
                                            AbsolutePoseProblem::KNEIP),
      threshold, params_.ransac_iterations, params_.ransac_probability,
      params_.optimize_3d2d_pose_from_inliers, best_result, ransac_inliers);

  if (success) {
    constructTrackletInliers(result.inliers, result.outliers, correspondences,
                             ransac_inliers, tracklets);

    if (result.inliers.size() < 5u) {
      result.status = TrackingStatus::FEW_MATCHES;
    } else {
      result.status = TrackingStatus::VALID;
      result.best_result = best_result;
    }

  } else {
    result.status = TrackingStatus::INVALID;
  }

  return result;
}

void OpticalFlowAndPoseOptimizer::updateFrameOutliersWithResult(
    const Result& result, Frame::Ptr frame_k_1, Frame::Ptr frame_k) const {
  // //original flow image that goes from k to k+1 (gross, im sorry!)
  // TODO: use flow_is_future param
  const cv::Mat& flow_image =
      frame_k->image_container_.get<ImageType::OpticalFlow>();
  const cv::Mat& motion_mask =
      frame_k->image_container_.get<ImageType::MotionMask>();

  auto camera = frame_k->camera_;
  const auto& refined_inliers = result.inliers;
  const auto& refined_flows = result.best_result.refined_flows;

  // outliers from the result. We will update this vector with new outliers
  auto refined_outliers = result.outliers;

  for (size_t i = 0; i < refined_inliers.size(); i++) {
    TrackletId tracklet_id = refined_inliers.at(i);
    gtsam::Point2 refined_flow = refined_flows.at(i);

    const Feature::Ptr feature_k_1 = frame_k_1->at(tracklet_id);
    Feature::Ptr feature_k = frame_k->at(tracklet_id);

    CHECK_EQ(feature_k->objectId(), result.best_result.object_id);

    const Keypoint kp_k_1 = feature_k_1->keypoint();
    Keypoint refined_keypoint = kp_k_1 + refined_flow;

    // check boundaries?
    if (!camera->isKeypointContained(refined_keypoint)) {
      refined_outliers.push_back(tracklet_id);
      continue;
    }

    feature_k->keypoint(refined_keypoint);
    ObjectId predicted_label =
        functional_keypoint::at<ObjectId>(refined_keypoint, motion_mask);
    if (predicted_label != result.best_result.object_id) {
      refined_outliers.push_back(tracklet_id);
      // TODO: other fields of the feature does not get updated? Inconsistencies
      // as measured flow, predicted kp etc are no longer correct!!?
      continue;
    }

    // we now have to update the prediced keypoint using the original flow!!
    // TODO: code copied from feature tracker
    const int x = functional_keypoint::u(refined_keypoint);
    const int y = functional_keypoint::v(refined_keypoint);
    double flow_xe = static_cast<double>(flow_image.at<cv::Vec2f>(y, x)[0]);
    double flow_ye = static_cast<double>(flow_image.at<cv::Vec2f>(y, x)[1]);
    // the measured flow after the origin has been updated
    OpticalFlow new_measured_flow(flow_xe, flow_ye);
    feature_k->measuredFlow(new_measured_flow);
    // TODO: check predicted flow is within image
    Keypoint predicted_kp = Feature::CalculatePredictedKeypoint(
        refined_keypoint, new_measured_flow);
    feature_k->predictedKeypoint(predicted_kp);
  }

  // update tracks
  for (const auto& outlier_tracklet : refined_outliers) {
    Feature::Ptr feature_k_1 = frame_k_1->at(outlier_tracklet);
    Feature::Ptr feature_k = frame_k->at(outlier_tracklet);

    CHECK(feature_k_1->usable());
    CHECK(feature_k->usable());

    feature_k->markOutlier();
    feature_k_1->markOutlier();
  }

  // refresh depth information for each frame
  CHECK(frame_k->updateDepths());
}

Pose3SolverResult EgoMotionSolver::geometricOutlierRejection3d3d(
    Frame::Ptr frame_k_1, Frame::Ptr frame_k,
    std::optional<gtsam::Rot3> R_curr_ref) {
  PointCloudCorrespondences correspondences;
  {
    utils::TimingStatsCollector("pc_correspondences");
    frame_k->getCorrespondences(
        correspondences, *frame_k_1, KeyPointType::STATIC,
        frame_k->landmarkWorldPointCloudCorrespondance());
  }

  return geometricOutlierRejection3d3d(correspondences, R_curr_ref);
}

Pose3SolverResult EgoMotionSolver::geometricOutlierRejection3d3d(
    const PointCloudCorrespondences& correspondences,
    std::optional<gtsam::Rot3> R_curr_ref) {
  const size_t& n_matches = correspondences.size();

  Pose3SolverResult result;
  if (n_matches < 5) {
    result.status = TrackingStatus::FEW_MATCHES;
    return result;
  }

  TrackletIds tracklets;
  BearingVectors ref_bearing_vectors, cur_bearing_vectors;

  for (size_t i = 0u; i < n_matches; i++) {
    const auto& corres = correspondences.at(i);
    const Landmark& ref_lmk = corres.ref_;
    const Landmark& cur_lmk = corres.cur_;
    ref_bearing_vectors.push_back(ref_lmk);
    cur_bearing_vectors.push_back(cur_lmk);

    tracklets.push_back(corres.tracklet_id_);
  }

  //! Setup adapter.
  Adapter3d3d adapter(ref_bearing_vectors, cur_bearing_vectors);

  if (R_curr_ref) {
    adapter.setR12((*R_curr_ref).matrix());
  }

  gtsam::Pose3 best_result;
  std::vector<int> ransac_inliers;

  bool success = runRansac<Problem3d3d>(
      std::make_shared<Problem3d3d>(adapter, params_.ransac_randomize),
      params_.ransac_threshold_stereo, params_.ransac_iterations,
      params_.ransac_probability, params_.optimize_3d3d_pose_from_inliers,
      best_result, ransac_inliers);

  if (success) {
    constructTrackletInliers(result.inliers, result.outliers, correspondences,
                             ransac_inliers, tracklets);

    result.status = TrackingStatus::VALID;
    result.best_result = best_result;
  } else {
    result.status = TrackingStatus::INVALID;
  }

  return result;
}

ObjectMotionSovler::ObjectMotionSovler(const Params& params,
                                       const CameraParams& camera_params)
    : EgoMotionSolver(static_cast<const EgoMotionSolver::Params&>(params),
                      camera_params) {}

Pose3SolverResult ObjectMotionSovler::geometricOutlierRejection3d2d(
    Frame::Ptr frame_k_1, Frame::Ptr frame_k, const gtsam::Pose3& T_world_k,
    ObjectId object_id) {
  utils::TimingStatsCollector timer("motion_solver.object_solve3d2d");
  AbsolutePoseCorrespondences dynamic_correspondences;
  // get the corresponding feature pairs
  bool corr_result = frame_k->getDynamicCorrespondences(
      dynamic_correspondences, *frame_k_1, object_id,
      frame_k->landmarkWorldKeypointCorrespondance());

  const size_t& n_matches = dynamic_correspondences.size();

  TrackletIds all_tracklets;
  std::transform(dynamic_correspondences.begin(), dynamic_correspondences.end(),
                 std::back_inserter(all_tracklets),
                 [](const AbsolutePoseCorrespondence& corres) {
                   return corres.tracklet_id_;
                 });
  CHECK_EQ(all_tracklets.size(), n_matches);

  Pose3SolverResult geometric_result =
      EgoMotionSolver::geometricOutlierRejection3d2d(dynamic_correspondences);
  Pose3SolverResult result = geometric_result;

  // if(geometric_result.status == TrackingStatus::VALID &&
  // motion_model_result.status == TrackingStatus::VALID) {
  //     if(geometric_result.inliers.size() >=
  //     motion_model_result.inliers.size()) {
  //         VLOG(10) << "Geometric model used (inliers " <<
  //         geometric_result.inliers.size() << " vs " <<
  //         motion_model_result.inliers.size(); result = geometric_result;
  //     }
  //     else {
  //         VLOG(10) << "Motion motion model used (inliers " <<
  //         motion_model_result.inliers.size() << " vs " <<
  //         geometric_result.inliers.size(); result = motion_model_result;
  //     }
  // }
  // //if no motion model, fall back on gemoetric result
  // else {
  //     result = geometric_result;
  // }

  if (result.status == TrackingStatus::VALID) {
    TrackletIds refined_inlier_tracklets = result.inliers;

    {
      // debug only (just checking that the inlier/outliers we get from the
      // geometric rejection match the original one)
      TrackletIds extracted_all_tracklets = refined_inlier_tracklets;
      extracted_all_tracklets.insert(extracted_all_tracklets.end(),
                                     result.outliers.begin(),
                                     result.outliers.end());
      CHECK_EQ(all_tracklets.size(), extracted_all_tracklets.size());
    }

    gtsam::Pose3 G_w = result.best_result.inverse();
    if (object_motion_params.refine_motion_with_joint_of) {
      OpticalFlowAndPoseOptimizer flow_optimizer(
          object_motion_params.joint_of_params);
      // Use the original result as the input to the refine joint optical flow
      // function the result.best_result variable is actually equivalent to
      // ^wG^{-1} and we want to solve something in the form e(T, flow) =
      // [u,v]_{k-1} + {k-1}_flow_k - pi(T^{-1}^wm_{k-1}) so T must take the
      // point from k-1 in the world frame to the local frame at k-1 ^wG^{-1} =
      //^wX_k \: {k-1}^wH_k (which takes does this) but the error term uses the
      // inverse of T hence we must parse in the inverse of G
      auto flow_opt_result = flow_optimizer.optimizeAndUpdate<CalibrationType>(
          frame_k_1, frame_k, refined_inlier_tracklets, result.best_result);
      // still need to take the inverse as we get the inverse of G out
      G_w = flow_opt_result.best_result.refined_pose.inverse();
      // inliers should be a subset of the original refined inlier tracks
      refined_inlier_tracklets = flow_opt_result.inliers;
    }
    // still need to take the inverse as we get the inverse of G out
    gtsam::Pose3 H_w = T_world_k * G_w;

    // if(params_.refine_object_motion_esimate) {
    if (object_motion_params.refine_motion_with_3d) {
      MotionOnlyRefinementOptimizer motion_refinement_graph(
          object_motion_params.object_motion_refinement_params);
      auto motion_refinement_result =
          motion_refinement_graph.optimizeAndUpdate<CalibrationType>(
              frame_k_1, frame_k, refined_inlier_tracklets, object_id, H_w);

      // should be further subset
      refined_inlier_tracklets = motion_refinement_result.inliers;
      H_w = motion_refinement_result.best_result;
    }
    // a lot of weird places where we mark things as inliers,take results of
    // some functions into others etc.. and is very confusion!
    // TODO: clean up!!!
    //  result = result_copy;
    result.best_result = H_w;
    result.inliers = refined_inlier_tracklets;
    determineOutlierIds(result.inliers, all_tracklets, result.outliers);
  }

  // if not valid, return motion result as is
  return result;
}

Pose3SolverResult ObjectMotionSovler::geometricOutlierRejection3d3d(
    Frame::Ptr frame_k_1, Frame::Ptr frame_k, const gtsam::Pose3& T_world_k,
    ObjectId object_id) {
  PointCloudCorrespondences dynamic_correspondences;
  // get the corresponding feature pairs
  bool corr_result = frame_k->getDynamicCorrespondences(
      dynamic_correspondences, *frame_k_1, object_id,
      frame_k->landmarkWorldPointCloudCorrespondance());

  const size_t& n_matches = dynamic_correspondences.size();

  Pose3SolverResult result;
  result =
      EgoMotionSolver::geometricOutlierRejection3d3d(dynamic_correspondences);

  if (result.status == TrackingStatus::VALID) {
    const gtsam::Pose3 G_w = result.best_result.inverse();
    const gtsam::Pose3 H_w = T_world_k * G_w;
    result.best_result = H_w;
  }

  // if not valid, return motion result as is
  return result;
}

// TODO: dont actually need all these variables
Pose3SolverResult ObjectMotionSovler::motionModelOutlierRejection3d2d(
    const AbsolutePoseCorrespondences& dynamic_correspondences,
    Frame::Ptr frame_k_1, Frame::Ptr frame_k, const gtsam::Pose3& T_world_k,
    ObjectId object_id) {
  Pose3SolverResult result;

  // get object motions in previous frame (so k-2 to k-1)
  const MotionEstimateMap& motion_estiamtes_k_1 = frame_k_1->motion_estimates_;
  if (!motion_estiamtes_k_1.exists(object_id)) {
    result.status = TrackingStatus::INVALID;
    return result;
  }

  // previous motion model: k-2 to k-1 in w
  const gtsam::Pose3 motion_model = motion_estiamtes_k_1.at(object_id);

  using Calibration = Camera::CalibrationType;
  const auto calibration =
      camera_params_.constructGtsamCalibration<Calibration>();

  auto I = gtsam::traits<gtsam::Pose3>::Identity();
  gtsam::PinholeCamera<Calibration> camera(I, calibration);

  const double reprojection_error = params_.ransac_threshold_pnp;

  TrackletIds tracklets;
  TrackletIds inlier_tracklets;

  utils::Accumulatord total_repr_error, inlier_repr_error;

  const size_t& n_matches = dynamic_correspondences.size();
  for (size_t i = 0u; i < n_matches; i++) {
    const AbsolutePoseCorrespondence& corres = dynamic_correspondences.at(i);
    tracklets.push_back(corres.tracklet_id_);

    const Keypoint& kp_k = corres.cur_;
    // the landmark int the world frame at k-1
    const Landmark& w_lmk_k_1 = corres.ref_;

    // using the motion, put the lmk in the world frame at k
    const Landmark w_lmk_k = motion_model * w_lmk_k_1;
    // using camera pose, put the lmk in the camera frame at k
    const Landmark c_lmk_c = T_world_k.inverse() * w_lmk_k;

    try {
      double repr_error = camera.reprojectionError(c_lmk_c, kp_k).squaredNorm();

      total_repr_error.Add(repr_error);
      if (repr_error < reprojection_error) {
        inlier_tracklets.push_back(corres.tracklet_id_);
        inlier_repr_error.Add(repr_error);
      }
    } catch (const gtsam::CheiralityException&) {
    }
  }

  TrackletIds outliers;
  determineOutlierIds(inlier_tracklets, tracklets, outliers);

  VLOG(20) << "(Object) motion model inliers/total(error) "
           << inlier_tracklets.size() << "(" << inlier_repr_error.Mean()
           << ") / " << tracklets.size() << "(" << total_repr_error.Mean()
           << ")";

  result.best_result = motion_model;
  result.inliers = inlier_tracklets;
  result.outliers = outliers;
  result.status = TrackingStatus::VALID;
  return result;
}

// void ObjectMotionSovler::refineLocalObjectMotionEstimate(
//     Pose3SolverResult& solver_result,
//     Frame::Ptr frame_k_1,
//     Frame::Ptr frame_k,
//     ObjectId object_id,
//     const RefinementSolver& solver) const
// {
//     CHECK(solver_result.status == TrackingStatus::VALID);

//     gtsam::NonlinearFactorGraph graph;
//     gtsam::Values values;

//     // noise models are chosen arbitrarily :)
//     gtsam::SharedNoiseModel landmark_motion_noise =
//         gtsam::noiseModel::Isotropic::Sigma(3u, 0.001);

//     gtsam::SharedNoiseModel projection_noise =
//         gtsam::noiseModel::Isotropic::Sigma(2u, 2);

//     static constexpr auto k_huber_value = 0.0001;

//     //make robust (I mean, why not?)
//     landmark_motion_noise = gtsam::noiseModel::Robust::Create(
//             gtsam::noiseModel::mEstimator::Huber::Create(k_huber_value),
//             landmark_motion_noise);
//     projection_noise = gtsam::noiseModel::Robust::Create(
//             gtsam::noiseModel::mEstimator::Huber::Create(k_huber_value),
//             projection_noise);

//     const gtsam::Key pose_k_1_key =
//     CameraPoseSymbol(frame_k_1->getFrameId()); const gtsam::Key pose_k_key =
//     CameraPoseSymbol(frame_k->getFrameId()); const gtsam::Key
//     object_motion_key = ObjectMotionSymbol(object_id, frame_k->getFrameId());

//     values.insert(pose_k_1_key, frame_k_1->getPose());
//     values.insert(pose_k_key, frame_k->getPose());
//     values.insert(object_motion_key, solver_result.best_result);

//     auto gtsam_calibration = boost::make_shared<Camera::CalibrationType>(
//         frame_k_1->getFrameCamera().calibration());

//     auto pose_prior = gtsam::noiseModel::Isotropic::Sigma(6u, 0.00001);
//     graph.addPrior(pose_k_1_key, frame_k_1->getPose(), pose_prior);
//     graph.addPrior(pose_k_key, frame_k->getPose(), pose_prior);

//     utils::TimingStatsCollector timer("motion_solver.object_nlo_refinement");
//     //TODO: some might be marked outliers after update depth
//     for(TrackletId tracklet_id : solver_result.inliers) {

//         Feature::Ptr feature_k_1 = frame_k_1->at(tracklet_id);
//         Feature::Ptr feature_k = frame_k->at(tracklet_id);

//         if(!feature_k_1->usable() || !feature_k->usable()) { continue; }

//         CHECK_NOTNULL(feature_k_1);
//         CHECK_NOTNULL(feature_k);

//         CHECK(feature_k_1->hasDepth());
//         CHECK(feature_k->hasDepth());

//         const Keypoint kp_k_1 = feature_k_1->keypoint_;
//         const Keypoint kp_k = feature_k->keypoint_;

//         const gtsam::Point3 lmk_k_1_world =
//         frame_k_1->backProjectToWorld(tracklet_id); const gtsam::Point3
//         lmk_k_world = frame_k->backProjectToWorld(tracklet_id);

//         const gtsam::Point3 lmk_k_1_local =
//         frame_k_1->backProjectToCamera(tracklet_id); const gtsam::Point3
//         lmk_k_local = frame_k->backProjectToCamera(tracklet_id);

//         const gtsam::Key lmk_k_1_key =
//         DynamicLandmarkSymbol(frame_k_1->getFrameId(), tracklet_id); const
//         gtsam::Key lmk_k_key = DynamicLandmarkSymbol(frame_k->getFrameId(),
//         tracklet_id);

//         //add initial for points
//         values.insert(lmk_k_1_key, lmk_k_1_world);
//         values.insert(lmk_k_key, lmk_k_world);

//         if(solver == RefinementSolver::PointError) {
//             graph.emplace_shared<PoseToPointFactor>(
//                 pose_k_1_key, //pose key at previous frames
//                 lmk_k_1_key,
//                 lmk_k_1_local,
//                 projection_noise
//             );

//             graph.emplace_shared<PoseToPointFactor>(
//                     pose_k_key, //pose key at current frames
//                     lmk_k_key,
//                     lmk_k_local,
//                     projection_noise
//             );
//         }
//         else if(solver == RefinementSolver::ProjectionError) {
//             graph.emplace_shared<GenericProjectionFactor>(
//                     kp_k_1,
//                     projection_noise,
//                     pose_k_1_key,
//                     lmk_k_1_key,
//                     gtsam_calibration,
//                     false, false
//             );

//             graph.emplace_shared<GenericProjectionFactor>(
//                     kp_k,
//                     projection_noise,
//                     pose_k_key,
//                     lmk_k_key,
//                     gtsam_calibration,
//                     false, false
//             );
//         }

//         graph.emplace_shared<LandmarkMotionTernaryFactor>(
//             lmk_k_1_key,
//             lmk_k_key,
//             object_motion_key,
//             landmark_motion_noise
//         );

//     }

//     double error_before = graph.error(values);
//     // std::vector<double> post_errors;
//     // std::set<TrackletId> outlier_tracks;

//     gtsam::NonlinearFactorGraph mutable_graph = graph;
//     gtsam::Values optimised_values = values;

//     utils::StatsCollector("motion_solver.object_nlo_refinement_num_vars_all").AddSample(optimised_values.size());

//     gtsam::LevenbergMarquardtParams opt_params;
//     if(VLOG_IS_ON(200))
//         opt_params.verbosity =
//         gtsam::NonlinearOptimizerParams::Verbosity::ERROR;

//     optimised_values = gtsam::LevenbergMarquardtOptimizer(mutable_graph,
//     optimised_values, opt_params).optimize(); double error_after =
//     mutable_graph.error(optimised_values);
//     // post_errors.push_back(error_after);

//     // gtsam::FactorIndices outlier_factors =
//     factor_graph_tools::determineFactorOutliers<LandmarkMotionTernaryFactor>(
//     //     mutable_graph,
//     //     optimised_values
//     // );

//     // //if we have outliers, enter iteration loop
//     // if(outlier_factors.size() > 0u) {
//     //     for(size_t itr = 0; itr < 4; itr++) {

//     //         //currently removing factors from graph makes them nullptr
//     //         gtsam::NonlinearFactorGraph mutable_graph_with_null =
//     mutable_graph;
//     //         for(auto outlier_idx : outlier_factors) {
//     //             auto factor = mutable_graph_with_null.at(outlier_idx);
//     //             DynamicPointSymbol point_symbol = factor->keys()[0];
//     //             outlier_tracks.insert(point_symbol.trackletId());
//     //             mutable_graph_with_null.remove(outlier_idx);
//     //         }
//     //         //now iterate over graph and add factors that are not null to
//     ensure all factors are ok
//     //         mutable_graph.resize(0);
//     //         for (size_t i = 0; i < mutable_graph_with_null.size(); i++) {

//     //             auto factor = mutable_graph_with_null.at(i);
//     //             if(factor) {
//     //                 mutable_graph.add(factor);
//     //             }
//     //         }
//     //         // LOG(INFO) << "Removed " << outlier_factors.size() << "
//     factors on iteration: " << itr;

//     //         values.insert(object_motion_key, solver_result.best_result);
//     //         //do we use values or optimised values here?
//     //         optimised_values =
//     gtsam::LevenbergMarquardtOptimizer(mutable_graph, optimised_values,
//     opt_params).optimize();
//     //         error_after = mutable_graph.error(optimised_values);
//     //         post_errors.push_back(error_after);

//     //         outlier_factors =
//     factor_graph_tools::determineFactorOutliers<LandmarkMotionTernaryFactor>(
//     //             mutable_graph,
//     //             optimised_values
//     //         );

//     //         if(outlier_factors.size() == 0) {
//     //             break;
//     //         }
//     //     }
//     // }

//     // size_t initial_size = graph.size();
//     // size_t inlier_size = mutable_graph.size();
//     // error_after = mutable_graph.error(optimised_values);
//     LOG(INFO) << "Object Motion refinement - error before: "
//         << error_before << " error after: " << error_after;
//     //     << " with initial size " << initial_size << " inlier size " <<
//     inlier_size;

//     //recover values!
//     solver_result.best_result =
//     optimised_values.at<gtsam::Pose3>(object_motion_key);

//     //for each outlier edge, update the set of inliers
//     // for(const auto tracklet_id : outlier_tracks) {
//     //     frame_k->at(tracklet_id)->inlier_ = false;

//     // }

// }

}  // namespace dyno
````

## File: frontend/vision/ObjectTracker.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include "dynosam/frontend/vision/ObjectTracker.hpp"

#include "dynosam/frontend/vision/VisionTools.hpp"

namespace dyno {

cv::Mat ByteObjectTracker::track(const cv::Mat& masks, FrameId frame_id) {
  const ObjectIds instance_labels = vision_tools::getObjectLabels(masks);
  std::vector<byte_track::DetectionBase::Ptr> input_detections;
  for (auto instance_label : instance_labels) {
    cv::Rect bounding_box;
    if (vision_tools::findObjectBoundingBox(masks, instance_label,
                                            bounding_box)) {
      input_detections.push_back(
          std::make_shared<byte_track::Detection>(bounding_box, 1.0));
    } else {
      VLOG(30) << "Could not find bb for instance label " << instance_label
               << " at frame " << frame_id << " and therefore could not track!";
    }
  }

  auto object_tracks = impl_tracker_.update(input_detections, frame_id);
  CHECK_EQ(object_tracks.size(), input_detections.size());

  ObjectIds old_labels = instance_labels;
  ObjectIds new_labels;  // the new tracked labels from the EKF
  for (const auto& track : object_tracks) {
    new_labels.push_back(track->get_track_id());
  }

  // update tracking labels
  cv::Mat updated_masks;
  vision_tools::relabelMasks(masks, updated_masks, old_labels, new_labels);
  return updated_masks;
}

}  // namespace dyno
````

## File: frontend/vision/ORBextractor.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

/**
 * This file is part of VDO-SLAM.
 *
 * Copyright (C) 2019-2020 Jun Zhang <jun doc zhang2 at anu dot edu doc au> (The Australian National University)
 * For more information see <https://github.com/halajun/VDO_SLAM>
 *
 **/
/**
 * Software License Agreement (BSD License)
 *
 *  Copyright (c) 2009, Willow Garage, Inc.
 *  All rights reserved.
 *
 *  Redistribution and use in source and binary forms, with or without
 *  modification, are permitted provided that the following conditions
 *  are met:
 *
 *   * Redistributions of source code must retain the above copyright
 *     notice, this list of conditions and the following disclaimer.
 *   * Redistributions in binary form must reproduce the above
 *     copyright notice, this list of conditions and the following
 *     disclaimer in the documentation and/or other materials provided
 *     with the distribution.
 *   * Neither the name of the Willow Garage nor the names of its
 *     contributors may be used to endorse or promote products derived
 *     from this software without specific prior written permission.
 *
 *  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 *  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 *  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
 *  FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
 *  COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
 *  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 *  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 *  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 *  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
 *  ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 *  POSSIBILITY OF SUCH DAMAGE.
 *
 */

#include <opencv4/opencv2/core/core.hpp>
#include <opencv4/opencv2/highgui/highgui.hpp>
#include <opencv4/opencv2/features2d/features2d.hpp>
#include <opencv4/opencv2/imgproc/imgproc.hpp>
#include <vector>
#include <time.h>
#include <iostream>

#include "dynosam/frontend/vision/ORBextractor.hpp"

using namespace cv;
using namespace std;

namespace dyno
{
const int PATCH_SIZE = 31;
const int HALF_PATCH_SIZE = 15;
const int EDGE_THRESHOLD = 19;

static float IC_Angle(const Mat& image, Point2f pt, const vector<int>& u_max)
{
  int m_01 = 0, m_10 = 0;

  const uchar* center = &image.at<uchar>(cvRound(pt.y), cvRound(pt.x));

  // Treat the center line differently, v=0
  for (int u = -HALF_PATCH_SIZE; u <= HALF_PATCH_SIZE; ++u)
    m_10 += u * center[u];

  // Go line by line in the circuI853lar patch
  int step = (int)image.step1();
  for (int v = 1; v <= HALF_PATCH_SIZE; ++v)
  {
    // Proceed over the two lines
    int v_sum = 0;
    int d = u_max[v];
    for (int u = -d; u <= d; ++u)
    {
      int val_plus = center[u + v * step], val_minus = center[u - v * step];
      v_sum += (val_plus - val_minus);
      m_10 += u * (val_plus + val_minus);
    }
    m_01 += v * v_sum;
  }

  return fastAtan2((float)m_01, (float)m_10);
}

const float factorPI = (float)(CV_PI / 180.f);
static void computeOrbDescriptor(const KeyPoint& kpt, const Mat& img, const Point* pattern, uchar* desc)
{
  float angle = (float)kpt.angle * factorPI;
  float a = (float)cos(angle), b = (float)sin(angle);

  const uchar* center = &img.at<uchar>(cvRound(kpt.pt.y), cvRound(kpt.pt.x));
  const int step = (int)img.step;

#define GET_VALUE(idx)                                                                                                 \
  center[cvRound(pattern[idx].x * b + pattern[idx].y * a) * step + cvRound(pattern[idx].x * a - pattern[idx].y * b)]

  for (int i = 0; i < 32; ++i, pattern += 16)
  {
    int t0, t1, val;
    t0 = GET_VALUE(0);
    t1 = GET_VALUE(1);
    val = t0 < t1;
    t0 = GET_VALUE(2);
    t1 = GET_VALUE(3);
    val |= (t0 < t1) << 1;
    t0 = GET_VALUE(4);
    t1 = GET_VALUE(5);
    val |= (t0 < t1) << 2;
    t0 = GET_VALUE(6);
    t1 = GET_VALUE(7);
    val |= (t0 < t1) << 3;
    t0 = GET_VALUE(8);
    t1 = GET_VALUE(9);
    val |= (t0 < t1) << 4;
    t0 = GET_VALUE(10);
    t1 = GET_VALUE(11);
    val |= (t0 < t1) << 5;
    t0 = GET_VALUE(12);
    t1 = GET_VALUE(13);
    val |= (t0 < t1) << 6;
    t0 = GET_VALUE(14);
    t1 = GET_VALUE(15);
    val |= (t0 < t1) << 7;

    desc[i] = (uchar)val;
  }

#undef GET_VALUE
}

static int bit_pattern_31_[256 * 4] = {
  8,   -3,  9,   5 /*mean (0), correlation (0)*/,
  4,   2,   7,   -12 /*mean (1.12461e-05), correlation (0.0437584)*/,
  -11, 9,   -8,  2 /*mean (3.37382e-05), correlation (0.0617409)*/,
  7,   -12, 12,  -13 /*mean (5.62303e-05), correlation (0.0636977)*/,
  2,   -13, 2,   12 /*mean (0.000134953), correlation (0.085099)*/,
  1,   -7,  1,   6 /*mean (0.000528565), correlation (0.0857175)*/,
  -2,  -10, -2,  -4 /*mean (0.0188821), correlation (0.0985774)*/,
  -13, -13, -11, -8 /*mean (0.0363135), correlation (0.0899616)*/,
  -13, -3,  -12, -9 /*mean (0.121806), correlation (0.099849)*/,
  10,  4,   11,  9 /*mean (0.122065), correlation (0.093285)*/,
  -13, -8,  -8,  -9 /*mean (0.162787), correlation (0.0942748)*/,
  -11, 7,   -9,  12 /*mean (0.21561), correlation (0.0974438)*/,
  7,   7,   12,  6 /*mean (0.160583), correlation (0.130064)*/,
  -4,  -5,  -3,  0 /*mean (0.228171), correlation (0.132998)*/,
  -13, 2,   -12, -3 /*mean (0.00997526), correlation (0.145926)*/,
  -9,  0,   -7,  5 /*mean (0.198234), correlation (0.143636)*/,
  12,  -6,  12,  -1 /*mean (0.0676226), correlation (0.16689)*/,
  -3,  6,   -2,  12 /*mean (0.166847), correlation (0.171682)*/,
  -6,  -13, -4,  -8 /*mean (0.101215), correlation (0.179716)*/,
  11,  -13, 12,  -8 /*mean (0.200641), correlation (0.192279)*/,
  4,   7,   5,   1 /*mean (0.205106), correlation (0.186848)*/,
  5,   -3,  10,  -3 /*mean (0.234908), correlation (0.192319)*/,
  3,   -7,  6,   12 /*mean (0.0709964), correlation (0.210872)*/,
  -8,  -7,  -6,  -2 /*mean (0.0939834), correlation (0.212589)*/,
  -2,  11,  -1,  -10 /*mean (0.127778), correlation (0.20866)*/,
  -13, 12,  -8,  10 /*mean (0.14783), correlation (0.206356)*/,
  -7,  3,   -5,  -3 /*mean (0.182141), correlation (0.198942)*/,
  -4,  2,   -3,  7 /*mean (0.188237), correlation (0.21384)*/,
  -10, -12, -6,  11 /*mean (0.14865), correlation (0.23571)*/,
  5,   -12, 6,   -7 /*mean (0.222312), correlation (0.23324)*/,
  5,   -6,  7,   -1 /*mean (0.229082), correlation (0.23389)*/,
  1,   0,   4,   -5 /*mean (0.241577), correlation (0.215286)*/,
  9,   11,  11,  -13 /*mean (0.00338507), correlation (0.251373)*/,
  4,   7,   4,   12 /*mean (0.131005), correlation (0.257622)*/,
  2,   -1,  4,   4 /*mean (0.152755), correlation (0.255205)*/,
  -4,  -12, -2,  7 /*mean (0.182771), correlation (0.244867)*/,
  -8,  -5,  -7,  -10 /*mean (0.186898), correlation (0.23901)*/,
  4,   11,  9,   12 /*mean (0.226226), correlation (0.258255)*/,
  0,   -8,  1,   -13 /*mean (0.0897886), correlation (0.274827)*/,
  -13, -2,  -8,  2 /*mean (0.148774), correlation (0.28065)*/,
  -3,  -2,  -2,  3 /*mean (0.153048), correlation (0.283063)*/,
  -6,  9,   -4,  -9 /*mean (0.169523), correlation (0.278248)*/,
  8,   12,  10,  7 /*mean (0.225337), correlation (0.282851)*/,
  0,   9,   1,   3 /*mean (0.226687), correlation (0.278734)*/,
  7,   -5,  11,  -10 /*mean (0.00693882), correlation (0.305161)*/,
  -13, -6,  -11, 0 /*mean (0.0227283), correlation (0.300181)*/,
  10,  7,   12,  1 /*mean (0.125517), correlation (0.31089)*/,
  -6,  -3,  -6,  12 /*mean (0.131748), correlation (0.312779)*/,
  10,  -9,  12,  -4 /*mean (0.144827), correlation (0.292797)*/,
  -13, 8,   -8,  -12 /*mean (0.149202), correlation (0.308918)*/,
  -13, 0,   -8,  -4 /*mean (0.160909), correlation (0.310013)*/,
  3,   3,   7,   8 /*mean (0.177755), correlation (0.309394)*/,
  5,   7,   10,  -7 /*mean (0.212337), correlation (0.310315)*/,
  -1,  7,   1,   -12 /*mean (0.214429), correlation (0.311933)*/,
  3,   -10, 5,   6 /*mean (0.235807), correlation (0.313104)*/,
  2,   -4,  3,   -10 /*mean (0.00494827), correlation (0.344948)*/,
  -13, 0,   -13, 5 /*mean (0.0549145), correlation (0.344675)*/,
  -13, -7,  -12, 12 /*mean (0.103385), correlation (0.342715)*/,
  -13, 3,   -11, 8 /*mean (0.134222), correlation (0.322922)*/,
  -7,  12,  -4,  7 /*mean (0.153284), correlation (0.337061)*/,
  6,   -10, 12,  8 /*mean (0.154881), correlation (0.329257)*/,
  -9,  -1,  -7,  -6 /*mean (0.200967), correlation (0.33312)*/,
  -2,  -5,  0,   12 /*mean (0.201518), correlation (0.340635)*/,
  -12, 5,   -7,  5 /*mean (0.207805), correlation (0.335631)*/,
  3,   -10, 8,   -13 /*mean (0.224438), correlation (0.34504)*/,
  -7,  -7,  -4,  5 /*mean (0.239361), correlation (0.338053)*/,
  -3,  -2,  -1,  -7 /*mean (0.240744), correlation (0.344322)*/,
  2,   9,   5,   -11 /*mean (0.242949), correlation (0.34145)*/,
  -11, -13, -5,  -13 /*mean (0.244028), correlation (0.336861)*/,
  -1,  6,   0,   -1 /*mean (0.247571), correlation (0.343684)*/,
  5,   -3,  5,   2 /*mean (0.000697256), correlation (0.357265)*/,
  -4,  -13, -4,  12 /*mean (0.00213675), correlation (0.373827)*/,
  -9,  -6,  -9,  6 /*mean (0.0126856), correlation (0.373938)*/,
  -12, -10, -8,  -4 /*mean (0.0152497), correlation (0.364237)*/,
  10,  2,   12,  -3 /*mean (0.0299933), correlation (0.345292)*/,
  7,   12,  12,  12 /*mean (0.0307242), correlation (0.366299)*/,
  -7,  -13, -6,  5 /*mean (0.0534975), correlation (0.368357)*/,
  -4,  9,   -3,  4 /*mean (0.099865), correlation (0.372276)*/,
  7,   -1,  12,  2 /*mean (0.117083), correlation (0.364529)*/,
  -7,  6,   -5,  1 /*mean (0.126125), correlation (0.369606)*/,
  -13, 11,  -12, 5 /*mean (0.130364), correlation (0.358502)*/,
  -3,  7,   -2,  -6 /*mean (0.131691), correlation (0.375531)*/,
  7,   -8,  12,  -7 /*mean (0.160166), correlation (0.379508)*/,
  -13, -7,  -11, -12 /*mean (0.167848), correlation (0.353343)*/,
  1,   -3,  12,  12 /*mean (0.183378), correlation (0.371916)*/,
  2,   -6,  3,   0 /*mean (0.228711), correlation (0.371761)*/,
  -4,  3,   -2,  -13 /*mean (0.247211), correlation (0.364063)*/,
  -1,  -13, 1,   9 /*mean (0.249325), correlation (0.378139)*/,
  7,   1,   8,   -6 /*mean (0.000652272), correlation (0.411682)*/,
  1,   -1,  3,   12 /*mean (0.00248538), correlation (0.392988)*/,
  9,   1,   12,  6 /*mean (0.0206815), correlation (0.386106)*/,
  -1,  -9,  -1,  3 /*mean (0.0364485), correlation (0.410752)*/,
  -13, -13, -10, 5 /*mean (0.0376068), correlation (0.398374)*/,
  7,   7,   10,  12 /*mean (0.0424202), correlation (0.405663)*/,
  12,  -5,  12,  9 /*mean (0.0942645), correlation (0.410422)*/,
  6,   3,   7,   11 /*mean (0.1074), correlation (0.413224)*/,
  5,   -13, 6,   10 /*mean (0.109256), correlation (0.408646)*/,
  2,   -12, 2,   3 /*mean (0.131691), correlation (0.416076)*/,
  3,   8,   4,   -6 /*mean (0.165081), correlation (0.417569)*/,
  2,   6,   12,  -13 /*mean (0.171874), correlation (0.408471)*/,
  9,   -12, 10,  3 /*mean (0.175146), correlation (0.41296)*/,
  -8,  4,   -7,  9 /*mean (0.183682), correlation (0.402956)*/,
  -11, 12,  -4,  -6 /*mean (0.184672), correlation (0.416125)*/,
  1,   12,  2,   -8 /*mean (0.191487), correlation (0.386696)*/,
  6,   -9,  7,   -4 /*mean (0.192668), correlation (0.394771)*/,
  2,   3,   3,   -2 /*mean (0.200157), correlation (0.408303)*/,
  6,   3,   11,  0 /*mean (0.204588), correlation (0.411762)*/,
  3,   -3,  8,   -8 /*mean (0.205904), correlation (0.416294)*/,
  7,   8,   9,   3 /*mean (0.213237), correlation (0.409306)*/,
  -11, -5,  -6,  -4 /*mean (0.243444), correlation (0.395069)*/,
  -10, 11,  -5,  10 /*mean (0.247672), correlation (0.413392)*/,
  -5,  -8,  -3,  12 /*mean (0.24774), correlation (0.411416)*/,
  -10, 5,   -9,  0 /*mean (0.00213675), correlation (0.454003)*/,
  8,   -1,  12,  -6 /*mean (0.0293635), correlation (0.455368)*/,
  4,   -6,  6,   -11 /*mean (0.0404971), correlation (0.457393)*/,
  -10, 12,  -8,  7 /*mean (0.0481107), correlation (0.448364)*/,
  4,   -2,  6,   7 /*mean (0.050641), correlation (0.455019)*/,
  -2,  0,   -2,  12 /*mean (0.0525978), correlation (0.44338)*/,
  -5,  -8,  -5,  2 /*mean (0.0629667), correlation (0.457096)*/,
  7,   -6,  10,  12 /*mean (0.0653846), correlation (0.445623)*/,
  -9,  -13, -8,  -8 /*mean (0.0858749), correlation (0.449789)*/,
  -5,  -13, -5,  -2 /*mean (0.122402), correlation (0.450201)*/,
  8,   -8,  9,   -13 /*mean (0.125416), correlation (0.453224)*/,
  -9,  -11, -9,  0 /*mean (0.130128), correlation (0.458724)*/,
  1,   -8,  1,   -2 /*mean (0.132467), correlation (0.440133)*/,
  7,   -4,  9,   1 /*mean (0.132692), correlation (0.454)*/,
  -2,  1,   -1,  -4 /*mean (0.135695), correlation (0.455739)*/,
  11,  -6,  12,  -11 /*mean (0.142904), correlation (0.446114)*/,
  -12, -9,  -6,  4 /*mean (0.146165), correlation (0.451473)*/,
  3,   7,   7,   12 /*mean (0.147627), correlation (0.456643)*/,
  5,   5,   10,  8 /*mean (0.152901), correlation (0.455036)*/,
  0,   -4,  2,   8 /*mean (0.167083), correlation (0.459315)*/,
  -9,  12,  -5,  -13 /*mean (0.173234), correlation (0.454706)*/,
  0,   7,   2,   12 /*mean (0.18312), correlation (0.433855)*/,
  -1,  2,   1,   7 /*mean (0.185504), correlation (0.443838)*/,
  5,   11,  7,   -9 /*mean (0.185706), correlation (0.451123)*/,
  3,   5,   6,   -8 /*mean (0.188968), correlation (0.455808)*/,
  -13, -4,  -8,  9 /*mean (0.191667), correlation (0.459128)*/,
  -5,  9,   -3,  -3 /*mean (0.193196), correlation (0.458364)*/,
  -4,  -7,  -3,  -12 /*mean (0.196536), correlation (0.455782)*/,
  6,   5,   8,   0 /*mean (0.1972), correlation (0.450481)*/,
  -7,  6,   -6,  12 /*mean (0.199438), correlation (0.458156)*/,
  -13, 6,   -5,  -2 /*mean (0.211224), correlation (0.449548)*/,
  1,   -10, 3,   10 /*mean (0.211718), correlation (0.440606)*/,
  4,   1,   8,   -4 /*mean (0.213034), correlation (0.443177)*/,
  -2,  -2,  2,   -13 /*mean (0.234334), correlation (0.455304)*/,
  2,   -12, 12,  12 /*mean (0.235684), correlation (0.443436)*/,
  -2,  -13, 0,   -6 /*mean (0.237674), correlation (0.452525)*/,
  4,   1,   9,   3 /*mean (0.23962), correlation (0.444824)*/,
  -6,  -10, -3,  -5 /*mean (0.248459), correlation (0.439621)*/,
  -3,  -13, -1,  1 /*mean (0.249505), correlation (0.456666)*/,
  7,   5,   12,  -11 /*mean (0.00119208), correlation (0.495466)*/,
  4,   -2,  5,   -7 /*mean (0.00372245), correlation (0.484214)*/,
  -13, 9,   -9,  -5 /*mean (0.00741116), correlation (0.499854)*/,
  7,   1,   8,   6 /*mean (0.0208952), correlation (0.499773)*/,
  7,   -8,  7,   6 /*mean (0.0220085), correlation (0.501609)*/,
  -7,  -4,  -7,  1 /*mean (0.0233806), correlation (0.496568)*/,
  -8,  11,  -7,  -8 /*mean (0.0236505), correlation (0.489719)*/,
  -13, 6,   -12, -8 /*mean (0.0268781), correlation (0.503487)*/,
  2,   4,   3,   9 /*mean (0.0323324), correlation (0.501938)*/,
  10,  -5,  12,  3 /*mean (0.0399235), correlation (0.494029)*/,
  -6,  -5,  -6,  7 /*mean (0.0420153), correlation (0.486579)*/,
  8,   -3,  9,   -8 /*mean (0.0548021), correlation (0.484237)*/,
  2,   -12, 2,   8 /*mean (0.0616622), correlation (0.496642)*/,
  -11, -2,  -10, 3 /*mean (0.0627755), correlation (0.498563)*/,
  -12, -13, -7,  -9 /*mean (0.0829622), correlation (0.495491)*/,
  -11, 0,   -10, -5 /*mean (0.0843342), correlation (0.487146)*/,
  5,   -3,  11,  8 /*mean (0.0929937), correlation (0.502315)*/,
  -2,  -13, -1,  12 /*mean (0.113327), correlation (0.48941)*/,
  -1,  -8,  0,   9 /*mean (0.132119), correlation (0.467268)*/,
  -13, -11, -12, -5 /*mean (0.136269), correlation (0.498771)*/,
  -10, -2,  -10, 11 /*mean (0.142173), correlation (0.498714)*/,
  -3,  9,   -2,  -13 /*mean (0.144141), correlation (0.491973)*/,
  2,   -3,  3,   2 /*mean (0.14892), correlation (0.500782)*/,
  -9,  -13, -4,  0 /*mean (0.150371), correlation (0.498211)*/,
  -4,  6,   -3,  -10 /*mean (0.152159), correlation (0.495547)*/,
  -4,  12,  -2,  -7 /*mean (0.156152), correlation (0.496925)*/,
  -6,  -11, -4,  9 /*mean (0.15749), correlation (0.499222)*/,
  6,   -3,  6,   11 /*mean (0.159211), correlation (0.503821)*/,
  -13, 11,  -5,  5 /*mean (0.162427), correlation (0.501907)*/,
  11,  11,  12,  6 /*mean (0.16652), correlation (0.497632)*/,
  7,   -5,  12,  -2 /*mean (0.169141), correlation (0.484474)*/,
  -1,  12,  0,   7 /*mean (0.169456), correlation (0.495339)*/,
  -4,  -8,  -3,  -2 /*mean (0.171457), correlation (0.487251)*/,
  -7,  1,   -6,  7 /*mean (0.175), correlation (0.500024)*/,
  -13, -12, -8,  -13 /*mean (0.175866), correlation (0.497523)*/,
  -7,  -2,  -6,  -8 /*mean (0.178273), correlation (0.501854)*/,
  -8,  5,   -6,  -9 /*mean (0.181107), correlation (0.494888)*/,
  -5,  -1,  -4,  5 /*mean (0.190227), correlation (0.482557)*/,
  -13, 7,   -8,  10 /*mean (0.196739), correlation (0.496503)*/,
  1,   5,   5,   -13 /*mean (0.19973), correlation (0.499759)*/,
  1,   0,   10,  -13 /*mean (0.204465), correlation (0.49873)*/,
  9,   12,  10,  -1 /*mean (0.209334), correlation (0.49063)*/,
  5,   -8,  10,  -9 /*mean (0.211134), correlation (0.503011)*/,
  -1,  11,  1,   -13 /*mean (0.212), correlation (0.499414)*/,
  -9,  -3,  -6,  2 /*mean (0.212168), correlation (0.480739)*/,
  -1,  -10, 1,   12 /*mean (0.212731), correlation (0.502523)*/,
  -13, 1,   -8,  -10 /*mean (0.21327), correlation (0.489786)*/,
  8,   -11, 10,  -6 /*mean (0.214159), correlation (0.488246)*/,
  2,   -13, 3,   -6 /*mean (0.216993), correlation (0.50287)*/,
  7,   -13, 12,  -9 /*mean (0.223639), correlation (0.470502)*/,
  -10, -10, -5,  -7 /*mean (0.224089), correlation (0.500852)*/,
  -10, -8,  -8,  -13 /*mean (0.228666), correlation (0.502629)*/,
  4,   -6,  8,   5 /*mean (0.22906), correlation (0.498305)*/,
  3,   12,  8,   -13 /*mean (0.233378), correlation (0.503825)*/,
  -4,  2,   -3,  -3 /*mean (0.234323), correlation (0.476692)*/,
  5,   -13, 10,  -12 /*mean (0.236392), correlation (0.475462)*/,
  4,   -13, 5,   -1 /*mean (0.236842), correlation (0.504132)*/,
  -9,  9,   -4,  3 /*mean (0.236977), correlation (0.497739)*/,
  0,   3,   3,   -9 /*mean (0.24314), correlation (0.499398)*/,
  -12, 1,   -6,  1 /*mean (0.243297), correlation (0.489447)*/,
  3,   2,   4,   -8 /*mean (0.00155196), correlation (0.553496)*/,
  -10, -10, -10, 9 /*mean (0.00239541), correlation (0.54297)*/,
  8,   -13, 12,  12 /*mean (0.0034413), correlation (0.544361)*/,
  -8,  -12, -6,  -5 /*mean (0.003565), correlation (0.551225)*/,
  2,   2,   3,   7 /*mean (0.00835583), correlation (0.55285)*/,
  10,  6,   11,  -8 /*mean (0.00885065), correlation (0.540913)*/,
  6,   8,   8,   -12 /*mean (0.0101552), correlation (0.551085)*/,
  -7,  10,  -6,  5 /*mean (0.0102227), correlation (0.533635)*/,
  -3,  -9,  -3,  9 /*mean (0.0110211), correlation (0.543121)*/,
  -1,  -13, -1,  5 /*mean (0.0113473), correlation (0.550173)*/,
  -3,  -7,  -3,  4 /*mean (0.0140913), correlation (0.554774)*/,
  -8,  -2,  -8,  3 /*mean (0.017049), correlation (0.55461)*/,
  4,   2,   12,  12 /*mean (0.01778), correlation (0.546921)*/,
  2,   -5,  3,   11 /*mean (0.0224022), correlation (0.549667)*/,
  6,   -9,  11,  -13 /*mean (0.029161), correlation (0.546295)*/,
  3,   -1,  7,   12 /*mean (0.0303081), correlation (0.548599)*/,
  11,  -1,  12,  4 /*mean (0.0355151), correlation (0.523943)*/,
  -3,  0,   -3,  6 /*mean (0.0417904), correlation (0.543395)*/,
  4,   -11, 4,   12 /*mean (0.0487292), correlation (0.542818)*/,
  2,   -4,  2,   1 /*mean (0.0575124), correlation (0.554888)*/,
  -10, -6,  -8,  1 /*mean (0.0594242), correlation (0.544026)*/,
  -13, 7,   -11, 1 /*mean (0.0597391), correlation (0.550524)*/,
  -13, 12,  -11, -13 /*mean (0.0608974), correlation (0.55383)*/,
  6,   0,   11,  -13 /*mean (0.065126), correlation (0.552006)*/,
  0,   -1,  1,   4 /*mean (0.074224), correlation (0.546372)*/,
  -13, 3,   -9,  -2 /*mean (0.0808592), correlation (0.554875)*/,
  -9,  8,   -6,  -3 /*mean (0.0883378), correlation (0.551178)*/,
  -13, -6,  -8,  -2 /*mean (0.0901035), correlation (0.548446)*/,
  5,   -9,  8,   10 /*mean (0.0949843), correlation (0.554694)*/,
  2,   7,   3,   -9 /*mean (0.0994152), correlation (0.550979)*/,
  -1,  -6,  -1,  -1 /*mean (0.10045), correlation (0.552714)*/,
  9,   5,   11,  -2 /*mean (0.100686), correlation (0.552594)*/,
  11,  -3,  12,  -8 /*mean (0.101091), correlation (0.532394)*/,
  3,   0,   3,   5 /*mean (0.101147), correlation (0.525576)*/,
  -1,  4,   0,   10 /*mean (0.105263), correlation (0.531498)*/,
  3,   -6,  4,   5 /*mean (0.110785), correlation (0.540491)*/,
  -13, 0,   -10, 5 /*mean (0.112798), correlation (0.536582)*/,
  5,   8,   12,  11 /*mean (0.114181), correlation (0.555793)*/,
  8,   9,   9,   -6 /*mean (0.117431), correlation (0.553763)*/,
  7,   -4,  8,   -12 /*mean (0.118522), correlation (0.553452)*/,
  -10, 4,   -10, 9 /*mean (0.12094), correlation (0.554785)*/,
  7,   3,   12,  4 /*mean (0.122582), correlation (0.555825)*/,
  9,   -7,  10,  -2 /*mean (0.124978), correlation (0.549846)*/,
  7,   0,   12,  -2 /*mean (0.127002), correlation (0.537452)*/,
  -1,  -6,  0,   -11 /*mean (0.127148), correlation (0.547401)*/
};

ORBextractor::ORBextractor(int _nfeatures, float _scaleFactor, int _nlevels, int _iniThFAST, int _minThFAST)
  : nfeatures(_nfeatures), scaleFactor(_scaleFactor), nlevels(_nlevels), iniThFAST(_iniThFAST), minThFAST(_minThFAST)
{
  mvScaleFactor.resize(nlevels);
  mvLevelSigma2.resize(nlevels);
  mvScaleFactor[0] = 1.0f;
  mvLevelSigma2[0] = 1.0f;
  for (int i = 1; i < nlevels; i++)
  {
    mvScaleFactor[i] = mvScaleFactor[i - 1] * scaleFactor;
    mvLevelSigma2[i] = mvScaleFactor[i] * mvScaleFactor[i];
  }

  mvInvScaleFactor.resize(nlevels);
  mvInvLevelSigma2.resize(nlevels);
  for (int i = 0; i < nlevels; i++)
  {
    mvInvScaleFactor[i] = 1.0f / mvScaleFactor[i];
    mvInvLevelSigma2[i] = 1.0f / mvLevelSigma2[i];
  }

  mvImagePyramid.resize(nlevels);

  mnFeaturesPerLevel.resize(nlevels);
  float factor = 1.0f / scaleFactor;
  float nDesiredFeaturesPerScale = nfeatures * (1 - factor) / (1 - (float)pow((double)factor, (double)nlevels));

  int sumFeatures = 0;
  for (int level = 0; level < nlevels - 1; level++)
  {
    mnFeaturesPerLevel[level] = cvRound(nDesiredFeaturesPerScale);
    sumFeatures += mnFeaturesPerLevel[level];
    nDesiredFeaturesPerScale *= factor;
  }
  mnFeaturesPerLevel[nlevels - 1] = std::max(nfeatures - sumFeatures, 0);

  const int npoints = 512;
  const Point* pattern0 = (const Point*)bit_pattern_31_;
  std::copy(pattern0, pattern0 + npoints, std::back_inserter(pattern));

  // This is for orientation
  // pre-compute the end of a row in a circular patch
  umax.resize(HALF_PATCH_SIZE + 1);

  int v, v0, vmax = cvFloor(HALF_PATCH_SIZE * sqrt(2.f) / 2 + 1);
  int vmin = cvCeil(HALF_PATCH_SIZE * sqrt(2.f) / 2);
  const double hp2 = HALF_PATCH_SIZE * HALF_PATCH_SIZE;
  for (v = 0; v <= vmax; ++v)
    umax[v] = cvRound(sqrt(hp2 - v * v));

  // Make sure we are symmetric
  for (v = HALF_PATCH_SIZE, v0 = 0; v >= vmin; --v)
  {
    while (umax[v0] == umax[v0 + 1])
      ++v0;
    umax[v] = v0;
    ++v0;
  }
}

static void computeOrientation(const Mat& image, vector<KeyPoint>& keypoints, const vector<int>& umax)
{
  for (vector<KeyPoint>::iterator keypoint = keypoints.begin(), keypointEnd = keypoints.end(); keypoint != keypointEnd;
       ++keypoint)
  {
    keypoint->angle = IC_Angle(image, keypoint->pt, umax);
  }
}

void ExtractorNode::DivideNode(ExtractorNode& n1, ExtractorNode& n2, ExtractorNode& n3, ExtractorNode& n4)
{
  const int halfX = ceil(static_cast<float>(UR.x - UL.x) / 2);
  const int halfY = ceil(static_cast<float>(BR.y - UL.y) / 2);

  // Define boundaries of childs
  n1.UL = UL;
  n1.UR = cv::Point2i(UL.x + halfX, UL.y);
  n1.BL = cv::Point2i(UL.x, UL.y + halfY);
  n1.BR = cv::Point2i(UL.x + halfX, UL.y + halfY);
  n1.vKeys.reserve(vKeys.size());

  n2.UL = n1.UR;
  n2.UR = UR;
  n2.BL = n1.BR;
  n2.BR = cv::Point2i(UR.x, UL.y + halfY);
  n2.vKeys.reserve(vKeys.size());

  n3.UL = n1.BL;
  n3.UR = n1.BR;
  n3.BL = BL;
  n3.BR = cv::Point2i(n1.BR.x, BL.y);
  n3.vKeys.reserve(vKeys.size());

  n4.UL = n3.UR;
  n4.UR = n2.BR;
  n4.BL = n3.BR;
  n4.BR = BR;
  n4.vKeys.reserve(vKeys.size());

  // Associate points to childs
  for (size_t i = 0; i < vKeys.size(); i++)
  {
    const cv::KeyPoint& kp = vKeys[i];
    if (kp.pt.x < n1.UR.x)
    {
      if (kp.pt.y < n1.BR.y)
        n1.vKeys.push_back(kp);
      else
        n3.vKeys.push_back(kp);
    }
    else if (kp.pt.y < n1.BR.y)
      n2.vKeys.push_back(kp);
    else
      n4.vKeys.push_back(kp);
  }

  if (n1.vKeys.size() == 1)
    n1.bNoMore = true;
  if (n2.vKeys.size() == 1)
    n2.bNoMore = true;
  if (n3.vKeys.size() == 1)
    n3.bNoMore = true;
  if (n4.vKeys.size() == 1)
    n4.bNoMore = true;
}

vector<cv::KeyPoint> ORBextractor::DistributeOctTree(const vector<cv::KeyPoint>& vToDistributeKeys, const int& minX,
                                                     const int& maxX, const int& minY, const int& maxY, const int& N,
                                                     const int& level)
{
  // Compute how many initial nodes
  const int nIni = round(static_cast<float>(maxX - minX) / (maxY - minY));

  const float hX = static_cast<float>(maxX - minX) / nIni;

  list<ExtractorNode> lNodes;

  vector<ExtractorNode*> vpIniNodes;
  vpIniNodes.resize(nIni);

  for (int i = 0; i < nIni; i++)
  {
    ExtractorNode ni;
    ni.UL = cv::Point2i(hX * static_cast<float>(i), 0);
    ni.UR = cv::Point2i(hX * static_cast<float>(i + 1), 0);
    ni.BL = cv::Point2i(ni.UL.x, maxY - minY);
    ni.BR = cv::Point2i(ni.UR.x, maxY - minY);
    ni.vKeys.reserve(vToDistributeKeys.size());

    lNodes.push_back(ni);
    vpIniNodes[i] = &lNodes.back();
  }

  // Associate points to childs
  for (size_t i = 0; i < vToDistributeKeys.size(); i++)
  {
    const cv::KeyPoint& kp = vToDistributeKeys[i];
    vpIniNodes[kp.pt.x / hX]->vKeys.push_back(kp);
  }

  list<ExtractorNode>::iterator lit = lNodes.begin();

  while (lit != lNodes.end())
  {
    if (lit->vKeys.size() == 1)
    {
      lit->bNoMore = true;
      lit++;
    }
    else if (lit->vKeys.empty())
      lit = lNodes.erase(lit);
    else
      lit++;
  }

  bool bFinish = false;

  int iteration = 0;

  vector<pair<int, ExtractorNode*> > vSizeAndPointerToNode;
  vSizeAndPointerToNode.reserve(lNodes.size() * 4);

  while (!bFinish)
  {
    iteration++;

    int prevSize = lNodes.size();

    lit = lNodes.begin();

    int nToExpand = 0;

    vSizeAndPointerToNode.clear();

    while (lit != lNodes.end())
    {
      if (lit->bNoMore)
      {
        // If node only contains one point do not subdivide and continue
        lit++;
        continue;
      }
      else
      {
        // If more than one point, subdivide
        ExtractorNode n1, n2, n3, n4;
        lit->DivideNode(n1, n2, n3, n4);

        // Add childs if they contain points
        if (n1.vKeys.size() > 0)
        {
          lNodes.push_front(n1);
          if (n1.vKeys.size() > 1)
          {
            nToExpand++;
            vSizeAndPointerToNode.push_back(make_pair(n1.vKeys.size(), &lNodes.front()));
            lNodes.front().lit = lNodes.begin();
          }
        }
        if (n2.vKeys.size() > 0)
        {
          lNodes.push_front(n2);
          if (n2.vKeys.size() > 1)
          {
            nToExpand++;
            vSizeAndPointerToNode.push_back(make_pair(n2.vKeys.size(), &lNodes.front()));
            lNodes.front().lit = lNodes.begin();
          }
        }
        if (n3.vKeys.size() > 0)
        {
          lNodes.push_front(n3);
          if (n3.vKeys.size() > 1)
          {
            nToExpand++;
            vSizeAndPointerToNode.push_back(make_pair(n3.vKeys.size(), &lNodes.front()));
            lNodes.front().lit = lNodes.begin();
          }
        }
        if (n4.vKeys.size() > 0)
        {
          lNodes.push_front(n4);
          if (n4.vKeys.size() > 1)
          {
            nToExpand++;
            vSizeAndPointerToNode.push_back(make_pair(n4.vKeys.size(), &lNodes.front()));
            lNodes.front().lit = lNodes.begin();
          }
        }

        lit = lNodes.erase(lit);
        continue;
      }
    }

    // Finish if there are more nodes than required features
    // or all nodes contain just one point
    if ((int)lNodes.size() >= N || (int)lNodes.size() == prevSize)
    {
      bFinish = true;
    }
    else if (((int)lNodes.size() + nToExpand * 3) > N)
    {
      while (!bFinish)
      {
        prevSize = lNodes.size();

        vector<pair<int, ExtractorNode*> > vPrevSizeAndPointerToNode = vSizeAndPointerToNode;
        vSizeAndPointerToNode.clear();

        sort(vPrevSizeAndPointerToNode.begin(), vPrevSizeAndPointerToNode.end());
        for (int j = vPrevSizeAndPointerToNode.size() - 1; j >= 0; j--)
        {
          ExtractorNode n1, n2, n3, n4;
          vPrevSizeAndPointerToNode[j].second->DivideNode(n1, n2, n3, n4);

          // Add childs if they contain points
          if (n1.vKeys.size() > 0)
          {
            lNodes.push_front(n1);
            if (n1.vKeys.size() > 1)
            {
              vSizeAndPointerToNode.push_back(make_pair(n1.vKeys.size(), &lNodes.front()));
              lNodes.front().lit = lNodes.begin();
            }
          }
          if (n2.vKeys.size() > 0)
          {
            lNodes.push_front(n2);
            if (n2.vKeys.size() > 1)
            {
              vSizeAndPointerToNode.push_back(make_pair(n2.vKeys.size(), &lNodes.front()));
              lNodes.front().lit = lNodes.begin();
            }
          }
          if (n3.vKeys.size() > 0)
          {
            lNodes.push_front(n3);
            if (n3.vKeys.size() > 1)
            {
              vSizeAndPointerToNode.push_back(make_pair(n3.vKeys.size(), &lNodes.front()));
              lNodes.front().lit = lNodes.begin();
            }
          }
          if (n4.vKeys.size() > 0)
          {
            lNodes.push_front(n4);
            if (n4.vKeys.size() > 1)
            {
              vSizeAndPointerToNode.push_back(make_pair(n4.vKeys.size(), &lNodes.front()));
              lNodes.front().lit = lNodes.begin();
            }
          }

          lNodes.erase(vPrevSizeAndPointerToNode[j].second->lit);

          if ((int)lNodes.size() >= N)
            break;
        }

        if ((int)lNodes.size() >= N || (int)lNodes.size() == prevSize)
          bFinish = true;
      }
    }
  }

  // Retain the best point in each node
  vector<cv::KeyPoint> vResultKeys;
  vResultKeys.reserve(nfeatures);
  for (list<ExtractorNode>::iterator lit = lNodes.begin(); lit != lNodes.end(); lit++)
  {
    vector<cv::KeyPoint>& vNodeKeys = lit->vKeys;
    cv::KeyPoint* pKP = &vNodeKeys[0];
    float maxResponse = pKP->response;

    for (size_t k = 1; k < vNodeKeys.size(); k++)
    {
      if (vNodeKeys[k].response > maxResponse)
      {
        pKP = &vNodeKeys[k];
        maxResponse = vNodeKeys[k].response;
      }
    }

    vResultKeys.push_back(*pKP);
  }

  return vResultKeys;
}

void ORBextractor::ComputeKeyPointsOctTree(vector<vector<KeyPoint> >& allKeypoints)
{
  allKeypoints.resize(nlevels);

  const float W = 30;

  for (int level = 0; level < nlevels; ++level)
  {
    const int minBorderX = EDGE_THRESHOLD - 3;
    const int minBorderY = minBorderX;
    const int maxBorderX = mvImagePyramid[level].cols - EDGE_THRESHOLD + 3;
    const int maxBorderY = mvImagePyramid[level].rows - EDGE_THRESHOLD + 3;

    vector<cv::KeyPoint> vToDistributeKeys;
    vToDistributeKeys.reserve(nfeatures * 10);

    const float width = (maxBorderX - minBorderX);
    const float height = (maxBorderY - minBorderY);

    const int nCols = width / W;
    const int nRows = height / W;
    const int wCell = ceil(width / nCols);
    const int hCell = ceil(height / nRows);

    for (int i = 0; i < nRows; i++)
    {
      const float iniY = minBorderY + i * hCell;
      float maxY = iniY + hCell + 6;

      if (iniY >= maxBorderY - 3)
        continue;
      if (maxY > maxBorderY)
        maxY = maxBorderY;

      for (int j = 0; j < nCols; j++)
      {
        const float iniX = minBorderX + j * wCell;
        float maxX = iniX + wCell + 6;
        if (iniX >= maxBorderX - 6)
          continue;
        if (maxX > maxBorderX)
          maxX = maxBorderX;

        vector<cv::KeyPoint> vKeysCell;
        FAST(mvImagePyramid[level].rowRange(iniY, maxY).colRange(iniX, maxX), vKeysCell, iniThFAST, true);

        if (vKeysCell.empty())
        {
          FAST(mvImagePyramid[level].rowRange(iniY, maxY).colRange(iniX, maxX), vKeysCell, minThFAST, true);
        }

        if (!vKeysCell.empty())
        {
          for (vector<cv::KeyPoint>::iterator vit = vKeysCell.begin(); vit != vKeysCell.end(); vit++)
          {
            (*vit).pt.x += j * wCell;
            (*vit).pt.y += i * hCell;
            vToDistributeKeys.push_back(*vit);
          }
        }
      }
    }

    vector<KeyPoint>& keypoints = allKeypoints[level];
    keypoints.reserve(nfeatures);

    keypoints = DistributeOctTree(vToDistributeKeys, minBorderX, maxBorderX, minBorderY, maxBorderY,
                                  mnFeaturesPerLevel[level], level);

    const int scaledPatchSize = PATCH_SIZE * mvScaleFactor[level];

    // Add border to coordinates and scale information
    const int nkps = keypoints.size();
    for (int i = 0; i < nkps; i++)
    {
      keypoints[i].pt.x += minBorderX;
      keypoints[i].pt.y += minBorderY;
      keypoints[i].octave = level;
      keypoints[i].size = scaledPatchSize;
    }
  }

  // compute orientations
  for (int level = 0; level < nlevels; ++level)
    computeOrientation(mvImagePyramid[level], allKeypoints[level], umax);
}

void ORBextractor::ComputeKeyPointsOld(std::vector<std::vector<KeyPoint> >& allKeypoints)
{
  allKeypoints.resize(nlevels);

  float imageRatio = (float)mvImagePyramid[0].cols / mvImagePyramid[0].rows;

  for (int level = 0; level < nlevels; ++level)
  {
    const int nDesiredFeatures = mnFeaturesPerLevel[level];

    const int levelCols = sqrt((float)nDesiredFeatures / (5 * imageRatio));
    const int levelRows = imageRatio * levelCols;

    const int minBorderX = EDGE_THRESHOLD;
    const int minBorderY = minBorderX;
    const int maxBorderX = mvImagePyramid[level].cols - EDGE_THRESHOLD;
    const int maxBorderY = mvImagePyramid[level].rows - EDGE_THRESHOLD;

    const int W = maxBorderX - minBorderX;
    const int H = maxBorderY - minBorderY;
    const int cellW = ceil((float)W / levelCols);
    const int cellH = ceil((float)H / levelRows);

    const int nCells = levelRows * levelCols;
    const int nfeaturesCell = ceil((float)nDesiredFeatures / nCells);

    vector<vector<vector<KeyPoint> > > cellKeyPoints(levelRows, vector<vector<KeyPoint> >(levelCols));

    vector<vector<int> > nToRetain(levelRows, vector<int>(levelCols, 0));
    vector<vector<int> > nTotal(levelRows, vector<int>(levelCols, 0));
    vector<vector<bool> > bNoMore(levelRows, vector<bool>(levelCols, false));
    vector<int> iniXCol(levelCols);
    vector<int> iniYRow(levelRows);
    int nNoMore = 0;
    int nToDistribute = 0;

    float hY = cellH + 6;

    for (int i = 0; i < levelRows; i++)
    {
      const float iniY = minBorderY + i * cellH - 3;
      iniYRow[i] = iniY;

      if (i == levelRows - 1)
      {
        hY = maxBorderY + 3 - iniY;
        if (hY <= 0)
          continue;
      }

      float hX = cellW + 6;

      for (int j = 0; j < levelCols; j++)
      {
        float iniX;

        if (i == 0)
        {
          iniX = minBorderX + j * cellW - 3;
          iniXCol[j] = iniX;
        }
        else
        {
          iniX = iniXCol[j];
        }

        if (j == levelCols - 1)
        {
          hX = maxBorderX + 3 - iniX;
          if (hX <= 0)
            continue;
        }

        Mat cellImage = mvImagePyramid[level].rowRange(iniY, iniY + hY).colRange(iniX, iniX + hX);

        cellKeyPoints[i][j].reserve(nfeaturesCell * 5);

        FAST(cellImage, cellKeyPoints[i][j], iniThFAST, true);

        if (cellKeyPoints[i][j].size() <= 3)
        {
          cellKeyPoints[i][j].clear();

          FAST(cellImage, cellKeyPoints[i][j], minThFAST, true);
        }

        const int nKeys = cellKeyPoints[i][j].size();
        nTotal[i][j] = nKeys;

        if (nKeys > nfeaturesCell)
        {
          nToRetain[i][j] = nfeaturesCell;
          bNoMore[i][j] = false;
        }
        else
        {
          nToRetain[i][j] = nKeys;
          nToDistribute += nfeaturesCell - nKeys;
          bNoMore[i][j] = true;
          nNoMore++;
        }
      }
    }

    // Retain by score

    while (nToDistribute > 0 && nNoMore < nCells)
    {
      int nNewFeaturesCell = nfeaturesCell + ceil((float)nToDistribute / (nCells - nNoMore));
      nToDistribute = 0;

      for (int i = 0; i < levelRows; i++)
      {
        for (int j = 0; j < levelCols; j++)
        {
          if (!bNoMore[i][j])
          {
            if (nTotal[i][j] > nNewFeaturesCell)
            {
              nToRetain[i][j] = nNewFeaturesCell;
              bNoMore[i][j] = false;
            }
            else
            {
              nToRetain[i][j] = nTotal[i][j];
              nToDistribute += nNewFeaturesCell - nTotal[i][j];
              bNoMore[i][j] = true;
              nNoMore++;
            }
          }
        }
      }
    }

    vector<KeyPoint>& keypoints = allKeypoints[level];
    keypoints.reserve(nDesiredFeatures * 2);

    const int scaledPatchSize = PATCH_SIZE * mvScaleFactor[level];

    // Retain by score and transform coordinates
    for (int i = 0; i < levelRows; i++)
    {
      for (int j = 0; j < levelCols; j++)
      {
        vector<KeyPoint>& keysCell = cellKeyPoints[i][j];
        KeyPointsFilter::retainBest(keysCell, nToRetain[i][j]);
        if ((int)keysCell.size() > nToRetain[i][j])
          keysCell.resize(nToRetain[i][j]);

        for (size_t k = 0, kend = keysCell.size(); k < kend; k++)
        {
          keysCell[k].pt.x += iniXCol[j];
          keysCell[k].pt.y += iniYRow[i];
          keysCell[k].octave = level;
          keysCell[k].size = scaledPatchSize;
          keypoints.push_back(keysCell[k]);
        }
      }
    }

    if ((int)keypoints.size() > nDesiredFeatures)
    {
      KeyPointsFilter::retainBest(keypoints, nDesiredFeatures);
      keypoints.resize(nDesiredFeatures);
    }
  }

  // and compute orientations
  for (int level = 0; level < nlevels; ++level)
    computeOrientation(mvImagePyramid[level], allKeypoints[level], umax);
}

static void computeDescriptors(const Mat& image, vector<KeyPoint>& keypoints, Mat& descriptors,
                               const vector<Point>& pattern)
{
  descriptors = Mat::zeros((int)keypoints.size(), 32, CV_8UC1);

  for (size_t i = 0; i < keypoints.size(); i++)
    computeOrbDescriptor(keypoints[i], image, &pattern[0], descriptors.ptr((int)i));
}

// ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
// so important !!! But so hard to find where it is !!!
// ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
void ORBextractor::operator()(InputArray _image, InputArray _mask, vector<KeyPoint>& _keypoints,
                              OutputArray _descriptors)
{
  if (_image.empty())
    return;

  Mat image = _image.getMat();
  assert(image.type() == CV_8UC1);

  // Pre-compute the scale pyramid
  ComputePyramid(image);

  vector<vector<KeyPoint> > allKeypoints;
  clock_t s_1, e_1;
  double fea_det_time;
  s_1 = clock();
  ComputeKeyPointsOctTree(allKeypoints);
  e_1 = clock();
  fea_det_time = (double)(e_1 - s_1) / CLOCKS_PER_SEC * 1000;
  // std::cout << "feature detection time: " << fea_det_time << std::endl;

  Mat descriptors;

  int nkeypoints = 0;
  for (int level = 0; level < nlevels; ++level)
    nkeypoints += (int)allKeypoints[level].size();

  if (nkeypoints == 0)
    _descriptors.release();
  else
  {
    _descriptors.create(nkeypoints, 32, CV_8U);
    descriptors = _descriptors.getMat();
  }

  _keypoints.clear();
  _keypoints.reserve(nkeypoints);

  int offset = 0;
  for (int level = 0; level < nlevels; ++level)
  {
    vector<KeyPoint>& keypoints = allKeypoints[level];
    int nkeypointsLevel = (int)keypoints.size();

    if (nkeypointsLevel == 0)
      continue;

    // preprocess the resized image
    Mat workingMat = mvImagePyramid[level].clone();
    GaussianBlur(workingMat, workingMat, Size(7, 7), 2, 2, BORDER_REFLECT_101);

    // Compute the descriptors
    Mat desc = descriptors.rowRange(offset, offset + nkeypointsLevel);
    clock_t s_2, e_2;
    double fea_des_time;
    s_2 = clock();
    // computeDescriptors(workingMat, keypoints, desc, pattern);
    e_2 = clock();
    fea_des_time = (double)(e_2 - s_2) / CLOCKS_PER_SEC * 1000;
    // std::cout << "compute descriptor time: " << fea_des_time << std::endl;

    offset += nkeypointsLevel;

    // Scale keypoint coordinates
    if (level != 0)
    {
      float scale = mvScaleFactor[level];  // getScale(level, firstLevel, scaleFactor);
      for (vector<KeyPoint>::iterator keypoint = keypoints.begin(), keypointEnd = keypoints.end();
           keypoint != keypointEnd; ++keypoint)
        keypoint->pt *= scale;
    }

    // And add the keypoints to the output
    _keypoints.insert(_keypoints.end(), keypoints.begin(), keypoints.end());
  }
}

void ORBextractor::ComputePyramid(cv::Mat image)
{
  for (int level = 0; level < nlevels; ++level)
  {
    float scale = mvInvScaleFactor[level];
    Size sz(cvRound((float)image.cols * scale), cvRound((float)image.rows * scale));
    Size wholeSize(sz.width + EDGE_THRESHOLD * 2, sz.height + EDGE_THRESHOLD * 2);
    Mat temp(wholeSize, image.type()), masktemp;
    mvImagePyramid[level] = temp(Rect(EDGE_THRESHOLD, EDGE_THRESHOLD, sz.width, sz.height));

    // Compute the resized image
    if (level != 0)
    {
      resize(mvImagePyramid[level - 1], mvImagePyramid[level], sz, 0, 0, INTER_LINEAR);

      copyMakeBorder(mvImagePyramid[level], temp, EDGE_THRESHOLD, EDGE_THRESHOLD, EDGE_THRESHOLD, EDGE_THRESHOLD,
                     BORDER_REFLECT_101 + BORDER_ISOLATED);
    }
    else
    {
      copyMakeBorder(image, temp, EDGE_THRESHOLD, EDGE_THRESHOLD, EDGE_THRESHOLD, EDGE_THRESHOLD, BORDER_REFLECT_101);
    }
  }
}

}  // namespace dyno
````

## File: frontend/vision/StaticFeatureTracker.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include "dynosam/frontend/vision/StaticFeatureTracker.hpp"

#include <opencv2/features2d.hpp>
#include <opencv2/imgproc.hpp>
#include <opencv2/opencv.hpp>
#include <opencv2/video/tracking.hpp>

#include "dynosam/common/Types.hpp"
#include "dynosam/frontend/vision/VisionTools.hpp"

namespace dyno {

StaticFeatureTracker::StaticFeatureTracker(const TrackerParams& params,
                                           Camera::Ptr camera,
                                           ImageDisplayQueue* display_queue)
    : FeatureTrackerBase(params, camera, display_queue) {}

ExternalFlowFeatureTracker::ExternalFlowFeatureTracker(
    const TrackerParams& params, Camera::Ptr camera,
    ImageDisplayQueue* display_queue)
    : StaticFeatureTracker(params, camera, display_queue),
      static_grid_(
          static_cell_size,
          std::ceil(static_cast<double>(camera->getParams().ImageWidth()) /
                    static_cell_size),
          std::ceil(static_cast<double>(camera->getParams().ImageHeight()) /
                    static_cell_size)) {
  const auto orb_params = params.orb_params;
  orb_detector_ = std::make_unique<ORBextractor>(
      params.max_nr_keypoints_before_anms,
      static_cast<float>(orb_params.scale_factor), orb_params.n_levels,
      orb_params.init_threshold_fast, orb_params.min_threshold_fast);

  CHECK(!img_size_.empty());
}

// if previous frame is null, assume that it is the first frame... and that
// frames are always processed in order!
FeatureContainer ExternalFlowFeatureTracker::trackStatic(
    Frame::Ptr previous_frame, const ImageContainer& image_container,
    FeatureTrackerInfo& tracker_info, const cv::Mat&) {
  const ImageWrapper<ImageType::RGBMono>& rgb_wrapper =
      image_container.getImageWrapper<ImageType::RGBMono>();
  const cv::Mat& rgb = rgb_wrapper.toRGB();
  cv::Mat mono = ImageType::RGBMono::toMono(rgb_wrapper);
  CHECK(!mono.empty());

  const cv::Mat& motion_mask = image_container.get<ImageType::MotionMask>();
  CHECK(!motion_mask.empty());

  cv::Mat descriptors;
  KeypointsCV detected_keypoints;
  (*orb_detector_)(mono, cv::Mat(), detected_keypoints, descriptors);

  // assign tracked features to grid and add to static features
  FeatureContainer static_features;

  const size_t& min_tracks =
      static_cast<size_t>(params_.max_features_per_frame);
  const FrameId frame_k = image_container.getFrameId();

  // appy tracking (ie get correspondences)
  // TODO: only track frames that have been tracked for some time?
  if (previous_frame) {
    // TODO: for now assume consequative frames
    const FrameId frame_k_1 = previous_frame->getFrameId();
    CHECK_EQ(frame_k_1 + 1u, frame_k);

    for (Feature::Ptr previous_feature : previous_frame->static_features_) {
      const size_t tracklet_id = previous_feature->trackletId();
      const size_t age = previous_feature->age();
      const Keypoint kp = previous_feature->predictedKeypoint();

      // check kp contained before we do a static grid look up to ensure we
      // don't go out of bounds
      if (!camera_->isKeypointContained(kp)) {
        continue;
      }

      const int x = functional_keypoint::u(kp);
      const int y = functional_keypoint::v(kp);
      const size_t cell_idx = static_grid_.getCellIndex(kp);
      const ObjectId instance_label = motion_mask.at<ObjectId>(y, x);

      if (static_grid_.isOccupied(cell_idx)) continue;

      if (previous_feature->usable() && instance_label == background_label) {
        size_t new_age = age + 1;
        Feature::Ptr feature = constructStaticFeature(
            image_container, kp, new_age, tracklet_id, frame_k);
        if (feature) {
          static_features.add(feature);
          static_grid_.occupancy_[cell_idx] = true;
        }
      }
    }
  }

  // number features tracked with optical flow
  const auto n_optical_flow = static_features.size();
  tracker_info.static_track_optical_flow = n_optical_flow;

  TrackletIdManager& tracked_id_manager = TrackletIdManager::instance();

  if (static_features.size() < min_tracks) {
    // iterate over new observations
    for (size_t i = 0; i < detected_keypoints.size(); i++) {
      if (static_features.size() >= min_tracks) {
        break;
      }

      const KeypointCV& kp_cv = detected_keypoints[i];
      const int& x = kp_cv.pt.x;
      const int& y = kp_cv.pt.y;

      // if not already tracked with optical flow
      if (motion_mask.at<int>(y, x) != background_label) {
        continue;
      }

      Keypoint kp(x, y);
      const size_t cell_idx = static_grid_.getCellIndex(kp);
      if (!static_grid_.isOccupied(cell_idx)) {
        const size_t age = 0;
        size_t tracklet_id = tracked_id_manager.getTrackletIdCount();
        Feature::Ptr feature = constructStaticFeature(image_container, kp, age,
                                                      tracklet_id, frame_k);
        if (feature) {
          tracked_id_manager.incrementTrackletIdCount();
          static_grid_.occupancy_[cell_idx] = true;
          static_features.add(feature);
        }
      }
    }
  }

  static_grid_.reset();

  size_t total_tracks = static_features.size();
  tracker_info.static_track_detections = total_tracks - n_optical_flow;
  return static_features;
}

Feature::Ptr ExternalFlowFeatureTracker::constructStaticFeature(
    const ImageContainer& image_container, const Keypoint& kp, size_t age,
    TrackletId tracklet_id, FrameId frame_id) const {
  // implicit double -> int cast for pixel location
  const int x = functional_keypoint::u(kp);
  const int y = functional_keypoint::v(kp);

  const cv::Mat& rgb = image_container.get<ImageType::RGBMono>();
  const cv::Mat& motion_mask = image_container.get<ImageType::MotionMask>();
  const cv::Mat& optical_flow = image_container.get<ImageType::OpticalFlow>();

  CHECK(!optical_flow.empty());
  CHECK(!motion_mask.empty());

  if (motion_mask.at<int>(y, x) != background_label) {
    return nullptr;
  }

  // check flow
  double flow_xe = static_cast<double>(optical_flow.at<cv::Vec2f>(y, x)[0]);
  double flow_ye = static_cast<double>(optical_flow.at<cv::Vec2f>(y, x)[1]);

  if (!(flow_xe != 0 && flow_ye != 0)) {
    return nullptr;
  }

  OpticalFlow flow(flow_xe, flow_ye);

  // check predicted flow is within image
  Keypoint predicted_kp = Feature::CalculatePredictedKeypoint(kp, flow);
  if (!camera_->isKeypointContained(predicted_kp)) {
    return nullptr;
  }

  Feature::Ptr feature = std::make_shared<Feature>();
  (*feature)
      .objectId(background_label)
      .frameId(frame_id)
      .keypointType(KeyPointType::STATIC)
      .age(age)
      .trackletId(tracklet_id)
      .keypoint(kp)
      .measuredFlow(flow)
      .predictedKeypoint(predicted_kp);
  return feature;
}

KltFeatureTracker::KltFeatureTracker(const TrackerParams& params,
                                     Camera::Ptr camera,
                                     ImageDisplayQueue* display_queue)
    : StaticFeatureTracker(params, camera, display_queue) {
  detector_ = std::make_shared<SparseFeatureDetector>(
      params, FunctionalDetector::FactoryCreate(params));
  CHECK_NOTNULL(detector_);
}

FeatureContainer KltFeatureTracker::trackStatic(
    Frame::Ptr previous_frame, const ImageContainer& image_container,
    FeatureTrackerInfo& tracker_info, const cv::Mat& detection_mask) {
  // tracked features and new features
  FeatureContainer new_tracks_and_detections;

  if (!previous_frame) {
    cv::Mat equialized_greyscale;
    equalizeImage(image_container, equialized_greyscale);

    FeatureContainer previous_inliers;
    detectFeatures(equialized_greyscale, image_container, previous_inliers,
                   new_tracks_and_detections, detection_mask);

    tracker_info.static_track_detections = new_tracks_and_detections.size();

    return new_tracks_and_detections;
  } else {
    // we have previous tracks
    cv::Mat current_equialized_greyscale;
    equalizeImage(image_container, current_equialized_greyscale);

    // we should have already calculated the processed rgb image from the
    // previous frame
    cv::Mat previous_equialized_greyscale;
    equalizeImage(previous_frame->image_container_,
                  previous_equialized_greyscale);

    FeatureContainer previous_inliers;
    auto iter = previous_frame->static_features_.beginUsable();
    for (const auto& inlier_feature : iter) {
      previous_inliers.add(inlier_feature);
    }

    // Tracklet ids associated with the set of previous inliers that are now
    // outliers
    TrackletIds previous_outliers;

    // track features from the previous frame and detect new ones if necessary
    CHECK(trackPoints(current_equialized_greyscale,
                      previous_equialized_greyscale, image_container,
                      previous_inliers, new_tracks_and_detections,
                      previous_outliers, tracker_info, detection_mask));

    // after tracking, mark features in the older frame as outliers
    // TODO: (jesse) actually not sure we HAVE to do this, but better to keep
    // things as consisent as possible
    previous_frame->static_features_.markOutliers(previous_outliers);

    return new_tracks_and_detections;
  }
}

void KltFeatureTracker::equalizeImage(const ImageContainer& image_container,
                                      cv::Mat& equialized_greyscale) const {
  const ImageWrapper<ImageType::RGBMono>& rgb_wrapper =
      image_container.getImageWrapper<ImageType::RGBMono>();
  const cv::Mat& rgb = rgb_wrapper.toRGB();
  cv::Mat mono = ImageType::RGBMono::toMono(rgb_wrapper);
  CHECK(!mono.empty());

  mono.copyTo(equialized_greyscale);
  // CHECK(clahe_);

  // clahe_->apply(mono, equialized_greyscale);
}

std::vector<cv::Point2f> KltFeatureTracker::detectRawFeatures(
    const cv::Mat& processed_img, int number_tracked, const cv::Mat& mask) {
  KeypointsCV keypoints;
  detector_->detect(processed_img, keypoints, number_tracked, mask);

  LOG(INFO) << keypoints.size();

  std::vector<cv::Point2f> points;
  cv::KeyPoint::convert(keypoints, points);
  return points;
}

bool KltFeatureTracker::detectFeatures(const cv::Mat& processed_img,
                                       const ImageContainer& image_container,
                                       const FeatureContainer& current_features,
                                       FeatureContainer& new_features,
                                       const cv::Mat& detection_mask) {
  const FrameId frame_k = image_container.getFrameId();

  const cv::Mat& motion_mask = image_container.get<ImageType::MotionMask>();
  // internal detection mask that is appended with new invalid pixels
  // this builds the static detection mask over the existing input mask
  cv::Mat detection_mask_impl;
  // If we are provided with an external detection/feature mask, initalise the
  // detection mask with this and add more invalid sections to it
  if (!detection_mask.empty()) {
    CHECK_EQ(motion_mask.rows, detection_mask.rows);
    CHECK_EQ(motion_mask.cols, detection_mask.cols);
    detection_mask_impl = detection_mask.clone();
  } else {
    detection_mask_impl = cv::Mat(motion_mask.size(), CV_8U, cv::Scalar(255));
  }
  CHECK_EQ(detection_mask_impl.type(), CV_8U);

  // slow
  // add mask over objects detected in the scene
  for (int i = 0; i < motion_mask.rows; i++) {
    for (int j = 0; j < motion_mask.cols; j++) {
      const ObjectId label = motion_mask.at<ObjectId>(i, j);

      if (label != background_label) {
        cv::circle(detection_mask_impl, cv::Point2f(j, i),
                   params_.min_distance_btw_tracked_and_detected_features,
                   cv::Scalar(0), cv::FILLED);
      }
    }
  }

  // add mask over current static features
  for (const auto& feature : current_features) {
    const Keypoint kp = feature->keypoint();
    CHECK(feature->usable());
    cv::circle(detection_mask_impl, cv::Point2f(kp(0), kp(1)),
               params_.min_distance_btw_tracked_and_detected_features,
               cv::Scalar(0), cv::FILLED);
  }

  std::vector<cv::Point2f> detected_points = detectRawFeatures(
      processed_img, current_features.size(), detection_mask_impl);

  for (const cv::Point2f& detected_point : detected_points) {
    Keypoint kp(static_cast<double>(detected_point.x),
                static_cast<double>(detected_point.y));
    const int x = functional_keypoint::u(kp);
    const int y = functional_keypoint::v(kp);

    if (!(camera_->isKeypointContained(kp) && isWithinShrunkenImage(kp))) {
      continue;
    }

    // with the detection mask this should never happen
    if (motion_mask.at<int>(y, x) != background_label) {
      continue;
    }

    Feature::Ptr feature = constructNewStaticFeature(kp, frame_k);
    if (feature) {
      new_features.add(feature);
    }
  }

  return true;
}

bool KltFeatureTracker::trackPoints(const cv::Mat& current_processed_img,
                                    const cv::Mat& previous_processed_img,
                                    const ImageContainer& image_container,
                                    const FeatureContainer& previous_features,
                                    FeatureContainer& tracked_features,
                                    TrackletIds& outlier_previous_features,
                                    FeatureTrackerInfo& tracker_info,
                                    const cv::Mat& detection_mask) {
  if (current_processed_img.empty() || previous_processed_img.empty() ||
      previous_features.empty()) {
    return false;
  }

  outlier_previous_features.clear();

  const cv::Mat& motion_mask = image_container.get<ImageType::MotionMask>();
  const FrameId frame_k = image_container.getFrameId();

  std::vector<uchar> status;
  std::vector<float> err;
  std::vector<cv::Point2f> current_points;
  // All tracklet ids from the set of previous features to track
  TrackletIds tracklet_ids;

  std::vector<cv::Point2f> previous_pts =
      previous_features.toOpenCV(&tracklet_ids);
  CHECK_EQ(previous_pts.size(), previous_features.size());
  CHECK_EQ(previous_pts.size(), tracklet_ids.size());

  // as per documentation the vector must have the same size as the input
  current_points.resize(previous_pts.size());
  const cv::Size klt_window_size(21, 21);  // Window size for KLT
  const int klt_max_level = 3;             // Max pyramid levels for KLT
  const cv::TermCriteria klt_criteria = cv::TermCriteria(
      cv::TermCriteria::EPS | cv::TermCriteria::COUNT, 30, 0.03);

  cv::calcOpticalFlowPyrLK(previous_processed_img, current_processed_img,
                           previous_pts, current_points, status, err,
                           klt_window_size, klt_max_level, klt_criteria);

  CHECK_EQ(previous_pts.size(), current_points.size());
  CHECK_EQ(status.size(), current_points.size());

  std::vector<cv::Point2f> good_current, good_previous;
  TrackletIds good_tracklets;
  // can also look at the err?
  for (size_t i = 0; i < status.size(); i++) {
    if (status[i]) {
      good_current.push_back(current_points.at(i));
      good_previous.push_back(previous_pts.at(i));
      good_tracklets.push_back(tracklet_ids.at(i));
    }
  }

  // Geometric verification using RANSAC
  const cv::Mat geometric_verification_mask =
      geometricVerification(good_previous, good_current);
  std::vector<cv::Point2f> verified_current, verified_previous;
  TrackletIds verified_tracklets;
  for (int i = 0; i < geometric_verification_mask.rows; ++i) {
    if (geometric_verification_mask.at<uchar>(i)) {
      verified_current.push_back(good_current.at(i));
      verified_previous.push_back(good_previous.at(i));
      verified_tracklets.push_back(good_tracklets.at(i));
    }
  }

  CHECK_EQ(verified_tracklets.size(), verified_current.size());

  // add to tracked features
  for (size_t i = 0; i < verified_tracklets.size(); i++) {
    TrackletId tracklet_id = verified_tracklets.at(i);

    const Feature::Ptr previous_feature =
        previous_features.getByTrackletId(tracklet_id);
    // TODO: check this is the same as the previos kp to guarnatee order?

    CHECK(previous_feature->usable());

    const cv::Point2f kp_cv = verified_current.at(i);
    Keypoint kp(static_cast<double>(kp_cv.x), static_cast<double>(kp_cv.y));

    const int x = functional_keypoint::u(kp);
    const int y = functional_keypoint::v(kp);

    if (motion_mask.at<int>(y, x) != background_label) {
      continue;
    }

    if (!(camera_->isKeypointContained(kp) && isWithinShrunkenImage(kp))) {
      continue;
    }
    Feature::Ptr feature = constructStaticFeatureFromPrevious(
        kp, previous_feature, tracklet_id, frame_k);
    if (feature) {
      tracked_features.add(feature);
    }
  }

  // Get the outliers associated with the previous_features container by taking
  // the set difference between the verified and total tracklets NOTE: verified
  // tracklets are not necessary the same as the tracklets in tracked_features
  // as tracked features may excluse some features (e.g. if not in the shrunken
  // image) or (will eventually) have new tracklets after a new detection takes
  // place we just want the set difference between the original features and
  // ones we KNOW are outliers
  determineOutlierIds(verified_tracklets, tracklet_ids,
                      outlier_previous_features);

  const auto& n_tracked = tracked_features.size();
  tracker_info.static_track_optical_flow = n_tracked;

  if (tracked_features.size() <
      static_cast<size_t>(params_.max_features_per_frame)) {
    // if we do not have enough features, detect more on the current image
    detectFeatures(current_processed_img, image_container, tracked_features,
                   tracked_features, detection_mask);

    const auto n_detected = tracked_features.size() - n_tracked;
    tracker_info.static_track_detections += n_detected;
  }

  return true;
}

cv::Mat KltFeatureTracker::geometricVerification(
    const std::vector<cv::Point2f>& good_old,
    const std::vector<cv::Point2f>& good_new) const {
  if (good_old.size() >= 4) {  // Minimum number of points required for RANSAC
    cv::Mat mask;
    cv::findHomography(good_old, good_new, cv::RANSAC, 5.0, mask);
    return mask;
  } else {
    return cv::Mat::ones(
        good_old.size(), 1,
        CV_8U);  // If not enough points, assume all are inliers
  }
}

Feature::Ptr KltFeatureTracker::constructStaticFeatureFromPrevious(
    const Keypoint& kp_current, Feature::Ptr previous_feature,
    const TrackletId tracklet_id, const FrameId frame_id) const {
  CHECK(previous_feature);
  CHECK_EQ(previous_feature->trackletId(), tracklet_id);

  size_t age = previous_feature->age();
  age++;

  TrackletId tracklet_to_use = tracklet_id;
  // if age is too large, or age is zero, retrieve new tracklet id
  if (age > params_.max_feature_track_age) {
    TrackletIdManager& tracked_id_manager = TrackletIdManager::instance();
    tracklet_to_use = tracked_id_manager.getTrackletIdCount();
    tracked_id_manager.incrementTrackletIdCount();
    age = 0u;
  }

  // update previous keypoint
  previous_feature->measuredFlow(kp_current - previous_feature->keypoint());
  // This is so awful, but happens becuase the way the code was originally
  // written, we expect flow from k to k+1 (grrrr)
  previous_feature->predictedKeypoint(kp_current);

  Feature::Ptr feature = std::make_shared<Feature>();
  (*feature)
      .objectId(background_label)
      .frameId(frame_id)
      .keypointType(KeyPointType::STATIC)
      .age(age)
      .markInlier()
      .trackletId(tracklet_to_use)
      .keypoint(kp_current);

  return feature;
}

Feature::Ptr KltFeatureTracker::constructNewStaticFeature(
    const Keypoint& kp_current, const FrameId frame_id) const {
  static const auto kAge = 0u;

  TrackletIdManager& tracked_id_manager = TrackletIdManager::instance();
  TrackletId tracklet_to_use = tracked_id_manager.getTrackletIdCount();
  tracked_id_manager.incrementTrackletIdCount();

  Feature::Ptr feature = std::make_shared<Feature>();
  (*feature)
      .objectId(background_label)
      .frameId(frame_id)
      .keypointType(KeyPointType::STATIC)
      .age(kAge)
      .markInlier()
      .trackletId(tracklet_to_use)
      .keypoint(kp_current);
  return feature;
}

}  // namespace dyno
````

## File: frontend/vision/StereoMatcher.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/frontend/vision/StereoMatcher.hpp"

#include "dynosam/common/ImageTypes.hpp" //for depth image

namespace dyno {

StereoMatcher::StereoMatcher(
    StereoCamera::Ptr stereo_camera,
    const StereoMatchingParams& stereo_param,
    const DenseStereoParams& dense_stereo_params)
    : stereo_camera_(stereo_camera),
      stereo_matching_params_(stereo_param),
      dense_stereo_params_(dense_stereo_params)
{
    CHECK_NOTNULL(stereo_camera);

    cv_stereo_matcher_ = constructStereoMatcher(
        stereo_camera_,
        stereo_matching_params_,
        dense_stereo_params_
    );
    CHECK_NOTNULL(cv_stereo_matcher_);

}

void StereoMatcher::denseStereoReconstruction(
                                  const cv::Mat& left_img,
                                  const cv::Mat& right_img,
                                  cv::Mat& depth_image) const
{

    CHECK_EQ(right_img.cols, left_img.cols);
    CHECK_EQ(right_img.rows, left_img.rows);
    CHECK_EQ(right_img.type(), left_img.type());

    cv::Mat left_rectified, right_rectified;
    stereo_camera_->undistortRectifyImages(left_rectified, right_rectified, left_img, right_img);

    cv::Mat disparity_img;
    disparityRectifiedReconstruction(left_rectified, right_rectified, disparity_img);

    constexpr auto depth_type = ImageType::Depth::OpenCVType;
    depth_image = cv::Mat::zeros(disparity_img.size(), depth_type);

    // Check
    // https://github.com/opencv/opencv/blob/master/samples/cpp/stereo_match.cpp
    // only if output is CV_16S
    cv::Mat disparity_float;
    disparity_img.convertTo(disparity_float, CV_32F, 1.0f / 16.0f);
    // disp_img.convertTo(floatDisp, CV_32F, 1.0f / 16.0f);
    disparity_img = disparity_float;

    const auto baseline = stereo_camera_->getBaseline();
    const auto fx = stereo_camera_->getStereoCameraCalibration().fx();

    // Get depth from disparity
    for (int i = 0u; i < disparity_img.rows; i++) {
        // Loop over rows
        const float* disp_ptr = disparity_img.ptr<float>(i);
        double* depth_ptr = depth_image.ptr<double>(i);

        for (int j = 0u; j < disparity_img.cols; j++) {
            // Loop over cols
            const double depth = (baseline * fx) / static_cast<double>(disp_ptr[j]);
            *(depth_ptr + j) = depth;
        }
    }

}

void StereoMatcher::disparityRectifiedReconstruction(const cv::Mat& rectified_left_img,
                                  const cv::Mat& rectified_right_img,
                                  cv::Mat& disparity_image,
                                  cv::Mat* disparity_viz) const
{
    CHECK_EQ(rectified_right_img.cols, rectified_left_img.cols);
    CHECK_EQ(rectified_right_img.rows, rectified_left_img.rows);
    CHECK_EQ(rectified_right_img.type(), rectified_left_img.type());

    //input expects 8bit single channel
    cv::Mat rectified_left_mono = ImageType::RGBMono::toMono(rectified_left_img);
    cv::Mat rectified_right_mono = ImageType::RGBMono::toMono(rectified_right_img);

    CHECK_EQ(rectified_left_mono.channels(),  1);
    CHECK_EQ(rectified_right_mono.channels(), 1);

    // initially computed as 16SC1
    cv_stereo_matcher_->compute(
      rectified_left_mono, rectified_right_mono, disparity_image);
    CHECK_EQ(disparity_image.type(), CV_16S);

    if (dense_stereo_params_.median_blur_disparity_) {
        cv::medianBlur(disparity_image, disparity_image, 5);
    }

    if(disparity_viz) {
        utils::getDisparityVis(disparity_image, *disparity_viz);
    }

}

cv::Ptr<cv::StereoMatcher> StereoMatcher::constructStereoMatcher(
        StereoCamera::ConstPtr stereo_camera,
        const StereoMatchingParams& /*stereo_param*/,
        const DenseStereoParams& dense_stereo_params)
{
    // Setup stereo matcher
    cv::Ptr<cv::StereoMatcher> cv_stereo_matcher;
    if (dense_stereo_params.use_sgbm_) {
        int mode;
        if (dense_stereo_params.use_mode_HH_) {
            LOG(INFO) << "Using mode = cv::StereoSGBM::MODE_HH for dense stereo reconstruction";
            mode = cv::StereoSGBM::MODE_HH;
        } else {
            LOG(INFO) << "Using mode = cv::StereoSGBM::MODE_SGBM for dense stereo reconstruction";
            mode = cv::StereoSGBM::MODE_SGBM;
        }
        cv_stereo_matcher =
            cv::StereoSGBM::create(dense_stereo_params.min_disparity_,
                                dense_stereo_params.num_disparities_,
                                dense_stereo_params.sad_window_size_,
                                dense_stereo_params.p1_,
                                dense_stereo_params.p2_,
                                dense_stereo_params.disp_12_max_diff_,
                                dense_stereo_params.pre_filter_cap_,
                                dense_stereo_params.uniqueness_ratio_,
                                dense_stereo_params.speckle_window_size_,
                                dense_stereo_params.speckle_range_,
                                mode);
    } else {
        LOG(INFO) << "Using StereoBM for dense stereo reconstruction";
        cv::Ptr<cv::StereoBM> sbm =
            cv::StereoBM::create(dense_stereo_params.num_disparities_,
                                dense_stereo_params.sad_window_size_);

        sbm->setPreFilterType(dense_stereo_params.pre_filter_type_);
        sbm->setPreFilterSize(dense_stereo_params.pre_filter_size_);
        sbm->setPreFilterCap(dense_stereo_params.pre_filter_cap_);
        sbm->setMinDisparity(dense_stereo_params.min_disparity_);
        sbm->setTextureThreshold(dense_stereo_params.texture_threshold_);
        sbm->setUniquenessRatio(dense_stereo_params.uniqueness_ratio_);
        sbm->setSpeckleRange(dense_stereo_params.speckle_range_);
        sbm->setSpeckleWindowSize(dense_stereo_params.speckle_window_size_);
        const cv::Rect& roi1 = stereo_camera->getROI1();
        const cv::Rect& roi2 = stereo_camera->getROI2();
        if (!roi1.empty() && !roi2.empty()) {
            sbm->setROI1(roi1);
            sbm->setROI2(roi2);
        } else {
            LOG(WARNING) << "ROIs are empty.";
        }

        cv_stereo_matcher = sbm;
    }

    CHECK_NOTNULL(cv_stereo_matcher);
    return cv_stereo_matcher;
}


}
````

## File: frontend/vision/TrackerParams.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/frontend/vision/TrackerParams.hpp"
#include "dynosam/utils/OpenCVUtils.hpp"
#include <config_utilities/config_utilities.h>
#include <config_utilities/types/eigen_matrix.h>

#include <gflags/gflags.h>

DEFINE_int32(shrink_row, 0, "Number of rows to shrink the tracking image by");
DEFINE_int32(shrink_col, 0, "Number of cols to shrink the tracking image by");

DEFINE_int32(semantic_mask_step_size, 3, "The step sized used across the semantic mask when sampling points");
DEFINE_bool(use_propogate_mask, true, "If true, the semantic mask will be propogated with optical flow");

namespace dyno {

void declare_config(TrackerParams::AnmsParams& config) {
    using namespace config;

    name("AnmsParams");
    enum_field(
        config.non_max_suppression_type,
        "non_max_suppression_type",
        {"TopN",
        "BrownANMS",
        "SDC",
        "KdTree",
        "RangeTree",
        "Ssc",
        "Binning"});

    field(config.nr_horizontal_bins, "nr_horizontal_bins");
    field(config.nr_vertical_bins, "nr_vertical_bins");
    field(config.binning_mask, "binning_mask");
}

void declare_config(TrackerParams::SubPixelCornerRefinementParams& config) {
    using namespace config;
    name("SubPixelCornerRefinementParams");
    field(config.window_size, "window_size");
    field(config.zero_zone, "zero_zone");
}

void declare_config(TrackerParams::GFFTParams& config) {
    using namespace config;
    name("GFFTParams");
    field(config.quality_level, "quality_level");
    field(config.block_size, "block_size");
    field(config.use_harris_corner_detector, "use_harris_corner_detector");
    field(config.k, "k");
}

void declare_config(TrackerParams::OrbParams& config) {
    using namespace config;
    name("OrbParams");
    field(config.scale_factor, "scale_factor");
    field(config.n_levels, "n_levels");
    field(config.init_threshold_fast, "init_threshold_fast");
    field(config.min_threshold_fast, "min_threshold_fast");
}

void declare_config(TrackerParams& config) {
    using namespace config;
    name("TrackerParams");

    enum_field(
        config.feature_detector_type,
        "feature_detector_type",
        std::vector<std::string>({"GFTT", "ORB_SLAM_ORB"}));

    field(config.use_anms, "use_anms");
    field(config.use_subpixel_corner_refinement, "use_subpixel_corner_refinement");
    field(config.use_clahe_filter, "use_clahe_filter");
    field(config.max_nr_keypoints_before_anms, "max_nr_keypoints_before_anms");
    field(config.min_distance_btw_tracked_and_detected_features, "min_distance_btw_tracked_and_detected_features");
    field(config.max_features_per_frame, "max_features_per_frame");
    field(config.max_feature_track_age, "max_feature_track_age");

    field(config.shrink_row, "shrink_row");
    field(config.shrink_col, "shrink_col");

    //update with FLAGS
    config.shrink_row = FLAGS_shrink_row;
    config.shrink_col = FLAGS_shrink_col;

    field(config.anms_params, "anms_params");
    field(config.subpixel_corner_refinement_params, "subpixel_corner_refinement_params");
    field(config.gfft_params, "gfft_params");
    field(config.orb_params, "orb_params");

    field(config.semantic_mask_step_size, "semantic_mask_step_size");
    field(config.use_propogate_mask, "use_propogate_mask");

    //update with FLAGS
    config.semantic_mask_step_size = FLAGS_semantic_mask_step_size;
    config.use_propogate_mask = FLAGS_use_propogate_mask;
}


} //dyno
````

## File: frontend/vision/UndistortRectifier.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include <eigen3/Eigen/Dense>
#include <opencv4/opencv2/core/eigen.hpp>

#include "dynosam/frontend/vision/UndistortRectifier.hpp"
#include "dynosam/utils/GtsamUtils.hpp"

#include <opencv4/opencv2/calib3d.hpp>
#include <opencv4/opencv2/core.hpp>


namespace dyno {

UndistorterRectifier::UndistorterRectifier(const cv::Mat& P,
                                           const CameraParams& cam_params,
                                           const cv::Mat& R)
    : map_x_(), map_y_(), P_(P), R_(R), cam_params_(cam_params) {
  initUndistortRectifyMaps(cam_params, R, P, &map_x_, &map_y_);
}

void UndistorterRectifier::UndistortRectifyKeypoints(
      const Keypoints& keypoints,
      Keypoints& undistorted_keypoints,
      const CameraParams& cam_params,
      std::optional<cv::Mat> P,
      std::optional<cv::Mat> R)
{
    const auto distortion_model = cam_params.getDistortionModel();
    const cv::Mat& distortion_matrix = cam_params.getDistortionCoeffs();
    const cv::Mat& camera_matrix = cam_params.getCameraMatrix();

    //but slow as we have to do all conversion back and forth
    //if the Eigen matrices are aligned we might not even need to do this...
    std::vector<cv::Point2f> keypoints_cv = utils::gtsamPointsToCv(keypoints);
    std::vector<cv::Point2f> undistorted_keypoints_cv;



    switch (distortion_model) {
        case DistortionModel::RADTAN: {
        cv::undistortPoints(keypoints_cv,
                            undistorted_keypoints_cv,
                            camera_matrix,
                            distortion_matrix,
                            R ? R.value() : cv::noArray(),
                            P ? P.value() : cv::noArray());
        } break;
        case DistortionModel::EQUIDISTANT: {
        // TODO: Create unit test for fisheye / equidistant model
        cv::fisheye::undistortPoints(keypoints_cv,
                                    undistorted_keypoints_cv,
                                    camera_matrix,
                                    distortion_matrix,
                                    R ? R.value() : cv::noArray(),
                                    P ? P.value() : cv::noArray());
        } break;
        default: {
            LOG(FATAL) << "Unknown distortion model " << to_string(distortion_model) << " when rectifying keypoints";
        }

    }

    undistorted_keypoints.clear();
    undistorted_keypoints = utils::cvPointsToGtsam(undistorted_keypoints_cv);
}

// NOTE: we don't pass P because we want normalized/canonical pixel
// coordinates (3D bearing vectors with last element = 1) for versors.
// If we were to pass P, it would convert back to pixel coordinates.
gtsam::Vector3 UndistorterRectifier::UndistortKeypointAndGetVersor(
      const Keypoint& keypoint,
      const CameraParams& cam_params,
      std::optional<cv::Mat> R)
{

  Keypoints distorted_keypoint;
  distorted_keypoint.push_back(keypoint);

  Keypoints undistorted_keypoint;
  UndistorterRectifier::UndistortRectifyKeypoints(
      distorted_keypoint,
      undistorted_keypoint,
      cam_params,
      std::nullopt,
      R);

  // Transform to unit vector.
  gtsam::Vector3 versor(
      undistorted_keypoint.at(0)(0), undistorted_keypoint.at(0)(1), 1.0);
  return versor.normalized();
}

gtsam::Vector3 UndistorterRectifier::undistortKeypointAndGetVersor(const Keypoint& keypoint) const {
  return UndistortKeypointAndGetVersor(keypoint, cam_params_, R_);
}

gtsam::Vector3 UndistorterRectifier::UndistortKeypointAndGetProjectedVersor(
      const Keypoint& keypoint,
      const CameraParams& cam_params,
      std::optional<cv::Mat> P,
      std::optional<cv::Mat> R)
{
  Keypoints distorted_keypoint;
  distorted_keypoint.push_back(keypoint);

  Keypoints undistorted_keypoint;
  UndistorterRectifier::UndistortRectifyKeypoints(
      distorted_keypoint,
      undistorted_keypoint,
      cam_params,
      std::nullopt,
      R);

  gtsam::Vector3 versor(
      undistorted_keypoint.at(0)(0), undistorted_keypoint.at(0)(1), 1.0);
  gtsam::Matrix K = gtsam::Matrix::Identity(3, 3);

  if(P) {
    //construct K from new camera matrix P
    cv::cv2eigen(*P, K);
  }
  else {
    cv::cv2eigen(cam_params.getCameraMatrix(), K);
  }

  return gtsam::Vector3 ( K.inverse() * versor).normalized();

}

gtsam::Vector3 UndistorterRectifier::undistortKeypointAndGetProjectedVersor(const Keypoint& keypoint) const {
  return UndistortKeypointAndGetProjectedVersor(keypoint, cam_params_, P_, R_);
}


void UndistorterRectifier::undistortRectifyImage(const cv::Mat& img,
                             cv::Mat& undistorted_img) const
{
  CHECK_EQ(map_x_.size, img.size);
  CHECK_EQ(map_y_.size, img.size);

  const auto original_image_type = img.type();
  //convert the input image into a CV type compatible with the rectify maps
  cv::Mat converted_img;
  img.copyTo(converted_img);
  converted_img.convertTo(converted_img, kRectifyImageType);

  cv::remap(converted_img,
            undistorted_img,
            map_x_,
            map_y_,
            remap_interpolation_type_,
            remap_use_constant_border_type_ ? cv::BORDER_CONSTANT
                                            : cv::BORDER_REPLICATE);

  //convert back to the original input image type
  undistorted_img.convertTo(undistorted_img, original_image_type);

}


void UndistorterRectifier::undistortRectifyKeypoints(const Keypoints& keypoints,
                                    Keypoints& undistorted_keypoints) const
{
  undistorted_keypoints.clear();
  UndistorterRectifier::UndistortRectifyKeypoints(
      keypoints, undistorted_keypoints, cam_params_, R_, P_);
}


// void UndistorterRectifier::undistortRectifyKeypoints(FeatureContainer* features) const
// {
//   CHECK_NOTNULL(features);
//   Keypoints keypoints, undistorted_keypoints;

//   //fill keypoints
//   for (const auto& feature : *features) {
//     keypoints.push_back(feature->keypoint_);
//   }

//   UndistorterRectifier::UndistortRectifyKeypoints(
//       keypoints, undistorted_keypoints, cam_params_, R_, P_);

//   CHECK_EQ(undistorted_keypoints.size(), features->size());
//   //udpate keypoints
//   for
// }



void UndistorterRectifier::initUndistortRectifyMaps(
    const CameraParams& cam_params,
    const cv::Mat& R,
    const cv::Mat& P,
    cv::Mat* map_x,
    cv::Mat* map_y) {
  CHECK_NOTNULL(map_x);
  CHECK_NOTNULL(map_y);
  // static constexpr int kImageType = CV_32FC1;
  // static constexpr int kImageType = CV_16SC2;

  const cv::Size image_size = cam_params.imageSize();
  const cv::Mat& distortion_matrix = cam_params.getDistortionCoeffs();
  const cv::Mat& camera_matrix = cam_params.getCameraMatrix();
  const DistortionModel distortion_model = cam_params.getDistortionModel();


  cv::Mat map_x_float, map_y_float;
  switch (distortion_model) {
    case DistortionModel::NONE: {
      map_x_float.create(image_size, kRectifyImageType);
      map_y_float.create(image_size, kRectifyImageType);
    } break;
    case DistortionModel::RADTAN: {
      cv::initUndistortRectifyMap(
          // Input
          camera_matrix,
          distortion_matrix,
          R,
          P,
          image_size,
          kRectifyImageType,
          // Output:
          map_x_float,
          map_y_float);
    } break;
    //TODO: Jesse should this not also be for FISH_EYE model?
    case DistortionModel::EQUIDISTANT: {
      cv::fisheye::initUndistortRectifyMap(
          // Input,
          camera_matrix,
          distortion_matrix,
          R,
          P,
          image_size,
          kRectifyImageType,
          // Output:
          map_x_float,
          map_y_float);
    } break;
    default: {
      LOG(FATAL) << "Unknown distortion model: "
                 << to_string(distortion_model) << " when constructing undistortion maps";
    }
  }

  //TODO: (Jesse) experiment with what marcus was talking about
  // TODO(marcus): can we add this in without causing errors like before?
  // The reason we convert from floating to fixed-point representations
  // of a map is that they can yield much faster (~2x) remapping operations.
  // cv::convertMaps(map_x_float, map_y_float, *map_x, *map_y, CV_16SC2, false);

  *map_x = map_x_float;
  *map_y = map_y_float;
}


} //dyno
````

## File: frontend/vision/VisionTools.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include "dynosam/frontend/vision/VisionTools.hpp"

#include <algorithm>  // std::set_difference, std::sort
#include <cmath>
#include <vector>  // std::vector

#include "dynosam/frontend/FrontendParams.hpp"
#include "dynosam/logger/Logger.hpp"

namespace dyno {

namespace vision_tools {

void getCorrespondences(FeaturePairs& correspondences,
                        const FeatureFilterIterator& previous_features,
                        const FeatureFilterIterator& current_features) {
  correspondences.clear();

  const FeatureContainer& previous_feature_container =
      previous_features.getContainer();

  for (const auto& curr_feature : current_features) {
    // check if previous feature and is valid
    if (previous_feature_container.exists(curr_feature->trackletId())) {
      const auto prev_feature = previous_feature_container.getByTrackletId(
          curr_feature->trackletId());
      CHECK(prev_feature);

      // having checked that feature is in the previous set, also check that it
      // ahderes to the filter
      if (!previous_features(prev_feature)) {
        continue;
      }
      correspondences.push_back({prev_feature, curr_feature});
    }
  }
}

ObjectIds getObjectLabels(const cv::Mat& image) {
  CHECK(!image.empty());
  // TODO: this could be optimised by mapping the data directly to a std::set?
  std::set<ObjectId> unique_labels;
  for (int i = 0; i < image.rows; i++) {
    for (int j = 0; j < image.cols; j++) {
      const ObjectId label = image.at<ObjectId>(i, j);

      if (label != background_label) {
        unique_labels.insert(label);
      }
    }
  }

  return ObjectIds(unique_labels.begin(), unique_labels.end());
}

// TODO: depricate!!
//  std::vector<std::vector<int> > trackDynamic(const FrontendParams& params,
//  const Frame& previous_frame, Frame::Ptr current_frame) {
//    auto& objects_by_instance_label = current_frame->object_observations_;

//   auto& previous_dynamic_feature_container =
//   previous_frame.dynamic_features_; auto& current_dynamic_feature_container =
//   current_frame->dynamic_features_;

//   ObjectIds instance_labels_to_remove;

//   //TODO: shrink object boundary?
//   for(auto& [instance_label, object_observation] : objects_by_instance_label)
//   {
//     double obj_center_depth = 0, sf_min=100, sf_max=0, sf_mean=0, sf_count=0;
//     std::vector<int> sf_range(10,0);

//     const size_t num_object_features =
//     object_observation.object_features_.size();
//     // LOG(INFO) << "tracking object observation with instance label " <<
//     instance_label << " and " << num_object_features << " features";

//     int feature_pairs_valid = 0;
//     int num_found = 0;
//     for(const TrackletId tracklet_id : object_observation.object_features_) {
//       if(previous_dynamic_feature_container.exists(tracklet_id)) {
//         num_found++;
//         CHECK(current_dynamic_feature_container.exists(tracklet_id));

//         Feature::Ptr current_feature =
//         current_dynamic_feature_container.getByTrackletId(tracklet_id);
//         Feature::Ptr previous_feature =
//         previous_dynamic_feature_container.getByTrackletId(tracklet_id);

//         if(!previous_feature->usable()) {
//           current_feature->markInvalid();
//           continue;
//         }

//         //this can happen in situations such as the updateDepths when depths
//         > thresh are marked invalud if(!current_feature->usable()) {
//         continue;}

//         CHECK(!previous_feature->isStatic());
//         CHECK(!current_feature->isStatic());

//         Landmark lmk_previous =
//         previous_frame.backProjectToWorld(tracklet_id); Landmark lmk_current
//         = current_frame->backProjectToWorld(tracklet_id);

//         Landmark flow_world = lmk_current - lmk_previous ;
//         double sf_norm = flow_world.norm();

//         feature_pairs_valid++;

//         if (sf_norm<params.scene_flow_magnitude)
//             sf_count = sf_count+1;
//         if(sf_norm<sf_min)
//             sf_min = sf_norm;
//         if(sf_norm>sf_max)
//             sf_max = sf_norm;
//         sf_mean = sf_mean + sf_norm;

//         {
//           if (0.0<=sf_norm && sf_norm<0.05)
//               sf_range[0] = sf_range[0] + 1;
//           else if (0.05<=sf_norm && sf_norm<0.1)
//               sf_range[1] = sf_range[1] + 1;
//           else if (0.1<=sf_norm && sf_norm<0.2)
//               sf_range[2] = sf_range[2] + 1;
//           else if (0.2<=sf_norm && sf_norm<0.4)
//               sf_range[3] = sf_range[3] + 1;
//           else if (0.4<=sf_norm && sf_norm<0.8)
//               sf_range[4] = sf_range[4] + 1;
//           else if (0.8<=sf_norm && sf_norm<1.6)
//               sf_range[5] = sf_range[5] + 1;
//           else if (1.6<=sf_norm && sf_norm<3.2)
//               sf_range[6] = sf_range[6] + 1;
//           else if (3.2<=sf_norm && sf_norm<6.4)
//               sf_range[7] = sf_range[7] + 1;
//           else if (6.4<=sf_norm && sf_norm<12.8)
//               sf_range[8] = sf_range[8] + 1;
//           else if (12.8<=sf_norm && sf_norm<25.6)
//               sf_range[9] = sf_range[9] + 1;
//         }

//       }

//     }

//     VLOG(10) << "Number feature pairs valid " << feature_pairs_valid << " out
//     of " << num_object_features << " for instance  " << instance_label << "
//     num found " << num_found;

//     //if no points found (i.e tracked)
//     //dont do anything as this is a new object so we cannot say if its
//     dynamic or not if(num_found == 0) {
//       //TODO: i guess?
// object_observation.marked_as_moving_ = true;
//     }
//     if (sf_count/num_object_features>params.scene_flow_percentage ||
//     num_object_features < 150u)
//     // else if (sf_count/num_object_features>params.scene_flow_percentage ||
//     num_object_features < 15)
//     {
//       // label this object as static background
//       // LOG(INFO) << "Instance object " << instance_label << " to static for
//       frame " << current_frame->frame_id_;
//       instance_labels_to_remove.push_back(instance_label);
//     }
//     else {
//       // LOG(INFO) << "Instance object " << instance_label << " marked as
//       dynamic"; object_observation.marked_as_moving_ = true;
//     }
//   }

//   //we do the removal after the iteration so as not to mess up the loop
//   for(const auto label : instance_labels_to_remove) {
//     VLOG(30) << "Removing label " << label;
//     //TODO: this is really really slow!!
//     current_frame->moveObjectToStatic(label);
//     // LOG(INFO) << "Done Removing label " << label;
//   }

//   // Relabel the objects that associate with the objects in last frame
//   objects_by_instance_label = current_frame->object_observations_; //get
//   iterator gain as the observations have changed

//   // per object iteration
//   for(auto& [instance_label, object_observation] : objects_by_instance_label)
//   {
//     CHECK(object_observation.marked_as_moving_) << "Object with instance
//     label "
//       << instance_label << " is not marked as moving, but should be";
//     // LOG(INFO) << "Assigning tracking label for instance " <<
//     instance_label;

//     const size_t num_object_features =
//     object_observation.object_features_.size();
//     // save semantic labels in last frame
//     ObjectIds instance_labels_prev;
//     //feature in each object iteration
//     for(const TrackletId tracklet_id : object_observation.object_features_) {
//       if(previous_dynamic_feature_container.exists(tracklet_id)) {
//         CHECK(current_dynamic_feature_container.exists(tracklet_id));

//         //TODO: inliers outliers!@!

//         Feature::Ptr previous_feature =
//         previous_dynamic_feature_container.getByTrackletId(tracklet_id);
//         instance_labels_prev.push_back(previous_feature->objectId());
//       }
//     }

//     const bool tracked_in_previous_frame = instance_labels_prev.size() > 0u;
//     //since we only call this function after boostrapping (frame > 1) this
//     condition only works when an object appears from frame 2 onwards
//     if(!tracked_in_previous_frame) {
//       // LOG(INFO) << "New object with instance label " << instance_label <<
//       " assigned tracking label " << Frame::global_object_id;
//       current_frame->updateObjectTrackingLabel(object_observation,
//       Frame::global_object_id); Frame::global_object_id++; continue;
//      }

//     // find label that appears most in instance_labels_prev()
//     // (1) count duplicates
//     std::map<int, int> dups;
//     for(int k : instance_labels_prev) ++dups[k];
//     // (2) and sort them by descending order
//     std::vector<std::pair<int, int> > sorted;
//     for (auto k : dups)
//         sorted.push_back(std::make_pair(k.first,k.second));

//     //copied from FeatureTracker.cc
//     auto sort_pair_int = [](const std::pair<int, int>& a, const
//     std::pair<int, int>& b) -> bool {
//       return (a.second > b.second);
//     };
//     std::sort(sorted.begin(), sorted.end(), sort_pair_int);

//     // label the (instance) object in current frame
//     ObjectId new_label = sorted[0].first;

//     //TODO: make global_object_id access a function?
//     if(Frame::global_object_id == 1) {
//       current_frame->updateObjectTrackingLabel(object_observation,
//       Frame::global_object_id); Frame::global_object_id++;
//     }
//     else {
//       //here we need to propogate the tracking label (if exists) from the
//       previous frame
//       //this means we expect all the features in the previous frame to have
//       the same tracking label
//       //(can this even not happen?)

//       //find previous observation to see if one has the same instance label
//       auto& previous_object_observations =
//       previous_frame.object_observations_; auto it = std::find_if(
//         previous_object_observations.begin(),
//         previous_object_observations.end(),
//         /*pair = <ObjectId, DynamicObjectObservation>*/
//         [&](const auto& pair) { return pair.second.instance_label_ ==
//         new_label; });

//       //object tracking label was in the previous frame
//       //tracking label has not been updated from previous frame? will still
//       be -1? if(it != previous_object_observations.end() &&
//       it->second.tracking_label_ != -1) {
//         ObjectId propogated_tracking_label = it->second.tracking_label_;
//         // LOG(INFO) << "Propogating tracking label " <<
//         propogated_tracking_label << " from frames " <<
//         previous_frame.frame_id_ << " -> " << current_frame->frame_id_;
//         current_frame->updateObjectTrackingLabel(object_observation,
//         propogated_tracking_label);
//       }
//       else {
//         // new object (or at least not tracked)
//         // LOG(INFO) << "New object with instance label " << instance_label
//         << " assigned tracking label " << Frame::global_object_id;

//         current_frame->updateObjectTrackingLabel(object_observation,
//         Frame::global_object_id); Frame::global_object_id++;
//       }

//     }

//     VLOG(20) << "Done tracking for instance label " << instance_label;
//   }
//   return std::vector<std::vector<int> >();
// }

std::vector<std::vector<int>> trackDynamic(const FrontendParams& params,
                                           const Frame& previous_frame,
                                           Frame::Ptr current_frame) {
  auto& objects_by_instance_label = current_frame->object_observations_;

  auto& previous_dynamic_feature_container = previous_frame.dynamic_features_;
  auto& current_dynamic_feature_container = current_frame->dynamic_features_;

  ObjectIds instance_labels_to_remove;

  // TODO: shrink object boundary?
  for (auto& [instance_label, object_observation] : objects_by_instance_label) {
    double obj_center_depth = 0, sf_min = 100, sf_max = 0, sf_mean = 0,
           sf_count = 0;
    std::vector<int> sf_range(10, 0);

    const size_t num_object_features =
        object_observation.object_features_.size();
    // LOG(INFO) << "tracking object observation with instance label " <<
    // instance_label << " and " << num_object_features << " features";

    int feature_pairs_valid = 0;
    int num_found = 0;
    for (const TrackletId tracklet_id : object_observation.object_features_) {
      if (previous_dynamic_feature_container.exists(tracklet_id)) {
        num_found++;
        CHECK(current_dynamic_feature_container.exists(tracklet_id));

        Feature::Ptr current_feature =
            current_dynamic_feature_container.getByTrackletId(tracklet_id);
        Feature::Ptr previous_feature =
            previous_dynamic_feature_container.getByTrackletId(tracklet_id);

        if (!previous_feature->usable()) {
          current_feature->markInvalid();
          continue;
        }

        // this can happen in situations such as the updateDepths when depths >
        // thresh are marked invalud
        if (!current_feature->usable()) {
          continue;
        }

        CHECK(!previous_feature->isStatic());
        CHECK(!current_feature->isStatic());

        Landmark lmk_previous = previous_frame.backProjectToWorld(tracklet_id);
        Landmark lmk_current = current_frame->backProjectToWorld(tracklet_id);

        Landmark flow_world = lmk_current - lmk_previous;
        double sf_norm = flow_world.norm();

        feature_pairs_valid++;

        if (sf_norm < params.scene_flow_magnitude) sf_count = sf_count + 1;
        if (sf_norm < sf_min) sf_min = sf_norm;
        if (sf_norm > sf_max) sf_max = sf_norm;
        sf_mean = sf_mean + sf_norm;

        {
          if (0.0 <= sf_norm && sf_norm < 0.05)
            sf_range[0] = sf_range[0] + 1;
          else if (0.05 <= sf_norm && sf_norm < 0.1)
            sf_range[1] = sf_range[1] + 1;
          else if (0.1 <= sf_norm && sf_norm < 0.2)
            sf_range[2] = sf_range[2] + 1;
          else if (0.2 <= sf_norm && sf_norm < 0.4)
            sf_range[3] = sf_range[3] + 1;
          else if (0.4 <= sf_norm && sf_norm < 0.8)
            sf_range[4] = sf_range[4] + 1;
          else if (0.8 <= sf_norm && sf_norm < 1.6)
            sf_range[5] = sf_range[5] + 1;
          else if (1.6 <= sf_norm && sf_norm < 3.2)
            sf_range[6] = sf_range[6] + 1;
          else if (3.2 <= sf_norm && sf_norm < 6.4)
            sf_range[7] = sf_range[7] + 1;
          else if (6.4 <= sf_norm && sf_norm < 12.8)
            sf_range[8] = sf_range[8] + 1;
          else if (12.8 <= sf_norm && sf_norm < 25.6)
            sf_range[9] = sf_range[9] + 1;
        }
      }
    }

    VLOG(10) << "Number feature pairs valid " << feature_pairs_valid
             << " out of " << num_object_features << " for instance  "
             << instance_label << " num found " << num_found;

    // if no points found (i.e tracked)
    // dont do anything as this is a new object so we cannot say if its dynamic
    // or not
    if (num_found == 0) {
      // TODO: i guess?
      object_observation.marked_as_moving_ = true;
    }
    if (sf_count / num_object_features > params.scene_flow_percentage ||
        num_object_features < 30)
    // else if (sf_count/num_object_features>params.scene_flow_percentage ||
    // num_object_features < 15)
    {
      // label this object as static background
      // LOG(INFO) << "Instance object " << instance_label << " to static for
      // frame " << current_frame->frame_id_;
      instance_labels_to_remove.push_back(instance_label);
    } else {
      // LOG(INFO) << "Instance object " << instance_label << " marked as
      // dynamic";
      object_observation.marked_as_moving_ = true;
    }
  }

  // we do the removal after the iteration so as not to mess up the loop
  for (const auto label : instance_labels_to_remove) {
    VLOG(30) << "Removing label " << label;
    // TODO: this is really really slow!!
    current_frame->moveObjectToStatic(label);
    // LOG(INFO) << "Done Removing label " << label;
  }

  return std::vector<std::vector<int>>();
}

bool findObjectBoundingBox(
    const cv::Mat& mask, ObjectId object_id, cv::Rect& detected_rect,
    std::vector<std::vector<cv::Point>>& detected_contours) {
  cv::Mat mask_copy = mask.clone();

  cv::Mat obj_mask = (mask_copy == object_id);
  cv::Mat dilated_obj_mask;
  // dilate to fill any small holes in the mask to get a more complete set of
  // contours
  cv::Mat dilate_element = cv::getStructuringElement(
      cv::MORPH_RECT, cv::Size(1, 11));  // a rectangle of 1*5
  cv::dilate(obj_mask, dilated_obj_mask, dilate_element, cv::Point(-1, -1));

  std::vector<std::vector<cv::Point>> contours;
  std::vector<cv::Vec4i> hierarchy;
  cv::findContours(dilated_obj_mask, contours, hierarchy, cv::RETR_TREE,
                   cv::CHAIN_APPROX_NONE);

  detected_contours = contours;

  if (contours.empty()) {
    detected_rect = cv::Rect();
    return false;
  } else if (contours.size() == 1u) {
    detected_rect = cv::boundingRect(contours.at(0));
  } else {
    std::vector<cv::Rect> rectangles;
    for (auto it : contours) {
      rectangles.push_back(cv::boundingRect(it));
    }
    cv::Rect merged_rect = rectangles[0];
    for (const auto& r : rectangles) {
      merged_rect |= r;
    }
    detected_rect = merged_rect;
  }
  return true;
}

bool findObjectBoundingBox(const cv::Mat& mask, ObjectId object_id,
                           cv::Rect& detected_rect) {
  std::vector<std::vector<cv::Point>> detected_contours;
  auto result =
      findObjectBoundingBox(mask, object_id, detected_rect, detected_contours);
  (void)detected_contours;
  return result;
}

bool findObjectBoundingBox(
    const cv::Mat& mask, ObjectId object_id,
    std::vector<std::vector<cv::Point>>& detected_contours) {
  cv::Rect detected_rect;
  auto result =
      findObjectBoundingBox(mask, object_id, detected_rect, detected_contours);
  (void)detected_rect;
  return result;
}

void shrinkMask(const cv::Mat& mask, cv::Mat& shrunk_mask, int erosion_size) {
  shrunk_mask = cv::Mat::zeros(mask.size(), mask.type());
  shrunk_mask.setTo(background_label);

  const ObjectIds original_object_labels = getObjectLabels(mask);

  const cv::Mat element = cv::getStructuringElement(
      cv::MORPH_RECT, cv::Size(2 * erosion_size + 1, 2 * erosion_size + 1));

  for (const auto object_id : original_object_labels) {
    cv::Mat obj_mask = (mask == object_id);
    cv::Mat eroded_mask;
    cv::erode(obj_mask, eroded_mask, element);
    shrunk_mask = shrunk_mask.setTo(object_id, eroded_mask);
  }
}

void computeObjectMaskBoundaryMask(const cv::Mat& mask, cv::Mat& boundary_mask,
                                   int thickness,
                                   bool use_as_feature_detection_mask) {
  cv::Mat thicc_boarder;  // god im so funny
  cv::Scalar fill_colour;

  // background should be 255 as we're can detect in this region and boarder
  // region should be zero
  if (use_as_feature_detection_mask) {
    thicc_boarder = cv::Mat(mask.size(), CV_8U, cv::Scalar(255));
    fill_colour = cv::Scalar(0);
  } else {
    thicc_boarder = cv::Mat(mask.size(), CV_8U, cv::Scalar(0));
    fill_colour = cv::Scalar(255);
  }

  const ObjectIds instance_labels = vision_tools::getObjectLabels(mask);
  for (const auto object_id : instance_labels) {
    std::vector<std::vector<cv::Point>> detected_contours;
    vision_tools::findObjectBoundingBox(mask, object_id, detected_contours);

    cv::drawContours(thicc_boarder, detected_contours, -1, fill_colour,
                     thickness);
  }

  boundary_mask = thicc_boarder;
}

void relabelMasks(const cv::Mat& mask, cv::Mat& relabelled_mask,
                  const ObjectIds& old_labels, const ObjectIds& new_labels) {
  if (old_labels.size() != old_labels.size()) {
    throw std::invalid_argument(
        "Old labels and new labels must have the same size");
  }

  // Create a map from old labels to new labels
  std::unordered_map<ObjectId, ObjectId> label_map;
  for (size_t i = 0; i < old_labels.size(); ++i) {
    label_map[old_labels[i]] = new_labels[i];
  }

  mask.copyTo(relabelled_mask);
  // / Relabel the pixels
  for (int r = 0; r < relabelled_mask.rows; ++r) {
    for (int c = 0; c < relabelled_mask.cols; ++c) {
      ObjectId pixelValue = relabelled_mask.at<ObjectId>(r, c);
      if (label_map.find(pixelValue) != label_map.end()) {
        relabelled_mask.at<ObjectId>(r, c) = label_map[pixelValue];
      }
    }
  }
}

gtsam::FastMap<ObjectId, Histogram> makeTrackletLengthHistorgram(
    const Frame::Ptr frame, const std::vector<size_t>& bins) {
  // one for every object + 1 for static points
  gtsam::FastMap<ObjectId, Histogram> histograms;

  // collect dynamic features
  for (const auto& [object_id, observations] : frame->getObjectObservations()) {
    Histogram hist(bh::make_histogram(bh::axis::variable<>(bins)));
    hist.name_ = "tacklet-length-" + std::to_string(object_id);

    for (auto tracklet_id : observations.object_features_) {
      const Feature::Ptr feature = frame->at(tracklet_id);
      CHECK(feature);
      if (feature->usable()) {
        hist.histogram_(feature->age());
      }
    }

    histograms.insert2(object_id, hist);
  }

  // collect static features
  Histogram static_hist(bh::make_histogram(bh::axis::variable<>(bins)));
  static_hist.name_ = "tacklet-length-0";
  for (const auto& static_feature : frame->static_features_.beginUsable()) {
    static_hist.histogram_(static_feature->age());
  }
  histograms.insert2(background_label, static_hist);
  return histograms;
}

cv::Mat depthTo3D(const ImageWrapper<ImageType::Depth>& depth_image,
                  const cv::Mat& K) {
  const cv::Mat& depth_map = depth_image;
  int H = depth_map.rows;
  int W = depth_map.cols;

  // Camera intrinsic parameters
  double fx = K.at<double>(0, 0);
  double fy = K.at<double>(1, 1);
  double cx = K.at<double>(0, 2);
  double cy = K.at<double>(1, 2);

  // Generate pixel grid
  cv::Mat u_grid, v_grid;
  cv::Mat u = cv::Mat::zeros(H, W, CV_64F);
  cv::Mat v = cv::Mat::zeros(H, W, CV_64F);

  for (int y = 0; y < H; ++y) {
    for (int x = 0; x < W; ++x) {
      u.at<double>(y, x) = static_cast<double>(x);
      v.at<double>(y, x) = static_cast<double>(y);
    }
  }

  // Normalize pixel coordinates by the intrinsic matrix
  cv::Mat X_norm = (u - cx) / fx;
  cv::Mat Y_norm = (v - cy) / fy;

  // Depth map scaling for 3D coordinates
  cv::Mat X_3D = X_norm.mul(depth_map);
  cv::Mat Y_3D = Y_norm.mul(depth_map);

  cv::Mat Z_3D = depth_map.clone();

  std::vector<cv::Mat> channels = {X_3D, Y_3D, Z_3D};
  cv::Mat point_cloud;
  cv::merge(channels, point_cloud);

  return point_cloud;  // 3-channel float matrix (H x W x 3)
}

void writeOutProjectMaskAndDepthMap(
    const ImageWrapper<ImageType::Depth>& depth_image,
    const ImageWrapper<ImageType::SemanticMask>& mask_image,
    const Camera& camera, FrameId frame_id) {
  cv::Mat point_cloud =
      depthTo3D(depth_image, camera.getParams().getCameraMatrix());

  const cv::Mat& mask = mask_image;

  // new mask of double type to match the type of the output cloud
  cv::Mat mask_double;
  mask.copyTo(mask_double);
  mask_double.convertTo(mask_double, CV_64F);

  std::vector<cv::Mat> channels = {point_cloud, mask_double};

  cv::Mat projected_cloud;
  cv::merge(channels, projected_cloud);

  static const auto folder_name = "project_mask";
  const std::string output_folder = getOutputFilePath(folder_name);

  // create write out directly if it does not exist
  createDirectory(output_folder);

  const std::string file_name =
      output_folder + "/" + std::to_string(frame_id) + ".yml";
  cv::FileStorage file(file_name, cv::FileStorage::WRITE);
  file << "matrix" << projected_cloud;
  file.release();
}

std::pair<gtsam::Vector3, gtsam::Matrix3> backProjectAndCovariance(
    const Feature& feature, const Camera& camera, double pixel_sigma,
    double depth_sigma) {
  const auto gtsam_camera = camera.getImplCamera();
  const auto keypoint = feature.keypoint();

  CHECK(feature.hasDepth());
  const auto depth = feature.depth();

  gtsam::Matrix32 J_keypoint;
  gtsam::Matrix31 J_depth;
  gtsam::Point3 landmark = gtsam_camera->backproject(
      keypoint, depth, boost::none, J_keypoint, J_depth, boost::none);

  // form measurement covariance matrices
  gtsam::Matrix22 pixel_covariance_matrix;
  pixel_covariance_matrix << pixel_sigma, 0.0, 0.0, pixel_sigma;

  // for depth uncertainty, we model it as a quadratic increase with distnace
  double depth_covariance = depth_sigma * std::pow(depth, 2);

  // calcualte 3x3 covairance matrix
  gtsam::Matrix33 covariance =
      J_keypoint * pixel_covariance_matrix * J_keypoint.transpose() +
      J_depth * depth_covariance * J_depth.transpose();
  return {landmark, covariance};
}

// void writeOutProjectMaskAndDepthMap(const ImageWrapper<ImageType::Depth>&
// depth_image, const ImageWrapper<ImageType::MotionMask>& mask_image, const
// Camera& camera, FrameId frame_id) {
//   writeOutProjectMaskAndDepthMap(depth_image,
//   ImageWrapper<ImageType::SemanticMask>(static_cast<const
//   cv::Mat&>(mask_image)), camera, frame_id);
// }

}  // namespace vision_tools

// void RGBDProcessor::updateMovingObjects(const Frame& previous_frame,
// Frame::Ptr current_frame,  cv::Mat& debug) const {
//   const cv::Mat& rgb =
//   current_frame->tracking_images_.get<ImageType::RGBMono>();

//   rgb.copyTo(debug);

//   const gtsam::Pose3& previous_pose = previous_frame.T_world_camera_;
//   const gtsam::Pose3& current_pose = current_frame->T_world_camera_;

//   const auto previous_dynamic_feature_container =
//   previous_frame.dynamic_features_; const auto
//   current_dynamic_feature_container = current_frame->dynamic_features_;

//   //iterate over each object seen in the previous frame and collect features
//   in current and previous frames to determine scene flow for(auto&
//   [object_id, current_object_observation] :
//   current_frame->object_observations_) {

//     int object_track_count = 0; //number of tracked points on the object
//     int sf_count = 0; //number of points on the object with a sufficient
//     scene flow thresh

//     const TrackletIds& object_features =
//     current_object_observation.object_features_; for(const auto tracklet_id :
//     object_features) {
//       if(previous_dynamic_feature_container.exists(tracklet_id)) {
//         CHECK(current_dynamic_feature_container.exists(tracklet_id));

//         Feature::Ptr current_feature =
//         current_dynamic_feature_container.getByTrackletId(tracklet_id);
//         Feature::Ptr previous_feature =
//         previous_dynamic_feature_container.getByTrackletId(tracklet_id);

//         if(!previous_feature->usable()) {
//           current_feature->markInvalid();
//           continue;
//         }

//         Landmark lmk_previous, lmk_current;
//         camera_->backProject(previous_feature->keypoint_,
//         previous_feature->depth_, &lmk_previous, previous_pose);
//         camera_->backProject(current_feature->keypoint_,
//         current_feature->depth_, &lmk_current, current_pose);

//         Landmark flow_world = lmk_previous - lmk_current;
//         double sf_norm = flow_world.norm();

//         if(sf_norm > params_.scene_flow_magnitude) {
//           sf_count++;
//         }

//         object_track_count++;
//       }
//     }

//     if(sf_count < 50) {
//       continue;
//     }
//     double average_flow_count = (double)sf_count /
//     (double)object_track_count;

//     LOG(INFO) << "Num points that are dynamic " << average_flow_count << "/"
//     << params_.scene_flow_percentage << " for object " << object_id;
//     if(average_flow_count > params_.scene_flow_percentage) {
//       current_object_observation.marked_as_moving_ = true;

//       static const cv::Scalar blue(255, 0, 0);

//       for(TrackletId track : object_features) {
//         Feature::Ptr current_feature =
//         current_dynamic_feature_container.getByTrackletId(track); const
//         Keypoint& px = current_feature->keypoint_; cv::circle(debug,
//         utils::gtsamPointToCV(px), 6, blue, 1);
//       }

//       //only debug stuff

//     }

//   }

// }

void determineOutlierIds(const TrackletIds& inliers,
                         const TrackletIds& tracklets, TrackletIds& outliers) {
  VLOG_IF(1, inliers.size() > tracklets.size())
      << "Usage warning: inlier size (" << inliers.size()
      << ") > tracklets size (" << tracklets.size()
      << "). Are you parsing inliers as tracklets incorrectly?";
  outliers.clear();
  TrackletIds inliers_sorted(inliers.size()),
      tracklets_sorted(tracklets.size());
  std::copy(inliers.begin(), inliers.end(), inliers_sorted.begin());
  std::copy(tracklets.begin(), tracklets.end(), tracklets_sorted.begin());

  std::sort(inliers_sorted.begin(), inliers_sorted.end());
  std::sort(tracklets_sorted.begin(), tracklets_sorted.end());

  // full set A (tracklets) must be first and inliers MUST be a subset of A for
  // the set_difference function to work
  std::set_difference(tracklets_sorted.begin(), tracklets_sorted.end(),
                      inliers_sorted.begin(), inliers_sorted.end(),
                      std::inserter(outliers, outliers.begin()));
}

}  // namespace dyno
````

## File: frontend/Frontend-Definitions.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/frontend/Frontend-Definitions.hpp"

namespace dyno {

// decltype(kRgbdFrontendOutputJsonFile) constexpr kRgbdFrontendOutputJsonFile;

} //dyno
````

## File: frontend/FrontendModule.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/frontend/FrontendModule.hpp"
#include "dynosam/logger/Logger.hpp"

#include <glog/logging.h>

namespace dyno {

FrontendModule::FrontendModule(const FrontendParams& params, ImageDisplayQueue* display_queue)
    :   Base("frontend"), base_params_(params), display_queue_(display_queue)
    {
        //create callback to update gt_packet_map_ values so the derived classes dont need to manage this
        registerInputCallback([=](FrontendInputPacketBase::ConstPtr input) {
            if(input->optional_gt_) gt_packet_map_.insert2(input->getFrameId(), *input->optional_gt_);
        });
    }

FrontendModule::~FrontendModule() {
    VLOG(5) << "Destructing frontend module";

    // if(!gt_packet_map_.empty()) {
    //     //OfstreamWrapper will ensure this goes to the FLAGS_output_path
    //     OfstreamWrapper::WriteOutJson(gt_packet_map_, "ground_truths.json");
    // }
}

void FrontendModule::validateInput(const FrontendInputPacketBase::ConstPtr& input) const {
    CHECK(input->image_container_);

    const ImageValidationResult result = validateImageContainer(input->image_container_);

    //throw exception if result is false
    checkAndThrow<InvalidImageContainerException>(result.valid_, *input->image_container_, result.requirement_);

}

} //dyno
````

## File: frontend/FrontendParams.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/frontend/FrontendParams.hpp"
#include "dynosam/common/Flags.hpp"
#include <string>

#include <config_utilities/config_utilities.h>

DEFINE_bool(refine_motion_estimate, true, "If true, 3D motion refinement will be used");
//TODO: clear up flags - this is defined in Flags.h
DEFINE_bool(refine_with_optical_flow, true, "If true, then joint refinement with optical flow will be used");


namespace dyno {

void declare_config(FrontendParams& config) {
    using namespace config;

    name("FrontendParams");
    field(config.scene_flow_magnitude, "scene_flow_magnitude");
    field(config.scene_flow_percentage, "scene_flow_percentage");

    field(config.max_background_depth, "max_background_depth");
    field(config.max_object_depth, "max_object_depth");

    field(config.use_ego_motion_pnp, "use_ego_motion_pnp");
    field(config.use_object_motion_pnp, "use_object_motion_pnp");
    field(config.refine_camera_pose_with_joint_of, "refine_camera_pose_with_joint_of");

    field(config.object_motion_solver_params, "object_motion_solver");
    field(config.ego_motion_solver_params, "camera_motion_solver");
    field(config.tracker_params, "tracker_params");


    //update with flags
    config.object_motion_solver_params.refine_motion_with_joint_of = FLAGS_refine_with_optical_flow;
    config.object_motion_solver_params.refine_motion_with_3d = FLAGS_refine_motion_estimate;
}


} //dyno
````

## File: frontend/FrontendPipeline.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */


#include "dynosam/frontend/FrontendPipeline.hpp"

#include <glog/logging.h>

namespace dyno {} //dyno
````

## File: frontend/RGBDInstance-Definitions.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include "dynosam/frontend/RGBDInstance-Definitions.hpp"

#include <glog/logging.h>

#include "dynosam/common/Types.hpp"
#include "dynosam/frontend/vision/VisionTools.hpp"
#include "dynosam/logger/Logger.hpp"

namespace dyno {

GenericTrackedStatusVector<LandmarkKeypointStatus>
collectLandmarkKeypointMeasurementsHelper(
    const StatusLandmarkVector& landmarks,
    const StatusKeypointVector& keypoints) {
  CHECK_EQ(landmarks.size(), keypoints.size());

  GenericTrackedStatusVector<LandmarkKeypointStatus> collection;

  for (size_t i = 0; i < landmarks.size(); i++) {
    const auto& lmk_status = landmarks.at(i);
    const auto& kp_status = keypoints.at(i);

    CHECK_EQ(lmk_status.trackletId(), kp_status.trackletId());
    CHECK_EQ(lmk_status.objectId(), kp_status.objectId());
    CHECK_EQ(lmk_status.frameId(), kp_status.frameId());
    // expect visual measurements being sent to the back-end to be local
    CHECK_EQ(lmk_status.referenceFrame(), ReferenceFrame::LOCAL);

    collection.push_back(LandmarkKeypointStatus(
        LandmarkKeypoint(lmk_status.value(), kp_status.value()),
        lmk_status.frameId(), lmk_status.trackletId(), lmk_status.objectId(),
        lmk_status.referenceFrame()));
  }

  return collection;
}

GenericTrackedStatusVector<LandmarkKeypointStatus>
RGBDInstanceOutputPacket::collectStaticLandmarkKeypointMeasurements() const {
  return collectLandmarkKeypointMeasurementsHelper(
      static_landmarks_, static_keypoint_measurements_);
}
GenericTrackedStatusVector<LandmarkKeypointStatus>
RGBDInstanceOutputPacket::collectDynamicLandmarkKeypointMeasurements() const {
  return collectLandmarkKeypointMeasurementsHelper(
      dynamic_landmarks_, dynamic_keypoint_measurements_);
}

RGBDFrontendLogger::RGBDFrontendLogger()
    : EstimationModuleLogger("frontend"),
      tracking_length_hist_file_name_(
          getOutputFilePath("tracklet_length_hist.json")) {}

void RGBDFrontendLogger::logTrackingLengthHistogram(const Frame::Ptr frame) {
  gtsam::FastMap<ObjectId, Histogram> histograms =
      vision_tools::makeTrackletLengthHistorgram(frame);
  // collect histograms per object and then nest them per frame
  // must cast keys (object id, frame id) to string to get the json library to
  // properly construct nested maps
  json per_object_hist;
  for (const auto& [object_id, hist] : histograms) {
    per_object_hist[std::to_string(object_id)] = hist;
  }
  tracklet_length_json_[std::to_string(frame->getFrameId())] = per_object_hist;
}

RGBDFrontendLogger::~RGBDFrontendLogger() {
  JsonConverter::WriteOutJson(tracklet_length_json_,
                              tracking_length_hist_file_name_);
}

}  // namespace dyno
````

## File: frontend/RGBDInstanceFrontendModule.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include "dynosam/frontend/RGBDInstanceFrontendModule.hpp"

#include <glog/logging.h>
#include <tbb/tbb.h>

#include <opencv4/opencv2/opencv.hpp>

#include "dynosam/common/Flags.hpp"  //for common flags
#include "dynosam/frontend/RGBDInstance-Definitions.hpp"
#include "dynosam/frontend/vision/MotionSolver.hpp"
#include "dynosam/frontend/vision/Vision-Definitions.hpp"
#include "dynosam/logger/Logger.hpp"
#include "dynosam/utils/SafeCast.hpp"
#include "dynosam/utils/TimingStats.hpp"

DEFINE_bool(use_frontend_logger, false,
            "If true, the frontend logger will be used");
DEFINE_bool(use_dynamic_track, true,
            "If true, the dynamic tracking will be used");

DEFINE_bool(log_projected_masks, false,
            "If true, projected masks will be saved at every frame");

namespace dyno {

RGBDInstanceFrontendModule::RGBDInstanceFrontendModule(
    const FrontendParams& frontend_params, Camera::Ptr camera,
    ImageDisplayQueue* display_queue)
    : FrontendModule(frontend_params, display_queue),
      camera_(camera),
      motion_solver_(frontend_params.ego_motion_solver_params,
                     camera->getParams()),
      object_motion_solver_(frontend_params.object_motion_solver_params,
                            camera->getParams()) {
  CHECK_NOTNULL(camera_);
  tracker_ =
      std::make_unique<FeatureTracker>(frontend_params, camera_, display_queue);

  if (FLAGS_use_frontend_logger) {
    LOG(INFO) << "Using front-end logger!";
    logger_ = std::make_unique<RGBDFrontendLogger>();
  }
}

RGBDInstanceFrontendModule::~RGBDInstanceFrontendModule() {
  if (FLAGS_save_frontend_json) {
    LOG(INFO) << "Saving frontend output as json";
    const std::string file_path =
        getOutputFilePath(kRgbdFrontendOutputJsonFile);
    JsonConverter::WriteOutJson(output_packet_record_, file_path,
                                JsonConverter::Format::BSON);
  }
}

FrontendModule::ImageValidationResult
RGBDInstanceFrontendModule::validateImageContainer(
    const ImageContainer::Ptr& image_container) const {
  return ImageValidationResult(image_container->hasDepth(),
                               "Depth is required");
}

FrontendModule::SpinReturn RGBDInstanceFrontendModule::boostrapSpin(
    FrontendInputPacketBase::ConstPtr input) {
  ImageContainer::Ptr image_container = input->image_container_;
  Frame::Ptr frame = tracker_->track(input->getFrameId(), input->getTimestamp(),
                                     *image_container);
  // LOG(INFO) << "Num usable static " << frame->numStaticUsableFeatures();
  CHECK(frame->updateDepths());
  // LOG(INFO) << "Num usable static " << frame->numStaticUsableFeatures();

  return {State::Nominal, nullptr};
}

FrontendModule::SpinReturn RGBDInstanceFrontendModule::nominalSpin(
    FrontendInputPacketBase::ConstPtr input) {
  ImageContainer::Ptr image_container = input->image_container_;

  Frame::Ptr frame = nullptr;
  {
    utils::TimingStatsCollector tracking_timer("tracking_timer");
    frame = tracker_->track(input->getFrameId(), input->getTimestamp(),
                            *image_container);
  }
  CHECK(frame);

  Frame::Ptr previous_frame = tracker_->getPreviousFrame();
  CHECK(previous_frame);

  LOG(INFO) << to_string(tracker_->getTrackerInfo());

  {
    utils::TimingStatsCollector update_depths_timer("depth_updater");
    frame->updateDepths();
  }
  // updates frame->T_world_camera_
  if (!solveCameraMotion(frame, previous_frame)) {
    LOG(ERROR) << "Could not solve for camera";
  }

  if (FLAGS_use_dynamic_track) {
    // TODO: bring back byte tracker??
    utils::TimingStatsCollector track_dynamic_timer("tracking_dynamic");
    vision_tools::trackDynamic(base_params_, *previous_frame, frame);
  }

  MotionEstimateMap motion_estimates;
  solveObjectMotions(frame, previous_frame, motion_estimates);

  // update the object_poses trajectory map which will be send to the viz
  propogateObjectPoses(motion_estimates, frame->getFrameId());

  if (logger_) {
    auto ground_truths = this->getGroundTruthPackets();
    logger_->logCameraPose(frame->getFrameId(), frame->getPose(),
                           ground_truths);
    logger_->logObjectMotion(frame->getFrameId(), motion_estimates,
                             ground_truths);
    logger_->logTrackingLengthHistogram(frame);
    logger_->logFrameIdToTimestamp(frame->getFrameId(), frame->getTimestamp());
  }

  DebugImagery debug_imagery;
  debug_imagery.tracking_image =
      tracker_->computeImageTracks(*previous_frame, *frame);
  if (display_queue_)
    display_queue_->push(
        ImageToDisplay("tracks", debug_imagery.tracking_image));

  debug_imagery.detected_bounding_boxes = frame->drawDetectedObjectBoxes();

  const ImageContainer& processed_image_container = frame->image_container_;
  debug_imagery.rgb_viz = ImageType::RGBMono::toRGB(
      processed_image_container.get<ImageType::RGBMono>());
  debug_imagery.flow_viz = ImageType::OpticalFlow::toRGB(
      processed_image_container.get<ImageType::OpticalFlow>());
  debug_imagery.mask_viz = ImageType::MotionMask::toRGB(
      processed_image_container.get<ImageType::MotionMask>());
  debug_imagery.depth_viz = ImageType::Depth::toRGB(
      processed_image_container.get<ImageType::Depth>());

  RGBDInstanceOutputPacket::Ptr output =
      constructOutput(*frame, motion_estimates, frame->T_world_camera_,
                      input->optional_gt_, debug_imagery);

  if (FLAGS_save_frontend_json)
    output_packet_record_.insert({output->getFrameId(), output});

  if (FLAGS_log_projected_masks)
    vision_tools::writeOutProjectMaskAndDepthMap(
        frame->image_container_.get<ImageType::Depth>(),
        frame->image_container_.get<ImageType::MotionMask>(),
        *frame->getCamera(), frame->getFrameId());

  if (logger_) {
    auto ground_truths = this->getGroundTruthPackets();
    logger_->logPoints(output->getFrameId(), output->T_world_camera_,
                       output->dynamic_landmarks_);
    // object_poses_ are in frontend module
    logger_->logObjectPose(output->getFrameId(), object_poses_, ground_truths);
    logger_->logObjectBbxes(output->getFrameId(), output->getObjectBbxes());
  }
  return {State::Nominal, output};
}

bool RGBDInstanceFrontendModule::solveCameraMotion(
    Frame::Ptr frame_k, const Frame::Ptr& frame_k_1) {
  Pose3SolverResult result;
  if (base_params_.use_ego_motion_pnp) {
    result = motion_solver_.geometricOutlierRejection3d2d(frame_k_1, frame_k);
  } else {
    // TODO: untested
    LOG(FATAL) << "Not tested";
    result = motion_solver_.geometricOutlierRejection3d3d(frame_k_1, frame_k);
  }

  VLOG(15) << (base_params_.use_ego_motion_pnp ? "3D2D" : "3D3D")
           << "camera pose estimate at frame " << frame_k->frame_id_
           << (result.status == TrackingStatus::VALID ? " success "
                                                      : " failure ")
           << ":\n"
           << "- Tracking Status: " << to_string(result.status) << '\n'
           << "- Total Correspondences: "
           << result.inliers.size() + result.outliers.size() << '\n'
           << "\t- # inliers: " << result.inliers.size() << '\n'
           << "\t- # outliers: " << result.outliers.size() << '\n';

  if (result.status == TrackingStatus::VALID) {
    frame_k->T_world_camera_ = result.best_result;
    TrackletIds tracklets = frame_k->static_features_.collectTracklets();
    CHECK_GE(tracklets.size(),
             result.inliers.size() +
                 result.outliers.size());  // tracklets shoudl be more (or same
                                           // as) correspondances as there will
                                           // be new points untracked
    frame_k->static_features_.markOutliers(result.outliers);

    if (base_params_.refine_camera_pose_with_joint_of) {
      OpticalFlowAndPoseOptimizer flow_optimizer(
          base_params_.object_motion_solver_params.joint_of_params);

      auto flow_opt_result = flow_optimizer.optimizeAndUpdate<CalibrationType>(
          frame_k_1, frame_k, result.inliers, result.best_result);
      frame_k->T_world_camera_ = flow_opt_result.best_result.refined_pose;
      VLOG(15) << "Refined camera pose with optical flow - error before: "
               << flow_opt_result.error_before.value_or(NaN)
               << " error_after: " << flow_opt_result.error_after.value_or(NaN);
    }
    return true;
  } else {
    frame_k->T_world_camera_ = gtsam::Pose3::Identity();
    return false;
  }
}

bool RGBDInstanceFrontendModule::solveObjectMotion(
    Frame::Ptr frame_k, Frame::Ptr frame_k_1, ObjectId object_id,
    MotionEstimateMap& motion_estimates) {
  Pose3SolverResult result;
  if (base_params_.use_object_motion_pnp) {
    result = object_motion_solver_.geometricOutlierRejection3d2d(
        frame_k_1, frame_k, frame_k->T_world_camera_, object_id);
  } else {
    result = object_motion_solver_.geometricOutlierRejection3d3d(
        frame_k_1, frame_k, frame_k->T_world_camera_, object_id);
  }

  VLOG(15) << (base_params_.use_object_motion_pnp ? "3D2D" : "3D3D")
           << " object motion estimate " << object_id << " at frame "
           << frame_k->frame_id_
           << (result.status == TrackingStatus::VALID ? " success "
                                                      : " failure ")
           << ":\n"
           << "- Tracking Status: " << to_string(result.status) << '\n'
           << "- Total Correspondences: "
           << result.inliers.size() + result.outliers.size() << '\n'
           << "\t- # inliers: " << result.inliers.size() << '\n'
           << "\t- # outliers: " << result.outliers.size() << '\n';
  // sanity check
  // if valid, remove outliers and add to motion estimation
  if (result.status == TrackingStatus::VALID) {
    frame_k->dynamic_features_.markOutliers(result.outliers);
    ReferenceFrameValue<Motion3> estimate(result.best_result,
                                          ReferenceFrame::GLOBAL);
    motion_estimates.insert({object_id, estimate});
    return true;
  } else {
    return false;
  }
}

void RGBDInstanceFrontendModule::solveObjectMotions(
    Frame::Ptr frame_k, Frame::Ptr frame_k_1,
    MotionEstimateMap& motion_estimates) {
  ObjectIds failed_object_tracks;

  // if only 1 object, no point parallelising
  if (motion_estimates.size() <= 1) {
    for (const auto& [object_id, observations] :
         frame_k->object_observations_) {
      if (!solveObjectMotion(frame_k, frame_k_1, object_id, motion_estimates)) {
        VLOG(5) << "Could not solve motion for object " << object_id
                << " from frame " << frame_k_1->getFrameId() << " -> "
                << frame_k->getFrameId();
        failed_object_tracks.push_back(object_id);
      }
    }
  } else {
    std::mutex mutex;
    // paralleilise the process of each function call.
    tbb::parallel_for_each(
        frame_k->object_observations_.begin(),
        frame_k->object_observations_.end(),
        [&](const std::pair<ObjectId, DynamicObjectObservation>& pair) {
          const auto object_id = pair.first;
          if (!solveObjectMotion(frame_k, frame_k_1, object_id,
                                 motion_estimates)) {
            VLOG(5) << "Could not solve motion for object " << object_id
                    << " from frame " << frame_k_1->getFrameId() << " -> "
                    << frame_k->getFrameId();

            std::lock_guard<std::mutex> lk(mutex);
            failed_object_tracks.push_back(object_id);
          }
        });
  }

  /// remove objects from the object observations list
  // does not remove the features etc but stops the object being propogated to
  // the backend as we loop over the object observations in the constructOutput
  // function
  for (auto object_id : failed_object_tracks) {
    frame_k->object_observations_.erase(object_id);
  }
}

RGBDInstanceOutputPacket::Ptr RGBDInstanceFrontendModule::constructOutput(
    const Frame& frame, const MotionEstimateMap& estimated_motions,
    const gtsam::Pose3& T_world_camera,
    const GroundTruthInputPacket::Optional& gt_packet,
    const DebugImagery::Optional& debug_imagery) {
  StatusKeypointVector static_keypoint_measurements;
  StatusLandmarkVector static_landmarks;
  for (const Feature::Ptr& f : frame.usableStaticFeaturesBegin()) {
    const TrackletId tracklet_id = f->trackletId();
    const Keypoint kp = f->keypoint();
    CHECK(f->isStatic());
    CHECK(Feature::IsUsable(f));

    // dont include features that have only been seen once as we havent had a
    // chance to validate it yet
    if (f->age() < 1) {
      continue;
    }

    MeasurementWithCovariance<Keypoint> kp_measurement(kp);
    MeasurementWithCovariance<Landmark> landmark_measurement(
        vision_tools::backProjectAndCovariance(*f, *camera_, 0.2, 0.1));

    static_keypoint_measurements.push_back(KeypointStatus::StaticInLocal(
        kp_measurement, frame.getFrameId(), tracklet_id));

    static_landmarks.push_back(LandmarkStatus::StaticInLocal(
        landmark_measurement, frame.getFrameId(), tracklet_id));
  }

  StatusKeypointVector dynamic_keypoint_measurements;
  StatusLandmarkVector dynamic_landmarks;
  for (const auto& [object_id, obs] : frame.object_observations_) {
    CHECK_EQ(object_id, obs.instance_label_);
    // TODO: add back in?
    //  CHECK(obs.marked_as_moving_);

    for (const TrackletId tracklet : obs.object_features_) {
      if (frame.isFeatureUsable(tracklet)) {
        const Feature::Ptr f = frame.at(tracklet);
        CHECK(!f->isStatic());
        CHECK_EQ(f->objectId(), object_id);

        // dont include features that have only been seen once as we havent had
        // a chance to validate it yet
        if (f->age() < 1) {
          continue;
        }

        const TrackletId tracklet_id = f->trackletId();
        const Keypoint kp = f->keypoint();

        MeasurementWithCovariance<Keypoint> kp_measurement(kp);
        MeasurementWithCovariance<Landmark> landmark_measurement(
            vision_tools::backProjectAndCovariance(*f, *camera_, 0.2, 0.1));

        dynamic_keypoint_measurements.push_back(KeypointStatus::DynamicInLocal(
            kp_measurement, frame.frame_id_, tracklet_id, object_id));

        dynamic_landmarks.push_back(LandmarkStatus::DynamicInLocal(
            landmark_measurement, frame.frame_id_, tracklet_id, object_id));
      }
    }
  }

  // update trajectory of camera poses to be visualised by the frontend viz
  // module
  camera_poses_.push_back(T_world_camera);

  return std::make_shared<RGBDInstanceOutputPacket>(
      static_keypoint_measurements, dynamic_keypoint_measurements,
      static_landmarks, dynamic_landmarks, T_world_camera, frame.timestamp_,
      frame.frame_id_, estimated_motions, object_poses_, camera_poses_, camera_,
      gt_packet, debug_imagery);
}

void RGBDInstanceFrontendModule::propogateObjectPoses(
    const MotionEstimateMap& motion_estimates, FrameId frame_id) {
  gtsam::Point3Vector object_centroids_k_1, object_centroids_k;

  for (const auto& [object_id, motion_estimate] : motion_estimates) {
    const auto frame_k_1 = tracker_->getPreviousFrame();
    const auto frame_k = tracker_->getCurrentFrame();

    auto object_points = FeatureFilterIterator(
        const_cast<FeatureContainer&>(frame_k_1->dynamic_features_),
        [object_id, &frame_k](const Feature::Ptr& f) -> bool {
          return Feature::IsUsable(f) && f->objectId() == object_id &&
                 frame_k->exists(f->trackletId()) &&
                 frame_k->isFeatureUsable(f->trackletId());
        });

    gtsam::Point3 centroid_k_1(0, 0, 0);
    gtsam::Point3 centroid_k(0, 0, 0);
    size_t count = 0;
    for (const auto& feature : object_points) {
      gtsam::Point3 lmk_k_1 =
          frame_k_1->backProjectToCamera(feature->trackletId());
      centroid_k_1 += lmk_k_1;

      gtsam::Point3 lmk_k = frame_k->backProjectToCamera(feature->trackletId());
      centroid_k += lmk_k;

      count++;
    }

    centroid_k_1 /= count;
    centroid_k /= count;

    centroid_k_1 = frame_k_1->getPose() * centroid_k_1;
    centroid_k = frame_k->getPose() * centroid_k;

    object_centroids_k_1.push_back(centroid_k_1);
    object_centroids_k.push_back(centroid_k);
  }

  if (FLAGS_init_object_pose_from_gt) {
    dyno::propogateObjectPoses(object_poses_, motion_estimates,
                               object_centroids_k_1, object_centroids_k,
                               frame_id, getGroundTruthPackets());
  } else {
    dyno::propogateObjectPoses(object_poses_, motion_estimates,
                               object_centroids_k_1, object_centroids_k,
                               frame_id);
  }
}

TrackingInputImages RGBDInstanceFrontendModule::constructTrackingImages(
    const ImageContainer::Ptr image_container) {
  // if we only have instance semgentation (not motion) then we need to make a
  // motion mask out of the semantic mask we cannot do this for the first frame
  // so we will just treat the semantic mask and the motion mask and then
  // subsequently elimate non-moving objects later on
  TrackingInputImages tracking_images;
  if (image_container->hasSemanticMask()) {
    CHECK(!image_container->hasMotionMask());
    // TODO: some bug when going from semantic mask to motion mask as motion
    // mask is empty in the tracker after this process!!! its becuase we dont
    // actually use the tracking_images!!
    auto intermediate_tracking_images =
        image_container->makeSubset<ImageType::RGBMono, ImageType::OpticalFlow,
                                    ImageType::SemanticMask>();
    tracking_images = TrackingInputImages(
        intermediate_tracking_images.getImageWrapper<ImageType::RGBMono>(),
        intermediate_tracking_images.getImageWrapper<ImageType::OpticalFlow>(),
        ImageWrapper<ImageType::MotionMask>(
            intermediate_tracking_images.get<ImageType::SemanticMask>()));
  } else {
    tracking_images =
        image_container->makeSubset<ImageType::RGBMono, ImageType::OpticalFlow,
                                    ImageType::MotionMask>();
  }
  return tracking_images;
}

}  // namespace dyno
````

## File: logger/Logger.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include "dynosam/logger/Logger.hpp"

#include <gflags/gflags.h>
#include <glog/logging.h>

#include <filesystem>

#include "dynosam/utils/Metrics.hpp"
#include "dynosam/utils/Statistics.hpp"

DEFINE_string(output_path, "./", "Path where to store dynosam's log output.");

namespace dyno {

namespace fs = std::filesystem;

std::string getOutputFilePath(const std::string& file_name) {
  fs::path out_path(FLAGS_output_path);
  return out_path / file_name;
}

void writeStatisticsSamplesToFile(const std::string& file_name) {
  utils::Statistics::WriteAllSamplesToCsvFile(getOutputFilePath(file_name));
}

void writeStatisticsSummaryToFile(const std::string& file_name) {
  utils::Statistics::WriteSummaryToCsvFile(getOutputFilePath(file_name));
}

void writeStatisticsModuleSummariesToFile() {
  utils::Statistics::WritePerModuleSummariesToCsvFile(getOutputFilePath(""));
}

bool createDirectory(const std::string& path) {
  std::filesystem::path dir_path(path);

  // Check if directory already exists
  if (std::filesystem::exists(dir_path)) {
    if (std::filesystem::is_directory(dir_path)) {
      VLOG(10) << "Directory already exists: " << path;
      return true;
    } else {
      VLOG(10) << "Path exists but is not a directory: " << path;
      return false;
    }
  }

  // Create directory if it doesn't exist
  if (std::filesystem::create_directories(dir_path)) {
    VLOG(10) << "Directory created: " << path;
    return true;
  } else {
    LOG(WARNING) << "Failed to create directory: " << path;
    return false;
  }
}

// This constructor will directly open the log file when called.
OfstreamWrapper::OfstreamWrapper(const std::string& filename,
                                 const bool& open_file_in_append_mode)
    : OfstreamWrapper(filename, FLAGS_output_path, open_file_in_append_mode) {}

OfstreamWrapper::OfstreamWrapper(const std::string& filename,
                                 const std::string& output_path,
                                 const bool& open_file_in_append_mode)
    : filename_(filename), output_path_(output_path) {
  openLogFile(open_file_in_append_mode);
}

// This destructor will directly close the log file when the wrapper is
// destructed. So no need to explicitly call .close();
OfstreamWrapper::~OfstreamWrapper() {
  VLOG(20) << "Closing output file: " << filename_.c_str();
  ofstream_.flush();
  ofstream_.close();
}

void OfstreamWrapper::closeAndOpenLogFile() {
  ofstream_.flush();
  ofstream_.close();
  CHECK(!filename_.empty());
  OpenFile(output_path_ + '/' + filename_, &ofstream_, false);
}

bool OfstreamWrapper::WriteOutCsvWriter(const CsvWriter& csv,
                                        const std::string& filename) {
  // set append mode to false as we never want to write over the top of a csv
  // file as this will upset the header
  OfstreamWrapper ofsw(filename, false);
  return csv.write(ofsw.ofstream_);
}

void OfstreamWrapper::openLogFile(bool open_file_in_append_mode) {
  CHECK(!filename_.empty());
  CHECK(!output_path_.empty());
  LOG(INFO) << "Opening output file: " << filename_.c_str();
  OpenFile((std::string)getFilePath(), &ofstream_, open_file_in_append_mode);
}

fs::path OfstreamWrapper::getFilePath() const {
  fs::path fs_out_path(output_path_);
  if (!fs::exists(fs_out_path))
    throw std::runtime_error("OfstreamWrapper - Output path does not exist: " +
                             output_path_);

  return fs_out_path / fs::path(filename_);
}

EstimationModuleLogger::EstimationModuleLogger(const std::string& module_name)
    : module_name_(module_name),
      object_pose_file_name_(module_name_ + "_object_pose_log.csv"),
      object_motion_file_name_(module_name_ + "_object_motion_log.csv"),
      object_bbx_file_name_(module_name_ + "_object_bbx_log.csv"),
      // camera_pose_errors_file_name_(module_name_ +
      // "_camera_pose_error_log.csv"),
      camera_pose_file_name_(module_name_ + "_camera_pose_log.csv"),
      map_points_file_name_(module_name_ + "_map_points_log.csv"),
      frame_id_to_timestamp_file_name_(
          "frame_id_timestamp.csv")  // NOTE: not prefixed by module
{
  camera_pose_csv_ = std::make_unique<CsvWriter>(
      CsvHeader("frame_id", "tx", "ty", "tz", "qx", "qy", "qz", "qw", "gt_tx",
                "gt_ty", "gt_tz", "gt_qx", "gt_qy", "gt_qz", "gt_qw"));

  object_pose_csv_ = std::make_unique<CsvWriter>(CsvHeader(
      "frame_id", "object_id", "tx", "ty", "tz", "qx", "qy", "qz", "qw",
      "gt_tx", "gt_ty", "gt_tz", "gt_qx", "gt_qy", "gt_qz", "gt_qw"));

  object_motion_csv_ = std::make_unique<CsvWriter>(CsvHeader(
      "frame_id", "object_id", "tx", "ty", "tz", "qx", "qy", "qz", "qw",
      "gt_tx", "gt_ty", "gt_tz", "gt_qx", "gt_qy", "gt_qz", "gt_qw"));

  map_points_csv_ = std::make_unique<CsvWriter>(CsvHeader(
      "frame_id", "object_id", "tracklet_id", "x_world", "y_world", "z_world"));

  object_bbx_csv_ = std::make_unique<CsvWriter>(
      CsvHeader("frame_id", "object_id", "min_bbx_x", "min_bbx_y", "min_bbx_z",
                "max_bbx_x", "max_bbx_y", "max_bbx_z", "px", "py", "pz", "qw",
                "qx", "qy", "qz"));

  frame_id_timestamp_csv_ =
      std::make_unique<CsvWriter>(CsvHeader("frame_id", "timestamp [ns]"));
}

EstimationModuleLogger::~EstimationModuleLogger() {
  LOG(INFO) << "Writing out " << module_name_ << " logger...";
  OfstreamWrapper::WriteOutCsvWriter(*object_pose_csv_, object_pose_file_name_);
  OfstreamWrapper::WriteOutCsvWriter(*object_bbx_csv_, object_bbx_file_name_);
  OfstreamWrapper::WriteOutCsvWriter(*object_motion_csv_,
                                     object_motion_file_name_);
  OfstreamWrapper::WriteOutCsvWriter(*camera_pose_csv_, camera_pose_file_name_);
  OfstreamWrapper::WriteOutCsvWriter(*map_points_csv_, map_points_file_name_);
  OfstreamWrapper::WriteOutCsvWriter(*frame_id_timestamp_csv_,
                                     frame_id_to_timestamp_file_name_);
}

std::optional<size_t> EstimationModuleLogger::logObjectMotion(
    FrameId frame_id, const MotionEstimateMap& motion_estimates,
    const std::optional<GroundTruthPacketMap>& gt_packets) {
  // if gt packet provided by no data exists at this frame
  if (gt_packets && !gt_packets->exists(frame_id)) {
    VLOG(100) << "No gt packet at frame id " << frame_id
              << ". Unable to log object motion errors";
    return {};
  }

  size_t number_logged = 0;
  for (const auto& [object_id, motions] : motion_estimates) {
    // use identity motion if no ground truth so that we keep the csv file
    // format
    gtsam::Pose3 gt_motion = gtsam::Pose3::Identity();
    const gtsam::Pose3& estimate = motions;

    // gt packet provided so only log if gt pose data found
    if (gt_packets) {
      if (gt_packets->exists(frame_id)) {
        const GroundTruthInputPacket& gt_packet_k = gt_packets->at(frame_id);
        // check object exists in this frame
        ObjectPoseGT object_gt_k;
        if (!gt_packet_k.getObject(object_id, object_gt_k)) {
          // if no packet for this object found, continue and do not log
          continue;
        } else {
          CHECK(object_gt_k.prev_H_current_world_);
          gt_motion = *object_gt_k.prev_H_current_world_;
        }
      } else {
        // gt packet has no entry for this frame id so skip
        continue;
      }
    }

    const auto& quat = estimate.rotation().toQuaternion();
    const auto& gt_quat = gt_motion.rotation().toQuaternion();

    *object_motion_csv_ << frame_id << object_id << estimate.x() << estimate.y()
                        << estimate.z() << quat.x() << quat.y() << quat.z()
                        << quat.w() << gt_motion.x() << gt_motion.y()
                        << gt_motion.z() << gt_quat.x() << gt_quat.y()
                        << gt_quat.z() << gt_quat.w();
    number_logged++;
  }
  return number_logged;
}

// assume poses are in world?
std::optional<size_t> EstimationModuleLogger::logObjectPose(
    FrameId frame_id, const ObjectPoseMap& propogated_poses,
    const std::optional<GroundTruthPacketMap>& gt_packets) {
  // if gt packet provided by no data exists at this frame
  if (gt_packets && !gt_packets->exists(frame_id)) {
    VLOG(100) << "No gt packet at frame id " << frame_id
              << ". Unable to log object pose errors";
    return {};
  }

  size_t number_logged = 0;
  // assume object poses get logged in frame order!!!
  for (const auto& [object_id, poses_map] : propogated_poses) {
    // do not draw if in current frame
    if (!poses_map.exists(frame_id)) {
      VLOG(10) << "Cannot log object pose (id=" << object_id << ") for frame "
               << frame_id << " as it does not exist in the map";
      continue;
    }

    // use identity if no ground truth so that we keep the csv file format
    gtsam::Pose3 gt_L_world_k = gtsam::Pose3::Identity();
    const gtsam::Pose3& L_world_k = poses_map.at(frame_id);

    // gt packet provided so only log if gt pose data found
    if (gt_packets) {
      if (gt_packets->exists(frame_id)) {
        const GroundTruthInputPacket& gt_packet_k = gt_packets->at(frame_id);
        // check object exists in this frame
        ObjectPoseGT object_gt_k;
        if (!gt_packet_k.getObject(object_id, object_gt_k)) {
          // if no packet for this object found, continue and do not log
          continue;
        } else {
          gt_L_world_k = object_gt_k.L_world_;
        }
      } else {
        // gt packet has no entry for this frame id so skip
        continue;
      }
    }
    // we have either skipped logging (if gt provided by the object gt pose was
    // not found) or no gt was provided
    const auto& gt_R_k = gt_L_world_k.rotation().toQuaternion();
    // estimate
    const auto R_world_k = L_world_k.rotation().toQuaternion();
    // write out object pose with gt
    *object_pose_csv_ << frame_id << object_id << L_world_k.x() << L_world_k.y()
                      << L_world_k.z() << R_world_k.x() << R_world_k.y()
                      << R_world_k.z() << R_world_k.w() << gt_L_world_k.x()
                      << gt_L_world_k.y() << gt_L_world_k.z() << gt_R_k.x()
                      << gt_R_k.y() << gt_R_k.z() << gt_R_k.w();

    number_logged++;
  }
  return number_logged;
}

std::optional<size_t> EstimationModuleLogger::logCameraPose(
    FrameId frame_id, const gtsam::Pose3& T_world_camera,
    const std::optional<GroundTruthPacketMap>& gt_packets) {
  gtsam::Pose3 gt_T_world_camera_k = gtsam::Pose3::Identity();
  // if gt packet provided by no data exists at this frame
  if (gt_packets && !gt_packets->exists(frame_id)) {
    VLOG(100) << "No gt packet at frame id " << frame_id
              << ". Unable to log object motions";
    return {};
  } else if (gt_packets && gt_packets->exists(frame_id)) {
    const GroundTruthInputPacket& gt_packet_k = gt_packets->at(frame_id);
    gt_T_world_camera_k = gt_packet_k.X_world_;
  }

  const auto& rot = T_world_camera.rotation().toQuaternion();
  const auto& gt_rot = gt_T_world_camera_k.rotation().toQuaternion();
  *camera_pose_csv_ << frame_id << T_world_camera.x() << T_world_camera.y()
                    << T_world_camera.z() << rot.x() << rot.y() << rot.z()
                    << rot.w() << gt_T_world_camera_k.x()
                    << gt_T_world_camera_k.y() << gt_T_world_camera_k.z()
                    << gt_rot.x() << gt_rot.y() << gt_rot.z() << gt_rot.w();
  return 1;
}

void EstimationModuleLogger::logPoints(FrameId frame_id,
                                       const gtsam::Pose3& T_world_local_k,
                                       const StatusLandmarkVector& landmarks) {
  for (const auto& status_lmks : landmarks) {
    const TrackletId tracklet_id = status_lmks.trackletId();
    ObjectId object_id = status_lmks.objectId();
    Landmark lmk_world = status_lmks.value();

    if (status_lmks.referenceFrame() == ReferenceFrame::LOCAL) {
      lmk_world = T_world_local_k * status_lmks.value();
    } else if (status_lmks.referenceFrame() == ReferenceFrame::OBJECT) {
      throw DynosamException(
          "Cannot log object point in the object reference frame");
    }

    *map_points_csv_ << frame_id << object_id << tracklet_id << lmk_world(0)
                     << lmk_world(1) << lmk_world(2);
  }
}

void EstimationModuleLogger::logObjectBbxes(FrameId frame_id,
                                            const BbxPerObject& object_bbxes) {
  for (const auto& [object_id, this_object_bbx] : object_bbxes) {
    const gtsam::Quaternion& q = this_object_bbx.orientation_.toQuaternion();
    *object_bbx_csv_ << frame_id << object_id
                     << this_object_bbx.min_bbx_point_.x()
                     << this_object_bbx.min_bbx_point_.y()
                     << this_object_bbx.min_bbx_point_.z()
                     << this_object_bbx.max_bbx_point_.x()
                     << this_object_bbx.max_bbx_point_.y()
                     << this_object_bbx.max_bbx_point_.z()
                     << this_object_bbx.bbx_position_.x()
                     << this_object_bbx.bbx_position_.y()
                     << this_object_bbx.bbx_position_.z() << q.w() << q.x()
                     << q.y() << q.z();
  }
}

void EstimationModuleLogger::logFrameIdToTimestamp(FrameId frame_id,
                                                   Timestamp timestamp) {
  long int nano_seconds = timestamp * 1e+9;
  *frame_id_timestamp_csv_ << frame_id << nano_seconds;
}

}  // namespace dyno
````

## File: pipeline/Pipeline-Definitions.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/pipeline/Pipeline-Definitions.hpp"
#include "dynosam/pipeline/PipelineParams.hpp"
#include "dynosam/utils/YamlParser.hpp"
#include "dynosam/common/Types.hpp"


namespace dyno {


decltype(DynoParams::kPipelineFilename) constexpr DynoParams::kPipelineFilename;
decltype(DynoParams::kFrontendFilename) constexpr DynoParams::kFrontendFilename;
decltype(DynoParams::kBackendFilename) constexpr DynoParams::kBackendFilename;
decltype(DynoParams::kCameraFilename) constexpr DynoParams::kCameraFilename;


DynoParams::DynoParams(const std::string& params_folder_path)
    : DynoParams(params_folder_path + "/" + kPipelineFilename,
                 params_folder_path + "/" + kFrontendFilename,
                 params_folder_path + "/" + kBackendFilename,
                 params_folder_path + "/" + kCameraFilename)   {}

DynoParams::DynoParams(
               const std::string& pipeline_params_filepath,
               const std::string& frontend_params_filepath,
               const std::string& backend_params_filepath,
               const std::string& cam_params_filepath)
    :   PipelineParams("DynosamParams")
{
// //currently just camera params
    // camera_params_ = CameraParams::fromYamlFile(params_folder_path + "CameraParams.yaml");

    // YamlParser pipeline_parser(params_folder_path + "PipelineParams.yaml");
    // pipeline_parser.getYamlParam("parallel_run", &parallel_run_);

    // pipeline_parser.getYamlParam("data_provider_type", &data_provider_type_);

    // int frontend_type_i;
    // pipeline_parser.getYamlParam("frontend_type", &frontend_type_i);
    // frontend_type_ = static_cast<FrontendType>(frontend_type_i);

}

bool DynoParams::fromYamlFile(const std::string&) { return true; }
std::string DynoParams::toString() const {
    return "string";
}

bool DynoParams::equals(const PipelineParams& obj, double tol) const {
    return true;
}


} //dyno
````

## File: pipeline/PipelineBase.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/pipeline/PipelineBase.hpp"

#include "dynosam/utils/Timing.hpp"
#include "dynosam/utils/TimingStats.hpp"

#include <glog/logging.h>

#include <memory>
#include <thread>  // std::this_thread::sleep_for
#include <chrono>  // std::chrono::

namespace dyno {

bool PipelineBase::spin()
  {
    LOG(INFO) << "Starting module " << module_name_;
    while (!isShutdown())
    {
      spinOnce();
    //   using namespace std::chrono_literals;
    //   std::this_thread::sleep_for(1ms);  // give CPU thread some sleep time... //TODO: only if threaded?
    }
    return true;
  }

void PipelineBase::shutdown()
{
    LOG(INFO) << "Shutting down module " << module_name_;
    is_shutdown_ = true;
    shutdownQueues();
}

bool PipelineBase::isWorking() const
{
    return is_thread_working_ || hasWork();
}


void PipelineBase::notifyFailures(const PipelineBase::ReturnCode& result)
{
    for (const auto& failure_callbacks : on_failure_callbacks_)
    {
        if (failure_callbacks)
        {
            failure_callbacks(result);
        }
        else
        {
            LOG(ERROR) << "Invalid OnFailureCallback for module: " << module_name_;
        }
    }
}

void PipelineBase::registerOnFailureCallback(const OnPipelineFailureCallback& callback_)
{
    CHECK(callback_);
    on_failure_callbacks_.push_back(callback_);
}



} //dyno
````

## File: pipeline/PipelineManager.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include "dynosam/pipeline/PipelineManager.hpp"

#include <glog/logging.h>

#include "dynosam/backend/RGBDBackendModule.hpp"
#include "dynosam/common/Map.hpp"
#include "dynosam/frontend/RGBDInstanceFrontendModule.hpp"
#include "dynosam/logger/Logger.hpp"
#include "dynosam/utils/TimingStats.hpp"

DEFINE_bool(use_backend, false, "If any backend should be initalised");

namespace dyno {

DynoPipelineManager::DynoPipelineManager(const DynoParams& params,
                                         DataProvider::Ptr data_loader,
                                         FrontendDisplay::Ptr frontend_display,
                                         BackendDisplay::Ptr backend_display)
    : params_(params),
      use_offline_frontend_(FLAGS_frontend_from_file),
      data_loader_(std::move(data_loader)),
      displayer_(&display_queue_, params.parallelRun())

{
  LOG(INFO) << "Starting DynoPipelineManager";

  CHECK(data_loader_);
  CHECK(frontend_display);

  data_interface_ =
      std::make_unique<DataInterfacePipeline>(params_.parallelRun());
  data_loader_->registerImageContainerCallback(
      std::bind(&dyno::DataInterfacePipeline::fillImageContainerQueue,
                data_interface_.get(), std::placeholders::_1));

  // ground truth
  data_loader_->registerGroundTruthPacketCallback(
      std::bind(&dyno::DataInterfacePipeline::addGroundTruthPacket,
                data_interface_.get(), std::placeholders::_1));

  // preprocessing
  data_interface_->registerImageContainerPreprocessor(
      std::bind(&dyno::DataProvider::imageContainerPreprocessor,
                data_loader_.get(), std::placeholders::_1));

  // push data from the data interface to the frontend module
  data_interface_->registerOutputQueue(&frontend_input_queue_);

  CameraParams camera_params;
  if (params_.preferDataProviderCameraParams() &&
      data_loader_->getCameraParams().has_value()) {
    LOG(INFO) << "Using camera params from DataProvider, not the config in the "
                 "CameraParams.yaml!";
    camera_params = *data_loader_->getCameraParams();
  } else {
    LOG(INFO) << "Using camera params specified in CameraParams.yaml!";
    camera_params = params_.camera_params_;
  }

  loadPipelines(camera_params, frontend_display, backend_display);
  launchSpinners();
}

DynoPipelineManager::~DynoPipelineManager() {
  shutdownPipelines();
  shutdownSpinners();

  // TODO: make shutdown hook!
  writeStatisticsSamplesToFile("statistics_samples.csv");
  writeStatisticsModuleSummariesToFile();
}

void DynoPipelineManager::shutdownSpinners() {
  if (frontend_pipeline_spinner_) frontend_pipeline_spinner_->shutdown();

  if (backend_pipeline_spinner_) backend_pipeline_spinner_->shutdown();

  if (data_provider_spinner_) data_provider_spinner_->shutdown();

  if (frontend_viz_pipeline_spinner_)
    frontend_viz_pipeline_spinner_->shutdown();

  if (backend_viz_pipeline_spinner_) backend_viz_pipeline_spinner_->shutdown();
}

void DynoPipelineManager::shutdownPipelines() {
  display_queue_.shutdown();
  frontend_pipeline_->shutdown();

  if (backend_pipeline_) backend_pipeline_->shutdown();

  data_interface_->shutdown();

  if (frontend_viz_pipeline_) frontend_viz_pipeline_->shutdown();
  if (backend_viz_pipeline_) backend_viz_pipeline_->shutdown();
  if (backend_viz_pipeline_) backend_viz_pipeline_->shutdown();
}

bool DynoPipelineManager::spin() {
  std::function<bool()> spin_func;

  if (use_offline_frontend_) {
    // if we have an offline frontend only spiun the frontend pipeline
    // and no need to spin the viz (TODO: right now this is only images and not
    // the actual pipelines...)
    spin_func = [=]() -> bool {
      if (frontend_pipeline_->isWorking()) {
        if (!params_.parallelRun()) {
          frontend_pipeline_->spinOnce();
          if (backend_pipeline_) backend_pipeline_->spinOnce();
        }
        return true;
      }
      return false;
    };
  } else {
    // regular spinner....
    spin_func = [=]() -> bool {
      if (data_loader_->spin() || frontend_pipeline_->isWorking()) {
        if (!params_.parallelRun()) {
          frontend_pipeline_->spinOnce();
          if (backend_pipeline_) backend_pipeline_->spinOnce();
        }
        spinViz();  // for now
        // a later problem!
        return true;
      }
      return false;
    };
  }

  utils::TimingStatsCollector timer("pipeline_spin");
  return spin_func();
}

bool DynoPipelineManager::spinViz() {
  // if()
  displayer_
      .process();  // when enabled this gives a segafault when the process ends.
                   // when commented out the program just waits at thee end
  return true;
}

void DynoPipelineManager::launchSpinners() {
  LOG(INFO) << "Running PipelineManager with parallel_run="
            << params_.parallelRun();

  if (params_.parallelRun()) {
    frontend_pipeline_spinner_ = std::make_unique<dyno::Spinner>(
        std::bind(&dyno::FrontendPipeline::spin, frontend_pipeline_.get()),
        "frontend-pipeline-spinner");

    if (backend_pipeline_)
      backend_pipeline_spinner_ = std::make_unique<dyno::Spinner>(
          std::bind(&dyno::BackendPipeline::spin, backend_pipeline_.get()),
          "backend-pipeline-spinner");
  }

  data_provider_spinner_ = std::make_unique<dyno::Spinner>(
      std::bind(&dyno::DataInterfacePipeline::spin, data_interface_.get()),
      "data-interface-spinner");

  if (frontend_viz_pipeline_)
    frontend_viz_pipeline_spinner_ = std::make_unique<dyno::Spinner>(
        std::bind(&dyno::FrontendVizPipeline::spin,
                  frontend_viz_pipeline_.get()),
        "frontend-display-spinner");

  if (backend_viz_pipeline_)
    backend_viz_pipeline_spinner_ = std::make_unique<dyno::Spinner>(
        std::bind(&dyno::BackendVizPipeline::spin, backend_viz_pipeline_.get()),
        "backend-display-spinner");
}

void DynoPipelineManager::loadPipelines(const CameraParams& camera_params,
                                        FrontendDisplay::Ptr frontend_display,
                                        BackendDisplay::Ptr backend_display) {
  BackendModule::Ptr backend = nullptr;
  // the registra for the frontend pipeline
  // this is agnostic to the actual pipeline type so we can add/register
  // a new queue to it regardless of the derived type (as long as it is at least
  // a MIMO, which it should be as this is the lowest type of actual pipeline
  // with any functionality)
  typename FrontendPipeline::OutputRegistra::Ptr frontend_output_registra =
      nullptr;
  const auto parallel_run = params_.parallelRun();

  switch (params_.frontend_type_) {
    case FrontendType::kRGBD: {
      LOG(INFO) << "Making RGBDInstance frontend";

      using MapType = RGBDBackendModule::MapType;
      typename MapType::Ptr map = MapType::create();

      Camera::Ptr camera = std::make_shared<Camera>(camera_params);
      CHECK_NOTNULL(camera);

      if (use_offline_frontend_) {
        LOG(INFO) << "Offline RGBD frontend";
        using OfflineFrontend =
            FrontendOfflinePipeline<RGBDBackendModule::ModuleTraits>;
        const std::string file_path =
            getOutputFilePath(kRgbdFrontendOutputJsonFile);
        LOG(INFO) << "Loading RGBD frontend output packets from " << file_path;

        OfflineFrontend::UniquePtr offline_frontend =
            std::make_unique<OfflineFrontend>("offline-rgbdfrontend",
                                              file_path);
        // make registra so we can register queues with this pipeline
        frontend_output_registra = offline_frontend->getOutputRegistra();
        // raw ptr type becuase we cannot copy the unique ptr!! This is only
        // becuase we need it in the lambda function which is a temporary
        // solution
        OfflineFrontend* offline_frontend_ptr = offline_frontend.get();
        // set get dataset size function (bit of a hack for now, and only for
        // the batch optimizer so it knows when to optimize!!)
        get_dataset_size_ = [offline_frontend_ptr]() -> FrameId {
          // get frame id of the final frame saved
          return CHECK_NOTNULL(offline_frontend_ptr)
              ->getFrontendOutputPackets()
              .rbegin()
              ->first;
        };
        // convert pipeline to base type
        frontend_pipeline_ = std::move(offline_frontend);
      } else {
        FrontendModule::Ptr frontend =
            std::make_shared<RGBDInstanceFrontendModule>(
                params_.frontend_params_, camera, &display_queue_);
        LOG(INFO) << "Made RGBDInstanceFrontendModule";
        // need to make the derived pipeline so we can set parallel run etc
        // the manager takes a pointer to the base MIMO so we can have different
        // types of pipelines
        FrontendPipeline::UniquePtr frontend_pipeline_derived =
            std::make_unique<FrontendPipeline>(
                "frontend-pipeline", &frontend_input_queue_, frontend);
        // make registra so we can register queues with this pipeline
        frontend_output_registra =
            frontend_pipeline_derived->getOutputRegistra();
        frontend_pipeline_derived->parallelRun(parallel_run);
        // conver pipeline to base type
        frontend_pipeline_ = std::move(frontend_pipeline_derived);

        get_dataset_size_ = [=]() -> FrameId {
          CHECK(data_loader_) << "Data Loader is null when accessing "
                                 "get_last_frame_ in BatchOptimizerParams";
          return data_loader_->datasetSize();
        };
      }

      // right now depends on the get_dataset_size_ function being det before
      // the optimzier is created!!!

      if (FLAGS_use_backend) {
        LOG(INFO) << "Construcing RGBD backend";

        // TODO: make better params!!
        auto updater_type = static_cast<RGBDBackendModule::UpdaterType>(
            FLAGS_backend_updater_enum);

        params_.backend_params_.full_batch_frame = (int)get_dataset_size_();

        backend = std::make_shared<RGBDBackendModule>(params_.backend_params_,
                                                      map, camera, updater_type,
                                                      &display_queue_);
      } else if (use_offline_frontend_) {
        LOG(WARNING)
            << "FLAGS_use_backend is false but use_offline_frontend "
               "(FLAGS_frontend_from_file) us true. "
            << " Pipeline will load data from frontend but send it nowhere!!";
      }

    } break;
    case FrontendType::kMono: {
      LOG(FATAL) << "MONO Not implemented!";
    } break;

    default: {
      LOG(FATAL) << "Not implemented!";
    } break;
  }

  CHECK_NOTNULL(frontend_pipeline_);
  CHECK_NOTNULL(frontend_output_registra);
  // register output queue to send the front-end output to the viz
  frontend_output_registra->registerQueue(&frontend_viz_input_queue_);

  if (backend) {
    backend_pipeline_ = std::make_unique<BackendPipeline>(
        "backend-pipeline", &backend_input_queue_, backend);
    backend_pipeline_->parallelRun(parallel_run);
    // also register connection between front and back
    frontend_output_registra->registerQueue(&backend_input_queue_);

    backend_pipeline_->registerOutputQueue(&backend_output_queue_);
  }

  // right now we cannot use the viz when we load from file as do not load
  // certain data values (e.g. camera and debug info) so these will be null -
  // the viz's try and access these causing a seg fault. Just need to add checks
  if (!use_offline_frontend_) {
    if (backend && backend_display) {
      backend_viz_pipeline_ = std::make_unique<BackendVizPipeline>(
          "backend-viz-pipeline", &backend_output_queue_, backend_display);
    }
    frontend_viz_pipeline_ = std::make_unique<FrontendVizPipeline>(
        "frontend-viz-pipeline", &frontend_viz_input_queue_, frontend_display);
  }
}

}  // namespace dyno
````

## File: pipeline/PipelineParams.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/pipeline/PipelineParams.hpp"
#include "dynosam/utils/YamlParser.hpp"

#include <config_utilities/parsing/yaml.h>
#include <config_utilities/config_utilities.h>
#include <gflags/gflags.h>

DEFINE_int32(data_provider_type, 0,"Which data provider (loader) to use. Associated with specific datasets");

namespace dyno {

void declare_config(DynoParams::PipelineParams& config) {
    using namespace config;

    name("PipelineParams");
    field(config.parallel_run, "parallel_run");
    field(config.prefer_data_provider_camera_params, "prefer_data_provider_camera_params");
    // field(config.data_provider_type, "data_provider_type");

    config.data_provider_type = FLAGS_data_provider_type;
}

DynoParams::DynoParams(const std::string& params_folder_path) {

    pipeline_params_ = config::fromYamlFile<PipelineParams>(params_folder_path + "PipelineParams.yaml");
    camera_params_ = config::fromYamlFile<CameraParams>(params_folder_path + "CameraParams.yaml");
    frontend_params_ = config::fromYamlFile<FrontendParams>(params_folder_path + "FrontendParams.yaml");
}


void DynoParams::printAllParams(bool print_glog_params) const {

    LOG(INFO) << "Frontend Params: " << config::toString(frontend_params_);
    LOG(INFO) << "Pipeline Params: " << config::toString(pipeline_params_);

    //TODO: currently cannot print camera params becuase we use intermediate variables in the loading process!!
    // LOG(INFO) << "Camera Params: " << config::toString(camera_params_);

    if(print_glog_params) google::ShowUsageWithFlags("");
}


} //dyno
````

## File: pipeline/PipelineSpinner.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/pipeline/PipelineSpinner.hpp"
#include "dynosam/common/Exceptions.hpp"

#include <glog/logging.h>
#include <functional>

namespace dyno {

PipelineSpinner::~PipelineSpinner() {
    shutdownAll();
}

void PipelineSpinner::registerPipeline(PipelineBase::Ptr pipeline, bool parallel_run) {
    const std::string name = pipeline->getModuleName();
    checkAndThrow(!pipelines_.exists(name), "Cannot register pipeline with name " + name + " as it has already been registered");

    if(parallel_run) {
        pipelines_.insert({name, std::move(std::make_unique<AsynchronousSpinnerPair>(pipeline))});
    }
    else {
        pipelines_.insert({name, std::move(std::make_unique<SynchronousSpinnerPair>(pipeline))});
    }

    ordering_.push_back(name);
}

void PipelineSpinner::launchAll() {
     auto lambda = [](SpinnerPair::UniquePtr& spinner_pair) {
        spinner_pair->launch();
    };
    appyAll(lambda);

    is_launched_ = true;
}
void PipelineSpinner::spinOnceAll() {
    if(!is_launched_) {
        LOG(WARNING) << "spinOnceAll called but the pipelines have not been launched yet!";
    }

    auto lambda = [](SpinnerPair::UniquePtr& spinner_pair) {
        spinner_pair->spinOnce();
    };
    appyAll(lambda);
}

void PipelineSpinner::shutdownAll() {
     auto lambda = [](SpinnerPair::UniquePtr& spinner_pair) {
        spinner_pair->shutdown();
    };
    appyAll(lambda);
}

void PipelineSpinner::appyAll(std::function<void(SpinnerPair::UniquePtr&)> apply) {
    for(const std::string& name : ordering_) {
        SpinnerPair::UniquePtr& spinner_pair = pipelines_.at(name);
        apply(spinner_pair);
    }
}

PipelineSpinner::SynchronousSpinnerPair::SynchronousSpinnerPair(PipelineBase::Ptr p) : SpinnerPair(p) {}

bool PipelineSpinner::SynchronousSpinnerPair::spinOnce() {
    return p_->spinOnce();
}

void PipelineSpinner::SynchronousSpinnerPair::shutdown() {
    LOG(INFO) << "Shutting down pipeline: " + p_->getModuleName();
    //shutdown pipeline
    p_->shutdown();
}


PipelineSpinner::AsynchronousSpinnerPair::AsynchronousSpinnerPair(PipelineBase::Ptr p) : SpinnerPair(p) {}

void PipelineSpinner::AsynchronousSpinnerPair::launch() {
    CHECK(spinner_ == nullptr);

    const std::string spinner_name = p_->getModuleName() + "-spinner";
    spinner_ = std::make_unique<Spinner>(std::bind(&PipelineBase::spin, p_.get()), spinner_name);
    LOG(INFO) << "Launching spinner: " << spinner_name;
}


bool PipelineSpinner::AsynchronousSpinnerPair::spinOnce() {
    return spinner_->isRunning();
}

void PipelineSpinner::AsynchronousSpinnerPair::shutdown() {
    LOG(INFO) << "Shutting down pipeline and associated spinner: " + p_->getModuleName();
    //shutdown pipeline
    p_->shutdown();
    //then shutdown spinner
    spinner_->shutdown();
}


} //dyno
````

## File: utils/CsvParser.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include "dynosam/utils/CsvParser.hpp"

#include <glog/logging.h>

#include <boost/algorithm/string.hpp>
#include <iomanip>
#include <iostream>

namespace dyno {

CsvReader::Row::Row(const std::string& line, const CsvHeader& header,
                    const char delimiter)
    : line_(line), header_(header), delimiter_(delimiter) {
  pos_data_.emplace_back(-1);
  std::string::size_type pos = 0;
  while ((pos = line_.find(delimiter_, pos)) != std::string::npos) {
    pos_data_.emplace_back(pos);
    ++pos;
  }
  // This checks for a trailing comma with no data after it.
  pos = line_.size();
  pos_data_.emplace_back(pos);

  // if usable header provided (one of non zero size, ie. non default header)
  // check that the number of elements in the parsed input string matches the
  // expected number of cols in the header
  if (usableHeader() && this->size() != header_.size()) {
    std::stringstream ss;
    ss << "Invalid row construction using a valid CsvHeader: input row has "
          "length "
       << this->size() << " and header has size " << header_.size()
       << " with column values - " << header_.toString();
    throw std::runtime_error(ss.str());
  }
}

std::string CsvReader::Row::operator[](std::size_t index) const {
  size_t size = pos_data_[index + 1] - (pos_data_[index] + 1);
  std::string_view view = std::string_view(&line_[pos_data_[index] + 1], size);
  std::string val = {view.begin(), view.end()};

  // removes all the leading and trailing white spaces.
  boost::algorithm::trim(val);
  return val;
}

CsvReader::Row CsvReader::Row::FromStream(std::istream& istream,
                                          const CsvHeader& header,
                                          const char delimiter) {
  std::string line;
  std::getline(istream, line);
  return Row(line, header, delimiter);
}

CsvWriter::CsvWriter(const CsvHeader& header, const std::string& seperator)
    : header_(header),
      column_number_(header.size()),
      seperator_(seperator),
      value_count_(0),
      ss_() {
  checkAndThrow<InvalidCsvHeaderException>(
      (column_number_ > 0u), "CsvHeader cannot be empty! (size of 0)");
  ss_.precision(15);
}

CsvWriter::~CsvWriter() { resetContent(); }

CsvWriter& CsvWriter::add(const char* str) {
  return this->add(std::string(str));
}

CsvWriter& CsvWriter::add(char* str) { return this->add(std::string(str)); }

CsvWriter& CsvWriter::add(const std::string& str) {
  std::string str_cpy = str;
  // if " character was found, escape it
  size_t position = str_cpy.find("\"", 0);
  bool foundQuotationMarks = position != std::string::npos;
  while (position != std::string::npos) {
    str_cpy.insert(position, "\"");
    position = str_cpy.find("\"", position + 2);
  }
  if (foundQuotationMarks) {
    str_cpy = "\"" + str_cpy + "\"";
  } else if (str_cpy.find(this->seperator_) != std::string::npos) {
    // if seperator_ was found and string was not escapted before, surround
    // string with "
    str_cpy = "\"" + str_cpy + "\"";
  }
  return this->add<std::string>(str_cpy);
}

CsvWriter& CsvWriter::newRow() {
  ss_ << std::endl;
  value_count_ = 0;
  return *this;
}

bool CsvWriter::write(const std::string& filename) const {
  return write(filename, false);
}

bool CsvWriter::write(const std::string& filename, bool append) const {
  std::ofstream file;
  bool appendNewLine = false;
  if (append) {
    // check if last char of the file is newline
    std::ifstream fin;
    fin.open(filename);
    if (fin.is_open()) {
      fin.seekg(-1, std::ios_base::end);  // go to end of file
      int lastChar = fin.peek();
      if (lastChar != -1 &&
          lastChar !=
              '\n')  // if file is not empry and last char is not new line char
        appendNewLine = true;
    }
    file.open(filename.c_str(), std::ios::out | std::ios::app);
  } else {
    file.open(filename.c_str(), std::ios::out | std::ios::trunc);
  }
  if (!file.is_open()) return false;
  if (append && appendNewLine) file << std::endl;

  return write(file);
}

bool CsvWriter::write(std::ostream& stream) const {
  if (!stream) {
    LOG(ERROR) << "Failed to write with CsVWriter as stream was not good!";
    return false;
  }

  stream << header_.toString(seperator_);
  stream << std::endl;
  stream.precision(15);
  stream << toString();
  return stream.good();
}

void CsvWriter::resetContent() {
  const static std::stringstream initial;
  ss_.str(std::string());
  ss_.clear();
  ss_.copyfmt(initial);
}

}  // namespace dyno
````

## File: utils/GtsamUtils.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/utils/GtsamUtils.hpp"

#include <eigen3/Eigen/Dense>

#include <opencv4/opencv2/core/eigen.hpp>

#include <glog/logging.h>

namespace dyno {
namespace utils {

// TODO: unit test
gtsam::Pose3 cvMatToGtsamPose3(const cv::Mat& H)
{
  CHECK_EQ(H.rows, 4);
  CHECK_EQ(H.cols, 4);

  cv::Mat R(3, 3, H.type());
  cv::Mat T(3, 1, H.type());

  for (int i = 0; i < 3; i++)
  {
    for (int j = 0; j < 3; j++)
    {
      R.at<double>(i, j) = H.at<double>(i, j);
    }
  }

  for (int i = 0; i < 3; i++)
  {
    T.at<double>(i, 0) = H.at<double>(i, 3);
  }

  return cvMatsToGtsamPose3(R, T);
}

gtsam::Pose3 cvMatsToGtsamPose3(const cv::Mat& R, const cv::Mat& T)
{
  return gtsam::Pose3(cvMatToGtsamRot3(R), cvMatToGtsamPoint3(T));
}

cv::Mat gtsamPose3ToCvMat(const gtsam::Pose3& pose)
{
  cv::Mat RT(4, 4, CV_64F);
  cv::eigen2cv(pose.matrix(), RT);
  RT.convertTo(RT, CV_64F);
  return RT;
}

gtsam::Rot3 cvMatToGtsamRot3(const cv::Mat& R)
{
  CHECK_EQ(R.rows, 3);
  CHECK_EQ(R.cols, 3);
  gtsam::Matrix rot_mat = gtsam::Matrix::Identity(3, 3);
  cv::cv2eigen(R, rot_mat);
  return gtsam::Rot3(rot_mat);
}

gtsam::Point3 cvMatToGtsamPoint3(const cv::Mat& cv_t)
{
  CHECK_EQ(cv_t.rows, 3);
  CHECK_EQ(cv_t.cols, 1);
  gtsam::Point3 gtsam_t;
  gtsam_t << cv_t.at<double>(0, 0), cv_t.at<double>(1, 0), cv_t.at<double>(2, 0);
  return gtsam_t;
}

cv::Mat gtsamPoint3ToCvMat(const gtsam::Point3& point)
{
  cv::Mat T(3, 1, CV_32F);
  cv::eigen2cv(point, T);
  T.convertTo(T, CV_32F);
  return T.clone();
}


gtsam::Pose3 poseVectorToGtsamPose3(const std::vector<double>& vector_pose) {
  CHECK_EQ(vector_pose.size(), 16u);
  CHECK_EQ(vector_pose[12], 0.0);
  CHECK_EQ(vector_pose[13], 0.0);
  CHECK_EQ(vector_pose[14], 0.0);
  CHECK_EQ(vector_pose[15], 1.0);
  return gtsam::Pose3(Eigen::Matrix4d(vector_pose.data()).transpose());
}

gtsam::Cal3_S2 Cvmat2Cal3_S2(const cv::Mat& M) {
  CHECK_EQ(M.rows, 3);  // We expect homogeneous camera matrix.
  CHECK_GE(M.cols, 3);  // We accept extra columns (which we do not use).
  const double& fx = M.at<double>(0, 0);
  const double& fy = M.at<double>(1, 1);
  const double& s = M.at<double>(0, 1);
  const double& u0 = M.at<double>(0, 2);
  const double& v0 = M.at<double>(1, 2);
  return gtsam::Cal3_S2(fx, fy, s, u0, v0);
}

gtsam::Pose3 openGvTfToGtsamPose3(const opengv::transformation_t& RT) {
  gtsam::Matrix poseMat = gtsam::Matrix::Identity(4, 4);
  poseMat.block<3, 4>(0, 0) = RT;
  return gtsam::Pose3(poseMat);
}

std::pair<cv::Mat, cv::Mat> Pose2cvmats(const gtsam::Pose3& pose) {
  const gtsam::Matrix3& rot = pose.rotation().matrix();
  const gtsam::Vector3& tran = pose.translation();
  return std::make_pair(gtsamMatrix3ToCvMat(rot), gtsamVector3ToCvMat(tran));
}

// TODO(Toni): template this on type double, float etc.
cv::Mat gtsamMatrix3ToCvMat(const gtsam::Matrix3& rot) {
  cv::Mat R = cv::Mat(3, 3, CV_64F);
  cv::eigen2cv(rot, R);
  return R;
}

cv::Mat gtsamVector3ToCvMat(const gtsam::Vector3& tran) {
  cv::Mat T = cv::Mat(3, 1, CV_64F);
  cv::eigen2cv(tran, T);
  return T;
}

cv::Point3d gtsamVector3ToCvPoint3(const gtsam::Vector3& tran) {
  return cv::Point3d(tran[0], tran[1], tran[2]);
}

} //utils
} //dyno
````

## File: utils/Histogram.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/utils/Histogram.hpp"

namespace dyno {

using json = nlohmann::json;

void to_json(json& j, const Histogram& histogram) {
    // json bin;
    // bin["upper"]
    // j[histogram.name_]

    auto impl_hist = histogram.histogram_;

    std::vector<std::vector<json>> bins(impl_hist.rank());

    for (auto&& x : bh::indexed(impl_hist)) {
        for (size_t i = 0; i < impl_hist.rank(); ++i) {

            json bin;
            bin["lower"] = x.bin(i).lower();
            bin["upper"] = x.bin(i).upper();

            std::stringstream ss;
            ss << *x;

            double value;
            ss >> value;
            bin["count"] = value;

            bins.at(i).push_back(bin);

        }
    }

    j[histogram.name_] = bins;

}

void from_json(const json& j, Histogram& histogram) {

}


} //dyno
````

## File: utils/JsonUtils.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include "dynosam/utils/JsonUtils.hpp"

#include "dynosam/frontend/FrontendOutputPacket.hpp"
#include "dynosam/frontend/RGBDInstance-Definitions.hpp"

#define DYNO_FRONTEND_OUTPUT_PACKET_BASE_TO_JSON(jason, input)       \
  jason["frontend_type"] = input.frontend_type_;                     \
  jason["static_keypoints"] = input.static_keypoint_measurements_;   \
  jason["dynamic_keypoints"] = input.dynamic_keypoint_measurements_; \
  jason["T_world_camera"] = input.T_world_camera_;                   \
  jason["timestamp"] = input.getTimestamp();                         \
  jason["frame_id"] = input.getFrameId();                            \
  jason["ground_truth"] = input.gt_packet_;

namespace nlohmann {

// void adl_serializer<dyno::FrontendOutputPacketBase>::to_json(json& j, const
// dyno::FrontendOutputPacketBase& input) {
//     DYNO_FRONTEND_OUTPUT_PACKET_BASE_TO_JSON(j, input)
// }

// dyno::FrontendOutputPacketBase
// adl_serializer<dyno::FrontendOutputPacketBase>::from_json(const json& j) {
//     using namespace dyno;

//     StatusKeypointMeasurements static_keypoints =
//     j["static_keypoints"].template get<StatusKeypointMeasurements>();
//     StatusKeypointMeasurements dynamic_keypoints =
//     j["dynamic_keypoints"].template get<StatusKeypointMeasurements>();
//     gtsam::Pose3 T_world_camera = j["T_world_camera"].template
//     get<gtsam::Pose3>(); Timestamp timestamp = j["timestamp"].template
//     get<Timestamp>(); FrameId frame_id = j["frame_id"].template
//     get<FrameId>(); GroundTruthInputPacket::Optional gt_packet =
//     j["ground_truth"].template get<GroundTruthInputPacket::Optional>();

// }

void adl_serializer<dyno::RGBDInstanceOutputPacket>::to_json(
    json& j, const dyno::RGBDInstanceOutputPacket& input) {
  using namespace dyno;

  DYNO_FRONTEND_OUTPUT_PACKET_BASE_TO_JSON(j, input)
  j["static_landmarks"] = input.static_landmarks_;
  j["dynamic_landmarks"] = input.dynamic_landmarks_;
  j["estimated_motions"] = input.estimated_motions_;
  j["propogated_object_poses"] = input.propogated_object_poses_;
  j["camera_poses"] = input.camera_poses_;
}

dyno::RGBDInstanceOutputPacket
adl_serializer<dyno::RGBDInstanceOutputPacket>::from_json(const json& j) {
  using namespace dyno;

  StatusKeypointVector static_keypoints =
      j["static_keypoints"].template get<StatusKeypointVector>();
  StatusKeypointVector dynamic_keypoints =
      j["dynamic_keypoints"].template get<StatusKeypointVector>();

  gtsam::Pose3 T_world_camera =
      j["T_world_camera"].template get<gtsam::Pose3>();

  Timestamp timestamp = j["timestamp"].template get<Timestamp>();
  FrameId frame_id = j["frame_id"].template get<FrameId>();

  StatusLandmarkVector static_landmarks =
      j["static_landmarks"].template get<StatusLandmarkVector>();
  StatusLandmarkVector dynamic_landmarks =
      j["dynamic_landmarks"].template get<StatusLandmarkVector>();
  // Base is a std::map with the right custom allocation
  // we use the std::map version so that all the automagic with nlohmann::json
  // can work
  MotionEstimateMap estimated_motions(
      j["estimated_motions"].template get<MotionEstimateMap::Base>());
  ObjectPoseMap propogated_object_poses =
      j["propogated_object_poses"].template get<ObjectPoseMap>();
  gtsam::Pose3Vector camera_poses =
      j["camera_poses"].template get<gtsam::Pose3Vector>();

  GroundTruthInputPacket::Optional gt_packet =
      j["ground_truth"].template get<GroundTruthInputPacket::Optional>();

  return dyno::RGBDInstanceOutputPacket(
      static_keypoints, dynamic_keypoints, static_landmarks, dynamic_landmarks,
      T_world_camera, timestamp, frame_id, estimated_motions,
      propogated_object_poses, camera_poses, nullptr, /* no camera*/
      gt_packet, std::nullopt                         /* no debug imagery */
  );
}

}  // namespace nlohmann
````

## File: utils/Metrics.cc
````
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/utils/Metrics.hpp"

namespace dyno::utils {

TRErrorPair::TRErrorPair(double translation, double rot) : translation_(translation), rot_(rot) {}

double TRErrorPair::getRotationErrorDegrees() const {
    return dyno::rads2Deg(rot_);
}

TRErrorPair TRErrorPair::CalculatePoseError(const gtsam::Pose3& M, const gtsam::Pose3& M_hat) {
    const gtsam::Pose3 E = M.inverse() * M_hat;

    // L2 norm - ie. magnitude
    const double t_e = E.translation().norm();

    const gtsam::Matrix33 rotation_error = E.rotation().matrix();
    std::pair<gtsam::Unit3, double> axis_angle = gtsam::Rot3(rotation_error).axisAngle();

    //need to wrap?
    const double r_e = axis_angle.second;

    return TRErrorPair(t_e, r_e);
}

TRErrorPair TRErrorPair::CalculateRelativePoseError(const gtsam::Pose3& M_ref,
                                                const gtsam::Pose3& M_curr,
                                                const gtsam::Pose3& M_hat_ref,
                                                const gtsam::Pose3& M_hat_curr) {
    // pose change between ref and current
    const gtsam::Pose3 ref_T_curr = M_ref.inverse() * M_curr;
    const gtsam::Pose3 ref_T_hat_curr = M_hat_ref.inverse() * M_hat_curr;
    return CalculatePoseError(ref_T_curr, ref_T_hat_curr);
}

void TRErrorPairVector::push_back(double translation, double rot) {
    Base::push_back(TRErrorPair(translation, rot));
}

TRErrorPair TRErrorPairVector::average() const
{
    if (Base::empty())
    {
        return TRErrorPair();
    }

    TRErrorPair error_pair;

    double x = 0, y = 0;
    for (const TRErrorPair& pair : *this)
    {
        error_pair.translation_ += pair.translation_;

        y += std::sin(pair.rot_);
        x += std::cos(pair.rot_);
    }

    const double length = static_cast<double>(this->size());
    x /= length;
    y /= length;
    error_pair.rot_ = dyno::wrapTwoPi(std::atan2(y, x));
    error_pair.translation_ /= length;

    return error_pair;
}


} //dyno::utils
````

## File: utils/OpenCVUtils.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/common/Types.hpp" //for template to_string
#include "dynosam/utils/OpenCVUtils.hpp"
#include "dynosam/visualizer/ColourMap.hpp"

#include <config_utilities/config_utilities.h>
#include <opencv4/opencv2/opencv.hpp>
#include <glog/logging.h>

#include<iostream>
#include<fstream>

namespace dyno {



template<>
std::string to_string<cv::Size>(const cv::Size& t) {
  return "[h=" + std::to_string(t.height) + " w=" + std::to_string(t.width) + "]";
}

template<>
std::string to_string<cv::Rect>(const cv::Rect& t) {
  return "[x=]" + std::to_string(t.x) + " y=" + std::to_string(t.y) + " h=" + std::to_string(t.height) + " w=" + std::to_string(t.width) + "]";
}


namespace utils {


void drawCircleInPlace(cv::Mat& img, const cv::Point2d& point, const cv::Scalar& colour, const double msize)
{
  cv::circle(img, point, msize, colour, 2);
}

std::string cvTypeToString(int type)
{
  std::string r;
  uchar depth = type & CV_MAT_DEPTH_MASK;
  uchar chans = 1 + (type >> CV_CN_SHIFT);
  switch (depth)
  {
    case CV_8U:
      r = "8U";
      break;
    case CV_8S:
      r = "8S";
      break;
    case CV_16U:
      r = "16U";
      break;
    case CV_16S:
      r = "16S";
      break;
    case CV_32S:
      r = "32S";
      break;
    case CV_32F:
      r = "32F";
      break;
    case CV_64F:
      r = "64F";
      break;
    default:
      r = "User";
      break;
  }
  r += "C";
  r += (chans + '0');
  return r;
}


std::string cvTypeToString(const cv::Mat& mat)
{
  return cvTypeToString(mat.type());
}

cv::Mat concatenateImagesHorizontally(const cv::Mat& left_img, const cv::Mat& right_img)
{
  cv::Mat left_img_tmp = left_img.clone();
  if (left_img_tmp.channels() == 1)
  {
    cv::cvtColor(left_img_tmp, left_img_tmp, cv::COLOR_GRAY2BGR);
  }
  cv::Mat right_img_tmp = right_img.clone();
  if (right_img_tmp.channels() == 1)
  {
    cv::cvtColor(right_img_tmp, right_img_tmp, cv::COLOR_GRAY2BGR);
  }

  cv::Size left_img_size = left_img_tmp.size();
  cv::Size right_img_size = right_img_tmp.size();

  CHECK_EQ(left_img_size.height, left_img_size.height) << "Cannot concat horizontally if images are not the same "
                                                          "height";

  cv::Mat dual_img(left_img_size.height, left_img_size.width + right_img_size.width, CV_8UC3);

  cv::Mat left(dual_img, cv::Rect(0, 0, left_img_size.width, left_img_size.height));
  left_img_tmp.copyTo(left);

  cv::Mat right(dual_img, cv::Rect(left_img_size.width, 0, right_img_size.width, right_img_size.height));

  right_img_tmp.copyTo(right);
  return dual_img;
}

cv::Mat concatenateImagesVertically(const cv::Mat& top_img, const cv::Mat& bottom_img)
{
  cv::Mat top_img_tmp = top_img.clone();
  if (top_img_tmp.channels() == 1)
  {
    cv::cvtColor(top_img_tmp, top_img_tmp, cv::COLOR_GRAY2BGR);
  }
  cv::Mat bottom_img_tmp = bottom_img.clone();
  if (bottom_img_tmp.channels() == 1)
  {
    cv::cvtColor(bottom_img_tmp, bottom_img_tmp, cv::COLOR_GRAY2BGR);
  }

  cv::Size top_img_size = bottom_img_tmp.size();
  cv::Size bottom_img_size = bottom_img_tmp.size();

  CHECK_EQ(top_img_size.width, bottom_img_size.width) << "Cannot concat vertically if images are not the same width";

  cv::Mat dual_img(top_img_size.height + bottom_img_size.height, top_img_size.width, CV_8UC3);

  cv::Mat top(dual_img, cv::Rect(0, 0, top_img_size.width, top_img_size.height));
  top_img_tmp.copyTo(top);

  cv::Mat bottom(dual_img, cv::Rect(0, top_img_size.height, bottom_img_size.width, bottom_img_size.height));

  bottom_img_tmp.copyTo(bottom);
  return dual_img;
}


void flowToRgb(const cv::Mat& flow, cv::Mat& rgb) {
  CHECK(flow.channels() == 2) << "Expecting flow in frame to have 2 channels";

  // Visualization part
  cv::Mat flow_parts[2];
  cv::split(flow, flow_parts);

  // Convert the algorithm's output into Polar coordinates
  cv::Mat magnitude, angle, magn_norm;
  cv::cartToPolar(flow_parts[0], flow_parts[1], magnitude, angle, true);
  cv::normalize(magnitude, magn_norm, 0.0f, 1.0f, cv::NORM_MINMAX);
  angle *= ((1.f / 360.f) * (180.f / 255.f));

  // Build hsv image
  cv::Mat _hsv[3], hsv, hsv8, bgr;
  _hsv[0] = angle;
  _hsv[1] = cv::Mat::ones(angle.size(), CV_32F);
  _hsv[2] = magn_norm;
  cv::merge(_hsv, 3, hsv);
  hsv.convertTo(hsv8, CV_8U, 255.0);

  // Display the results
  cv::cvtColor(hsv8, rgb, cv::COLOR_HSV2BGR);
}


cv::Mat labelMaskToRGB(const cv::Mat& mask, int background_label, const cv::Mat& rgb) {
  CHECK(mask.channels() == 1) << "Expecting mask input to have channels 1";
  CHECK(rgb.channels() == 3) << "Expecting rgb input to have channels 3";
  cv::Mat mask_viz;
  rgb.copyTo(mask_viz);

  for (int i = 0; i < mask.rows; i++)
  {
    for (int j = 0; j < mask.cols; j++)
    {
      // background is zero
      if (mask.at<int>(i, j) != background_label)
      {

        cv::Scalar color = Color::uniqueId(mask.at<int>(i, j));
        // rgb or bgr?
        mask_viz.at<cv::Vec3b>(i, j)[0] = color[0];
        mask_viz.at<cv::Vec3b>(i, j)[1] = color[1];
        mask_viz.at<cv::Vec3b>(i, j)[2] = color[2];
      }
    }
  }

  return mask_viz;
}


cv::Mat labelMaskToRGB(const cv::Mat& mask, int background_label) {
  cv::Mat rgb = cv::Mat::zeros(mask.size(), CV_8UC3);
  return labelMaskToRGB(mask, background_label, rgb);
}

void getDisparityVis(cv::InputArray src, cv::OutputArray dst, int unknown_disparity) {
  CHECK(!src.empty() && (src.depth() == CV_16S || src.depth() == CV_32F) &&
        (src.channels() == 1));
  // cv::Mat srcMat = src.getMat();
  cv::Mat srcMat = src.getMat();
  dst.create(srcMat.rows, srcMat.cols, CV_8UC1);
  cv::Mat& dstMat = dst.getMatRef();

  // Check its extreme values.
  double min_val;
  double max_val;
  cv::minMaxLoc(src, &min_val, &max_val);

  // Multiply by 1.25 just to saturate a bit the extremums.
  double scale = 2.0 * 255.0 / (max_val - min_val);
  srcMat.convertTo(dstMat, CV_8UC1, scale / 16.0);
  dstMat &= (srcMat != unknown_disparity);
}

void drawLabeledBoundingBox(cv::Mat& image, const std::string& label, const cv::Scalar& colour, const cv::Rect& bounding_box) {
  constexpr static double kFontScale = 0.6;
  constexpr static int kFontFace = cv::FONT_HERSHEY_SIMPLEX;
  constexpr static int kThickness = 1;

  // Top left corner.
  const cv::Point& tlc = bounding_box.tl();

  // Display the label at the top of the bounding box.
  int base_line;
  cv::Size label_size = cv::getTextSize(label, kFontFace, kFontScale, kThickness, &base_line);

  //draw on filled black rectangle that the object label will then be drawn over to make it easier to see
  constexpr static int pixel_buffer = 2; //pixel buffer around the rectangle
  const cv::Point tlc_black_rectangle(tlc.x, tlc.y - label_size.height - pixel_buffer);
  const cv::Point brc_black_rectangle(tlc.x + label_size.width + pixel_buffer, tlc.y);
  cv::rectangle(image, tlc_black_rectangle, brc_black_rectangle, cv::Scalar(0,0,0), -1);
  // Put the label on the black rectangle.
  // Note: text origin starts from the bottom left corner of the text string in the image and we add a pixel buffer along y to make it look better
  cv::putText(image, label, cv::Point(tlc.x, tlc.y - 2), kFontFace, kFontScale, cv::Scalar(255, 255, 255), kThickness);
  //draw bounding box with line thickness kThickness
  cv::rectangle(image, bounding_box, colour, 2);
}



bool compareCvMatsUpToTol(const cv::Mat& mat1, const cv::Mat& mat2, const double& tol)
{
  CHECK_EQ(mat1.size(), mat2.size());
  CHECK_EQ(mat1.type(), mat2.type());

  // treat two empty mat as identical as well
  if (mat1.empty() && mat2.empty())
  {
    LOG(WARNING) << "CvMatCmp: asked comparison of 2 empty matrices.";
    return true;
  }

  // Compare the two matrices!
  cv::Mat diff = mat1 - mat2;
  return cv::checkRange(diff, true, nullptr, -tol, tol);
}



const float FLOW_TAG_FLOAT = 202021.25f;
const char *FLOW_TAG_STRING = "PIEH";

cv::Mat readOpticalFlow( const std::string& path ) {
    using namespace cv;

    Mat_<Point2f> flow;
    std::ifstream file(path.c_str(), std::ios_base::binary);
    if ( !file.good() )
        return CV_CXX_MOVE(flow); // no file - return empty matrix

    float tag;
    file.read((char*) &tag, sizeof(float));
    if ( tag != FLOW_TAG_FLOAT )
        return CV_CXX_MOVE(flow);

    int width, height;

    file.read((char*) &width, 4);
    file.read((char*) &height, 4);

    flow.create(height, width);

    for ( int i = 0; i < flow.rows; ++i )
    {
        for ( int j = 0; j < flow.cols; ++j )
        {
            Point2f u;
            file.read((char*) &u.x, sizeof(float));
            file.read((char*) &u.y, sizeof(float));
            if ( !file.good() )
            {
                flow.release();
                return CV_CXX_MOVE(flow);
            }

            flow(i, j) = u;
        }
    }
    file.close();
    return CV_CXX_MOVE(flow);

}


bool writeOpticalFlow( const std::string& path, const cv::Mat& flow) {
    using namespace cv;
    const int nChannels = 2;

    Mat input = flow;
    if ( input.channels() != nChannels || input.depth() != CV_32F || path.length() == 0 )
        return false;

    std::ofstream file(path.c_str(), std::ofstream::binary);
    if ( !file.good() )
        return false;

    int nRows, nCols;

    nRows = (int) input.size().height;
    nCols = (int) input.size().width;

    const int headerSize = 12;
    char header[headerSize];
    memcpy(header, FLOW_TAG_STRING, 4);
    // size of ints is known - has been asserted in the current function
    memcpy(header + 4, reinterpret_cast<const char*>(&nCols), sizeof(nCols));
    memcpy(header + 8, reinterpret_cast<const char*>(&nRows), sizeof(nRows));
    file.write(header, headerSize);
    if ( !file.good() )
        return false;

//    if ( input.isContinuous() ) //matrix is continous - treat it as a single row
//    {
//        nCols *= nRows;
//        nRows = 1;
//    }

    int row;
    char* p;
    for ( row = 0; row < nRows; row++ )
    {
        p = input.ptr<char>(row);
        file.write(p, nCols * nChannels * sizeof(float));
        if ( !file.good() )
            return false;
    }
    file.close();
    return true;
}

} //utils
} //dyno
````

## File: utils/Spinner.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/utils/Spinner.hpp"

#include <glog/logging.h>
#include <chrono>

using namespace std::chrono_literals;


namespace dyno {


Spinner::Spinner(Evoke func, const std::string& name, bool auto_start) : func_(func), name_(name)
{
  VLOG(10) << "Creating spinner - " << name;

  if (auto_start)
  {
    VLOG(10) << "Auto starting spinner - " << name;
    start();
  }
}

Spinner::~Spinner()
{
  shutdown();
}

void Spinner::start()
{

  if(!is_running_) {
    spin_thread_ = std::make_unique<std::thread>(func_);
    is_running_ = true;
  }
}

void Spinner::shutdown()
{
  if (spin_thread_ && spin_thread_->joinable())
  {
    VLOG(10) << "Spinner " << name_ << " shutting down";
    spin_thread_->join();
    is_running_ = false;
  }
  else
  {
    // VLOG(10) << "Spinner " << name_ << "already shutdown";
  }
}

void LoopingSpinner::shutdown()
{
  is_shutdown_ = true;
  std::this_thread::sleep_for(100ms);
  Spinner::shutdown();
}

void LoopingSpinner::spin()
{
  while (!is_shutdown_)
  {
    spinOnce_();
    std::this_thread::sleep_for(delay_);
  }
}


}
````

## File: utils/Statistics.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

/********************************************************************************
 Copyright 2017 Autonomous Systems Lab, ETH Zurich, Switzerland

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
*********************************************************************************/

/* ----------------------------------------------------------------------------
 * Copyright 2017, Massachusetts Institute of Technology,
 * Cambridge, MA 02139
 * All Rights Reserved
 * Authors: Luca Carlone, et al. (see THANKS for the full author list)
 * See LICENSE for the license information
 * -------------------------------------------------------------------------- */

/**
 * @file   Statistics.cpp
 * @brief  For logging statistics in a thread-safe manner.
 * @author Antoni Rosinol
 */

#include "dynosam/utils/Statistics.hpp"
#include "dynosam/utils/CsvParser.hpp"

#include <cmath>
#include <fstream>
#include <iomanip>
#include <ostream>
#include <sstream>

#include <filesystem>

namespace dyno {


namespace fs = std::filesystem;

namespace utils {

Statistics& Statistics::Instance() {
  static Statistics instance;
  return instance;
}

Statistics::Statistics() : max_tag_length_(0) {}

Statistics::~Statistics() {}

// Static functions to query the stats collectors:
size_t Statistics::GetHandle(std::string const& tag) {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  // Search for an existing tag.
  map_t::iterator i = Instance().tag_map_.find(tag);
  if (i == Instance().tag_map_.end()) {
    // If it is not there, create a tag.
    size_t handle = Instance().stats_collectors_.size();
    Instance().tag_map_[tag] = handle;
    Instance().stats_collectors_.push_back(StatisticsMapValue());
    // Track the maximum tag length to help printing a table of values later.
    Instance().max_tag_length_ =
        std::max(Instance().max_tag_length_, tag.size());
    return handle;
  } else {
    return i->second;
  }
}

// Return true if a handle has been initialized for a specific tag.
// In contrast to GetHandle(), this allows testing for existence without
// modifying the tag/handle map.
bool Statistics::HasHandle(std::string const& tag) {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  bool tag_found = Instance().tag_map_.count(tag);
  return tag_found;
}

std::string Statistics::GetTag(size_t handle) {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  std::string tag;
  // Perform a linear search for the tag.
  for (typename map_t::value_type current_tag : Instance().tag_map_) {
    if (current_tag.second == handle) {
      return current_tag.first;
    }
  }
  return tag;
}

StatsCollectorImpl::StatsCollectorImpl(size_t handle) : handle_(handle) {}

StatsCollectorImpl::StatsCollectorImpl(std::string const& tag)
    : handle_(Statistics::GetHandle(tag)) {}

size_t StatsCollectorImpl::GetHandle() const { return handle_; }
void StatsCollectorImpl::AddSample(double sample) const {
  Statistics::Instance().AddSample(handle_, sample);
}
void StatsCollectorImpl::IncrementOne() const {
  Statistics::Instance().AddSample(handle_, 1.0);
}

std::vector<std::string> Statistics::getTagByModule(std::string const& query_module) {
  std::vector<std::string> modules;
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  for(const auto&[tag, _] : GetStatsCollectors()) {

    std::optional<std::string> module = getModuleNameFromTag(tag);

    //if query module is empty match with those tags in the global namespace
    if(query_module.empty() && !module) {
      modules.push_back(tag);
    }
    else if(module && *module == query_module) {
      modules.push_back(tag);
    }
  }
  return modules;
}


void Statistics::AddSample(size_t handle, double seconds) {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  stats_collectors_[handle].AddValue(seconds);
}
double Statistics::GetLastValue(size_t handle) {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  return Instance().stats_collectors_[handle].GetLastValue();
}
double Statistics::GetLastValue(std::string const& tag) {
  return GetLastValue(GetHandle(tag));
}
double Statistics::GetTotal(size_t handle) {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  return Instance().stats_collectors_[handle].Sum();
}
double Statistics::GetTotal(std::string const& tag) {
  return GetTotal(GetHandle(tag));
}
double Statistics::GetMean(size_t handle) {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  return Instance().stats_collectors_[handle].Mean();
}
double Statistics::GetMean(std::string const& tag) {
  return GetMean(GetHandle(tag));
}
size_t Statistics::GetNumSamples(size_t handle) {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  return Instance().stats_collectors_[handle].TotalSamples();
}
size_t Statistics::GetNumSamples(std::string const& tag) {
  return GetNumSamples(GetHandle(tag));
}
std::vector<double> Statistics::GetAllSamples(size_t handle) {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  return Instance().stats_collectors_[handle].GetAllValues();
}
std::vector<double> Statistics::GetAllSamples(std::string const& tag) {
  return GetAllSamples(GetHandle(tag));
}
double Statistics::GetVariance(size_t handle) {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  return Instance().stats_collectors_[handle].LazyVariance();
}
double Statistics::GetVariance(std::string const& tag) {
  return GetVariance(GetHandle(tag));
}
double Statistics::GetMin(size_t handle) {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  return Instance().stats_collectors_[handle].Min();
}
double Statistics::GetMin(std::string const& tag) {
  return GetMin(GetHandle(tag));
}
double Statistics::GetMax(size_t handle) {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  return Instance().stats_collectors_[handle].Max();
}
double Statistics::GetMax(std::string const& tag) {
  return GetMax(GetHandle(tag));
}
double Statistics::GetMedian(size_t handle) {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  return Instance().stats_collectors_[handle].Median();
}
double Statistics::GetMedian(std::string const& tag) {
  return GetMedian(GetHandle(tag));
}
double Statistics::GetQ1(size_t handle) {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  return Instance().stats_collectors_[handle].Q1();
}
double Statistics::GetQ1(std::string const& tag) {
  return GetQ1(GetHandle(tag));
}
double Statistics::GetQ3(size_t handle) {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  return Instance().stats_collectors_[handle].Q1();
}
double Statistics::GetQ3(std::string const& tag) {
  return GetQ3(GetHandle(tag));
}
double Statistics::GetHz(size_t handle) {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  return Instance().stats_collectors_[handle].MeanCallsPerSec();
}
double Statistics::GetHz(std::string const& tag) {
  return GetHz(GetHandle(tag));
}

// Delta time statistics.
double Statistics::GetMeanDeltaTime(std::string const& tag) {
  return GetMeanDeltaTime(GetHandle(tag));
}
double Statistics::GetMeanDeltaTime(size_t handle) {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  return Instance().stats_collectors_[handle].MeanDeltaTime();
}
double Statistics::GetMaxDeltaTime(std::string const& tag) {
  return GetMaxDeltaTime(GetHandle(tag));
}
double Statistics::GetMaxDeltaTime(size_t handle) {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  return Instance().stats_collectors_[handle].MaxDeltaTime();
}
double Statistics::GetMinDeltaTime(std::string const& tag) {
  return GetMinDeltaTime(GetHandle(tag));
}
double Statistics::GetMinDeltaTime(size_t handle) {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  return Instance().stats_collectors_[handle].MinDeltaTime();
}
double Statistics::GetLastDeltaTime(std::string const& tag) {
  return GetLastDeltaTime(GetHandle(tag));
}
double Statistics::GetLastDeltaTime(size_t handle) {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  return Instance().stats_collectors_[handle].GetLastDeltaTime();
}
double Statistics::GetVarianceDeltaTime(std::string const& tag) {
  return GetVarianceDeltaTime(GetHandle(tag));
}
double Statistics::GetVarianceDeltaTime(size_t handle) {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  return Instance().stats_collectors_[handle].LazyVarianceDeltaTime();
}

std::string Statistics::SecondsToTimeString(double seconds) {
  double secs = fmod(seconds, 60);
  int minutes = (seconds / 60);
  int hours = (seconds / 3600);
  minutes = minutes - (hours * 60);

  char buffer[256];
  snprintf(buffer,
           sizeof(buffer),
#ifdef SM_TIMING_SHOW_HOURS
           "%02d:"
#endif
#ifdef SM_TIMING_SHOW_MINUTES
           "%02d:"
#endif
           "%09.6f",
#ifdef SM_TIMING_SHOW_HOURS
           hours,
#endif
#ifdef SM_TIMING_SHOW_MINUTES
           minutes,
#endif
           secs);
  return buffer;
}

void Statistics::Print(std::ostream& out) {  // NOLINT
  const map_t& tag_map = Instance().tag_map_;

  if (tag_map.empty()) {
    return;
  }

  out << "Statistics\n";

  out.width((std::streamsize)Instance().max_tag_length_);
  out.setf(std::ios::left, std::ios::adjustfield);
  out << "-----------";
  out.width(7);
  out.setf(std::ios::right, std::ios::adjustfield);
  out << "#\t";
  out << "Log Hz\t";
  out << "{avg     +- std    }\t";
  out << "[min,max]\n";

  for (const typename map_t::value_type& t : tag_map) {
    size_t i = t.second;
    out.width((std::streamsize)Instance().max_tag_length_);
    out.setf(std::ios::left, std::ios::adjustfield);
    // Print Name of tag
    out << t.first << "\t";

    out.setf(std::ios::right, std::ios::adjustfield);
    // Print #
    out << std::setw(5) << GetNumSamples(i) << "\t";
    if (GetNumSamples(i) > 0) {
      out << std::showpoint << GetHz(i) << "\t";
      double mean = GetMean(i);
      double stddev = sqrt(GetVariance(i));
      out << "{" << std::showpoint << mean;
      out << " +- ";
      out << std::showpoint << stddev << "}\t";

      double min_value = GetMin(i);
      double max_value = GetMax(i);

      //out.width(5);
      out << std::noshowpoint << "[" << min_value << "," << max_value << "]";
    }
    out << std::endl;
  }
}

void Statistics::WriteAllSamplesToCsvFile(const std::string& path) {
  const map_t& tag_map = Instance().tag_map_;
  if (tag_map.empty()) {
    return;
  }

  static const CsvHeader header("label", "samples");
  CsvWriter writer(header);


  VLOG(1) << "Writing statistics to file: " << path;
  for (const map_t::value_type& tag : tag_map) {
    const size_t& index = tag.second;
    if (GetNumSamples(index) > 0) {
      const std::string& label = tag.first;

      // // Add header to csv file. tag.first is the stats label.
      writer << label;

      // Each row has all samples.
      std::stringstream ss;
      const std::vector<double>& samples = GetAllSamples(index);
      for (const auto& sample : samples) {
        //make space separated so not to confuse the CsvWriter
        //treat all samples as a single column
        ss << ' ' << sample;
      }
      writer << ss.str();
    }
  }

  writer.write(path);
}


void Statistics::WriteSummaryToCsvFile(const std::string &path) {
  const map_t& tag_map = Instance().tag_map_;
  if (tag_map.empty()) {
    return;
  }

  static const CsvHeader header("label", "num samples", "log Hz", "mean", "stddev", "min", "max");
  CsvWriter writer(header);


  for (const map_t::value_type& tag : tag_map) {
    SummaryWriterHelper(writer, tag);
  }

  writer.write(path);

}

void Statistics::WriteToYamlFile(const std::string& path) {
  std::ofstream output_file(path);

  if (!output_file) {
    LOG(ERROR) << "Could not write statistics: Unable to open file: " << path;
    return;
  }

  const map_t& tag_map = Instance().tag_map_;
  if (tag_map.empty()) {
    return;
  }

  VLOG(1) << "Writing statistics to file: " << path;
  for (const map_t::value_type& tag : tag_map) {
    const size_t index = tag.second;

    if (GetNumSamples(index) > 0) {
      std::string label = tag.first;

      // We do not want colons or hashes in a label, as they might interfere
      // with reading the yaml later.
      std::replace(label.begin(), label.end(), ':', '_');
      std::replace(label.begin(), label.end(), '#', '_');

      output_file << label << ":\n";
      output_file << "  samples: " << GetNumSamples(index) << "\n";
      output_file << "  mean: " << GetMean(index) << "\n";
      output_file << "  stddev: " << sqrt(GetVariance(index)) << "\n";
      output_file << "  min: " << GetMin(index) << "\n";
      output_file << "  max: " << GetMax(index) << "\n";
      output_file << "  median: " << GetMedian(index) << "\n";
      output_file << "  q1: " << GetQ1(index) << "\n";
      output_file << "  q3: " << GetQ3(index) << "\n";
    }
    output_file << "\n";
  }
}

void Statistics::WritePerModuleSummariesToCsvFile(const std::string& folder_path) {
  const map_t& tag_map = Instance().tag_map_;
  if (tag_map.empty()) {
    return;
  }

  static const CsvHeader header("label", "num samples", "log Hz", "mean", "stddev", "min", "max");
  std::map<std::string, CsvWriter::Ptr> module_to_writer;

  CsvWriter global_writer(header);

  for (const map_t::value_type& tag : tag_map) {
    std::optional<std::string> module_name = getModuleNameFromTag(tag.first);

    // writer to use
    CsvWriter* writer;
    if(module_name) {
      //tag is not in global namespace so use the specificied namespace
      //if we dont have a writer for this module, make one
      if(module_to_writer.find(*module_name) == module_to_writer.end()) {
        module_to_writer[*module_name] = std::make_shared<CsvWriter>(header);
      }

      //set writer to use
      writer = module_to_writer.at(*module_name).get();
    }
    else {
      //no module name so use the global namespace
      writer = &global_writer;
    }
    CHECK_NOTNULL(writer);
    SummaryWriterHelper(*writer, tag);

  }

  //write out all summaries
  for(auto& [module_name, writer] : module_to_writer) {
    std::string full_path = fs::path(folder_path) / std::string("stats_" + module_name + ".csv");
    LOG(INFO) << "Writing out csv stats module: " << full_path;
    writer->write(full_path);
  }

  {
    std::string full_path = fs::path(folder_path) / "stats_global.csv";
    LOG(INFO) << "Writing out csv stats module: " << full_path;
    global_writer.write(full_path);
  }

}

std::string Statistics::Print() {
  std::stringstream ss;
  Print(ss);
  return ss.str();
}

void Statistics::Reset() {
  std::lock_guard<std::mutex> lock(Instance().mutex_);
  Instance().tag_map_.clear();
}


std::optional<std::string> Statistics::getModuleNameFromTag(const std::string& tag) {
  size_t pos = tag.find('.');
  if (pos != std::string::npos) {
      return tag.substr(0, pos);
  }
  return {};
}

}  // namespace utils

}  // namespace dyno
````

## File: utils/TimingStats.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/utils/TimingStats.hpp"

namespace dyno {
namespace utils {

TimingStatsCollector::TimingStatsCollector(const std::string& tag)
    :   tic_time_(Timer::tic()), collector_(tag + " [ms]") {}


TimingStatsCollector::~TimingStatsCollector() {
    tocAndLog();
}

void TimingStatsCollector::reset() {
    tic_time_ = Timer::tic();
    is_valid_ = true;
}

bool TimingStatsCollector::isValid() const {
    return is_valid_;
}

void TimingStatsCollector::tocAndLog() {
    if(is_valid_) {
        auto toc = Timer::toc<std::chrono::nanoseconds>(tic_time_);
        auto milliseconds = std::chrono::duration_cast<std::chrono::milliseconds>(toc);
        collector_.AddSample( static_cast<double>(milliseconds.count()));
        is_valid_ = false;
    }
}

} //utils
} //dyno
````

## File: visualizer/ColourMap.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/visualizer/ColourMap.hpp"

#include <algorithm>
#include <random>

namespace dyno {

std::random_device rd;
std::mt19937 gen(rd());
std::uniform_int_distribution<uint8_t> dis(0, 255);


bool Color::operator==(const Color& other) const {
  return r == other.r && g == other.g && b == other.b && a == other.a;
}

bool Color::operator<(const Color& other) const {
  if (r != other.r) {
    return r < other.r;
  }
  if (g != other.g) {
    return g < other.g;
  }
  if (b != other.b) {
    return b < other.b;
  }
  return a < other.a;
}

void Color::merge(const Color& other, float weight) { *this = blend(other, weight); }

Color Color::blend(const Color& other, float weight) const {
  weight = std::clamp(weight, 0.0f, 1.0f);
  return Color(static_cast<uint8_t>(r * (1.0f - weight) + other.r * weight),
               static_cast<uint8_t>(g * (1.0f - weight) + other.g * weight),
               static_cast<uint8_t>(b * (1.0f - weight) + other.b * weight),
               static_cast<uint8_t>(a * (1.0f - weight) + other.a * weight));
}

Color Color::random() { return Color(dis(gen), dis(gen), dis(gen), dis(gen)); }

std::array<float, 3> rgbFromChromaHue(float chroma, float hue) {
  const float hue_prime = hue * 6.0f;
  const float x = chroma * (1.0f - std::abs(std::fmod(hue_prime, 2) - 1.0f));
  if (hue_prime < 1) {
    return {chroma, x, 0};
  } else if (hue_prime < 2) {
    return {x, chroma, 0};
  } else if (hue_prime < 3) {
    return {0, chroma, x};
  } else if (hue_prime < 4) {
    return {0, x, chroma};
  } else if (hue_prime < 5) {
    return {x, 0, chroma};
  }
  return {chroma, 0, x};
}

Color Color::fromHSV(float hue, float saturation, float value) {
  hue = std::clamp(hue, 0.0f, 1.0f);
  saturation = std::clamp(saturation, 0.0f, 1.0f);
  value = std::clamp(value, 0.0f, 1.0f);
  const float chroma = value * saturation;
  const auto [r1, g1, b1] = rgbFromChromaHue(chroma, hue);
  const float m = value - chroma;
  return Color(static_cast<uint8_t>((r1 + m) * 255),
               static_cast<uint8_t>((g1 + m) * 255),
               static_cast<uint8_t>((b1 + m) * 255));
}

Color Color::fromHLS(float hue, float luminance, float saturation) {
  hue = std::clamp(hue, 0.0f, 1.0f);
  luminance = std::clamp(luminance, 0.0f, 1.0f);
  saturation = std::clamp(saturation, 0.0f, 1.0f);
  const float chroma = (1.0f - std::abs(2.0f * luminance - 1.0f)) * saturation;
  const auto [r1, g1, b1] = rgbFromChromaHue(chroma, hue);
  const float m = luminance - chroma / 2.0f;
  return Color(static_cast<uint8_t>((r1 + m) * 255),
               static_cast<uint8_t>((g1 + m) * 255),
               static_cast<uint8_t>((b1 + m) * 255));
}

Color Color::gray(float value) {
  value = std::clamp(value, 0.0f, 1.0f);
  return Color(static_cast<uint8_t>(value * 255),
               static_cast<uint8_t>(value * 255),
               static_cast<uint8_t>(value * 255));
}

Color Color::quality(float value) {
  value = std::clamp(value, 0.0f, 1.0f);
  Color color;
  if (value > 0.5f) {
    color.r = (1.f - value) * 2 * 255;
    color.g = 255;
  } else {
    color.r = 255;
    color.g = value * 2 * 255;
  }
  return color;
}

Color Color::spectrum(float value, const std::vector<Color>& colors) {
  if (colors.empty()) {
    return Color::black();
  }
  value = std::clamp(value, 0.0f, 1.0f);
  const size_t num_steps = colors.size() - 1;
  const size_t index = static_cast<size_t>(value * num_steps);
  if (index >= num_steps) {
    return colors.at(num_steps);
  }
  const float weight = value * num_steps - index;
  return colors.at(index).blend(colors.at(index + 1), weight);
}

Color Color::ironbow(float value) { return spectrum(value, ironbow_colors_); }

Color Color::rainbow(float value) {
  return fromHLS(std::clamp(value, 0.0f, 1.0f), 0.5f, 1.0f);
}

/**
 * @brief Map a potentially infinite number of ids to a never repeating pattern in [0,
 * 1].
 */
float exponentialOffsetId(size_t id, size_t ids_per_revolution) {
  const size_t revolution = id / ids_per_revolution;
  const float progress_along_revolution =
      std::fmod(static_cast<float>(id) / ids_per_revolution, 1.f);
  float offset = 0.0f;
  if (ids_per_revolution < id + 1u) {
    const size_t current_episode = std::floor(std::log2(revolution));
    const size_t episode_start = std::exp2(current_episode);
    const size_t current_subdivision = revolution - episode_start;
    const float subdivision_step_size = 1.0f / (ids_per_revolution * 2 * episode_start);
    offset = (2.0f * current_subdivision + 1) * subdivision_step_size;
  }
  return progress_along_revolution + offset;
}

Color Color::rainbowId(size_t id, size_t ids_per_revolution) {
  return Color::rainbow(exponentialOffsetId(id, ids_per_revolution));
}

Color Color::uniqueId(size_t id, float saturation, float value) {
    constexpr static float phi = (1 + std::sqrt(5.0))/2.0;
    auto n = (float)id * phi - std::floor((float)id * phi);

    //want in 255 mode
    float hue = std::floor(n * 256.0) / 255.0;
    return Color::fromHSV(hue, saturation, value);
}

const std::vector<Color> Color::ironbow_colors_ = {
    {0, 0, 0}, {145, 20, 145}, {255, 138, 0}, {255, 230, 40}, {255, 255, 255}};

std::ostream& operator<<(std::ostream& out, const Color& color) {
  out << "[RGBA: " << static_cast<int>(color.r) << ", " << static_cast<int>(color.g)
      << ", " << static_cast<int>(color.b) << ", " << static_cast<int>(color.a) << "]";
  return out;
}



} //dyno
````

## File: visualizer/OpenCVFrontendDisplay.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/visualizer/OpenCVFrontendDisplay.hpp"

#include <opencv4/opencv2/opencv.hpp>
#include <glog/logging.h>

namespace dyno {

void OpenCVFrontendDisplay::spinOnce(const FrontendOutputPacketBase::ConstPtr& frontend_output) {
    LOG(INFO) << "In frontend opencv display";

    // cv::Mat input_frames;
    // if(frontend_output.input) {
    //     frontend_output.input->image_packet_->draw(input_frames);
    // }

    // cv::imshow("Input Frames", input_frames);
    // cv::waitKey(1);
}


} //dyno
````

## File: visualizer/Visualizer-Definitions.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include "dynosam/visualizer/Visualizer-Definitions.hpp"

#include <glog/logging.h>

#include <opencv4/opencv2/opencv.hpp>

namespace dyno {

OpenCVImageDisplayQueue::OpenCVImageDisplayQueue(
    ImageDisplayQueue* display_queue, bool parallel_run)
    : display_queue_(CHECK_NOTNULL(display_queue)),
      parallel_run_(parallel_run) {}

void OpenCVImageDisplayQueue::process() {
  bool queue_state = false;
  if (parallel_run_) {
    ImageToDisplay image_to_display;
    queue_state = display_queue_->popBlocking(image_to_display);

    if (queue_state) {
      // cv::namedWindow(image_to_display.name_);
      cv::imshow(image_to_display.name_, image_to_display.image_);
      cv::waitKey(1);  // Not needed because we are using startWindowThread()
    }
  } else {
    std::vector<ImageToDisplay> images_to_display;
    queue_state = display_queue_->popAll(images_to_display, 3);

    if (queue_state) {
      for (const auto& image_to_display : images_to_display) {
        cv::imshow(image_to_display.name_, image_to_display.image_);
      }
      cv::waitKey(1);
    }
  }
}

}  // namespace dyno
````

## File: visualizer/VisualizerPipelines.cc
````
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/visualizer/VisualizerPipelines.hpp"

namespace dyno {}
````

## File: dynosam_tests.md
````markdown
This file is a merged representation of the entire codebase, combined into a single document by Repomix.

# File Summary

## Purpose
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)

# Directory Structure
```
algorithms/
  test_algorithms.cc
backend/
  test_backend_structures.cc
  test_dynamic_point_symbol.cc
  test_factors.cc
  test_landmark_quadric_factor.cc
  test_rgbd_backend.cc
  test_triangulation.cc
camera/
  ZEDCamera/
    test_ZED_camera_init.cc
  test_camera_params.cc
  test_camera.cc
core_utils/
  test_code_concepts.cc
  test_csv.cc
  test_histogram.cc
  test_numerical.cc
  test_structured_containers.cc
  test_types.cc
data/
  sensor.yaml
  tracking_params.yaml
dataproviders/
  ZEDDataProvider/
    test_ZED_data_provider_conversion_and_mask.cc
  test_dataset_provider.cc
internal/
  helpers.hpp
  simulator.cc
  simulator.hpp
  tmp_file.cc
  tmp_file.hpp
pipelines_params/
  test_params.cc
  test_pipelines.cc
thread_safety/
  test_threadsafe_imu_buffer.cc
  thread_safe_queue_tests.cc
  thread_safe_temporal_buffer_test.cc
vision_map/
  test_map.cc
  test_tools.cc
visualization/
  test_viz.cc
test_main.cc
```

# Files

## File: algorithms/test_algorithms.cc
```
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/common/Algorithms.hpp"

#include <glog/logging.h>
#include <gtest/gtest.h>
#include <exception>

#include <cmath>

using namespace dyno;

// Taken from https://www.hungarianalgorithm.com/examplehungarianalgorithm.php
TEST(Algorithms, HungarianSanityCheckSquareMatrix) {
    gtsam::Matrix44 costs;
    costs << 82, 83, 69, 92,
             77, 37, 49, 92,
             11, 69, 5, 86,
             8, 9, 98, 23;

    Eigen::VectorXi assignment;
    const double assigned_cost = internal::HungarianAlgorithm().solve(costs, assignment);

    EXPECT_EQ(assigned_cost, 140.0);

    //answer should be assignment of:
    // W1 -> J3, row0 -> col2
    // W2 -> J2, row1 -> col1
    // W3 -> J1, row2 -> col0,
    // W4 -> J4, row3 -> col3
    Eigen::VectorXd expected_assignment(4);
    expected_assignment << 2, 1, 0, 3;

    Eigen::VectorXd assignmentd = assignment.cast<double>();
    //need to cast to Eigen::VectorXd to make gtsam templating/traits happy as
    //all their matrix operations are templated on double's!
    EXPECT_TRUE(gtsam::assert_equal(expected_assignment, assignmentd));

}

TEST(Algorithms, HungarianSanityCheckMoreJobs) {
    gtsam::Matrix45 costs;
    costs << 82, 83, 69, 92, 23,
             77, 37, 49, 92, 7,
             11, 69, 5, 86, 9,
             8, 9, 98, 23, 85;

    Eigen::VectorXi assignment;
    internal::HungarianAlgorithm().solve(costs, assignment);

    Eigen::VectorXd expected_assignment(4);
    expected_assignment << 2, 1, 0, 3;

    Eigen::VectorXd assignmentd = assignment.cast<double>();
    EXPECT_EQ(assignment.rows(), 4u);

}

TEST(Algorithms, HungarianTestSimpleArgMax) {
    //a simple assignment where we want to find the MAX score
    //testing that we just use -log in the score
    gtsam::Matrix33 costs;
    costs << 3, 12, 20,
             2, 51, 4,
             33, 14, 5;

    //assignment should be
    //W1 -> J3
    //W2 -> J2
    //W3 -> J1
    //apply scalign to the costs to turn the cost function from an argmax to an argmin problem
    //which the hungrian problem sovles
    gtsam::Matrix33 loged_costs = costs.unaryExpr([](double x) { return 1.0/x * 10; });

    LOG(INFO) << loged_costs;

    Eigen::VectorXi assignment;
    internal::HungarianAlgorithm().solve(loged_costs, assignment);

    Eigen::VectorXd expected_assignment(3);
    expected_assignment << 2, 1, 0;

    Eigen::VectorXd assignmentd = assignment.cast<double>();
    EXPECT_TRUE(gtsam::assert_equal(expected_assignment, assignmentd));

}

TEST(Algorithms, OptimalAssignment) {

}
```

## File: backend/test_backend_structures.cc
```
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/backend/BackendDefinitions.hpp"
#include "dynosam/backend/FactorGraphTools.hpp"

using namespace dyno;


#include <glog/logging.h>
#include <gtest/gtest.h>


TEST(DynamicObjectSymbol, testReconstructMotionInfo) {

    gtsam::Key motion_key = ObjectMotionSymbol(12, 10);

    ObjectId recovered_object_id;
    FrameId recovered_frame_id;
    EXPECT_TRUE(reconstructMotionInfo(motion_key, recovered_object_id, recovered_frame_id));

    EXPECT_EQ(recovered_object_id, 12);
    EXPECT_EQ(recovered_frame_id, 10);

}

TEST(DynamicObjectSymbol, testReconstrucPoseInfo) {

    gtsam::Key pose_key = ObjectPoseSymbol(12, 12);

    ObjectId recovered_object_id;
    FrameId recovered_frame_id;
    EXPECT_TRUE(reconstructPoseInfo(pose_key, recovered_object_id, recovered_frame_id));

    EXPECT_EQ(recovered_object_id, 12);
    EXPECT_EQ(recovered_frame_id, 12);

}

TEST(DynamicObjectSymbol, testReconstructMotionInfoFromOtherSymbol) {

    {
        gtsam::Key motion_key = CameraPoseSymbol(10);

        ObjectId recovered_object_id;
        FrameId recovered_frame_id;
        EXPECT_FALSE(reconstructMotionInfo(motion_key, recovered_object_id, recovered_frame_id));
    }

    {
        gtsam::Key pose_key = ObjectPoseSymbol(10, 12);
        ObjectId recovered_object_id;
        FrameId recovered_frame_id;
        EXPECT_FALSE(reconstructMotionInfo(pose_key, recovered_object_id, recovered_frame_id));
    }
}

TEST(BackendDefinitions, testDynoChrExtractor) {
    gtsam::Key motion_key = ObjectMotionSymbol(12, 10);
    EXPECT_EQ(DynoChrExtractor(motion_key), kObjectMotionSymbolChar);

    gtsam::Key cam_pose_key = CameraPoseSymbol(2);
    EXPECT_EQ(DynoChrExtractor(cam_pose_key), kPoseSymbolChar);

    gtsam::Key dynamic_point_key = DynamicLandmarkSymbol(2, 10);
    EXPECT_EQ(DynoChrExtractor(dynamic_point_key), kDynamicLandmarkSymbolChar);

    gtsam::Key static_point_key = StaticLandmarkSymbol(2);
    EXPECT_EQ(DynoChrExtractor(static_point_key), kStaticLandmarkSymbolChar);
}

// TEST(FactorGraphTools, testReconstructMotionInfoFromOtherSymbol) {

//     gtsam::Key motion_key = CameraPoseSymbol(10);

//     ObjectId recovered_object_id;
//     FrameId recovered_frame_id;
//     EXPECT_FALSE(reconstructMotionInfo(motion_key, recovered_object_id, recovered_frame_id));
// }
```

## File: backend/test_dynamic_point_symbol.cc
```
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/backend/DynamicPointSymbol.hpp"


#include <glog/logging.h>
#include <gtest/gtest.h>
#include <exception>

using namespace dyno;

TEST(CantorPairingFunction, basicPair)
{
    //checking that examples provided on wikipedia are correct (ie imeplementation is good)
    //https://en.wikipedia.org/wiki/Pairing_function#Examples
    // EXPECT_EQ(CantorPairingFunction::pair({47, 32}), 3192);

}

TEST(CantorPairingFunction, basicZip)
{
    //checking that examples provided on wikipedia are correct (ie imeplementation is good)
    //https://en.wikipedia.org/wiki/Pairing_function#Examples

    // const auto z = CantorPairingFunction::unzip(1432);
    // EXPECT_EQ(z.first, 52);
    // EXPECT_EQ(z.second, 1);

}

TEST(CantorPairingFunction, testAccess)
{
    const auto x = 15;
    const auto y = 79;

    const auto z = CantorPairingFunction::pair({x, y});
    const auto result = CantorPairingFunction::depair(z);
    EXPECT_EQ(result.first, x);
    EXPECT_EQ(result.second, y);

}

TEST(CantorPairingFunction, testReconstructionSpecialCase)
{
    const auto x = 46528; //tracklet fails at runtime. Special test for this case

    const auto z = CantorPairingFunction::pair({x, 1});
    const auto result = CantorPairingFunction::depair(z);
    EXPECT_EQ(result.first, x);
    EXPECT_EQ(result.second, 1);

}


TEST(DynamicPointSymbol, testReconstruction)
{
    const auto x = 15;
    const auto y = 79;

    DynamicPointSymbol dps('m', x, y);
    gtsam::Symbol sym(dps);

    gtsam::Key key(sym);
    DynamicPointSymbol reconstructed_dps(key);
    EXPECT_EQ(dps, reconstructed_dps);
    EXPECT_EQ(x, reconstructed_dps.trackletId());
    EXPECT_EQ(y, reconstructed_dps.frameId());

}

TEST(DynamicPointSymbol, testReconstructionSpecialCase)
{
    const TrackletId bad_id = 46528; //tracklet fails at runtime. Special test for this case
    // const TrackletId bad_id = 46000; //tracklet fails at runtime. Special test for this case

    DynamicPointSymbol dps('m', bad_id, 0);
    gtsam::Symbol sym(dps);

    gtsam::Key key(sym);
    DynamicPointSymbol reconstructed_dps(key);
    EXPECT_EQ(dps, reconstructed_dps);
    EXPECT_EQ(bad_id, reconstructed_dps.trackletId());

}
```

## File: backend/test_factors.cc
```
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/factors/LandmarkMotionPoseFactor.hpp"
#include "dynosam/factors/Pose3FlowProjectionFactor.h"
#include "dynosam/factors/LandmarkMotionTernaryFactor.hpp"
#include "dynosam/utils/GtsamUtils.hpp"

#include "internal/helpers.hpp"


#include "dynosam/backend/FactorGraphTools.hpp"
#include "dynosam/backend/BackendDefinitions.hpp"

#include <gtsam/geometry/Pose3.h>
#include <gtsam/nonlinear/NonlinearFactorGraph.h>
#include <gtsam/base/numericalDerivative.h>

#include <glog/logging.h>
#include <gtest/gtest.h>
#include <exception>

using namespace dyno;


TEST(LandmarkMotionPoseFactor, visualiseJacobiansWithNonZeros) {

    gtsam::Pose3 L1(gtsam::Rot3::Rodrigues(-0.1, 0.2, 0.25),
                            gtsam::Point3(0.05, -0.10, 0.20));

    gtsam::Pose3 L2(gtsam::Rot3::Rodrigues(0.3, 0.2, -0.5),
                            gtsam::Point3(0.5, -0.15, 0.1));

    gtsam::Point3 p1(0.1, 2, 4);
    gtsam::Point3 p2(0.2, 3, 2);

    auto object_pose_k_1_key = ObjectPoseSymbol(0, 0);
    auto object_pose_k_key = ObjectPoseSymbol(0, 1);

    auto object_point_key_k_1 = DynamicLandmarkSymbol(0, 1);
    auto object_point_key_k = DynamicLandmarkSymbol(1, 1);

    LOG(INFO) << (std::string)object_point_key_k_1;

    gtsam::Values values;
    values.insert(object_pose_k_1_key, L1);
    values.insert(object_pose_k_key, L2);
    values.insert(object_point_key_k_1, p1);
    values.insert(object_point_key_k, p2);

    auto landmark_motion_noise = gtsam::noiseModel::Isotropic::Sigma(3u, 0.1);

    gtsam::NonlinearFactorGraph graph;
    graph.emplace_shared<LandmarkMotionPoseFactor>(
                        object_point_key_k_1,
                        object_point_key_k,
                        object_pose_k_1_key,
                        object_pose_k_key,
                        landmark_motion_noise
                    );

    NonlinearFactorGraphManager nlfgm(graph, values);

    cv::Mat block_jacobians = nlfgm.drawBlockJacobian(
        gtsam::Ordering::OrderingType::COLAMD,
        factor_graph_tools::DrawBlockJacobiansOptions::makeDynoSamOptions());

    // cv::imshow("LandmarkMotionPoseFactor block jacobians", block_jacobians);
    // cv::waitKey(0);

}

TEST(Pose3FlowProjectionFactor, testJacobians) {

    gtsam::Pose3 previous_pose = utils::createRandomAroundIdentity<gtsam::Pose3>(0.4);
    static gtsam::Pose3 kDeltaPose(gtsam::Rot3::Rodrigues(-0.1, 0.2, 0.25),
                            gtsam::Point3(0.05, -0.10, 0.20));
    gtsam::Pose3 current_pose = previous_pose * kDeltaPose;

    gtsam::Point2 kp(1.2, 2.4);
    double depth = 0.5;
    gtsam::Point2 flow(0.1, -0.3);

    auto noise = gtsam::noiseModel::Isotropic::Sigma(2u, 0.1);

    auto camera_params = dyno_testing::makeDefaultCameraParams();
    gtsam::Cal3_S2 calibration = camera_params.constructGtsamCalibration<gtsam::Cal3_S2>();

    Pose3FlowProjectionFactor<gtsam::Cal3_S2> factor(
        0,
        1,
        kp,
        depth,
        previous_pose,
        calibration,
        noise
    );

    gtsam::Matrix H1, H2;
    gtsam::Vector error = factor.evaluateError(flow, current_pose, H1, H2);

    //now do numerical jacobians
    gtsam::Matrix numerical_H1 =
            gtsam::numericalDerivative21<gtsam::Vector2, gtsam::Point2, gtsam::Pose3>(
                std::bind(&Pose3FlowProjectionFactor<gtsam::Cal3_S2>::evaluateError, &factor,
                std::placeholders::_1, std::placeholders::_2, boost::none, boost::none),
            flow, current_pose);

    gtsam::Matrix numerical_H2 =
            gtsam::numericalDerivative22<gtsam::Vector2, gtsam::Point2, gtsam::Pose3>(
                std::bind(&Pose3FlowProjectionFactor<gtsam::Cal3_S2>::evaluateError, &factor,
                std::placeholders::_1, std::placeholders::_2,boost::none, boost::none),
            flow, current_pose);

    EXPECT_TRUE(gtsam::assert_equal(H1, numerical_H1, 1e-4));
    EXPECT_TRUE(gtsam::assert_equal(H2, numerical_H2, 1e-4));



}


TEST(LandmarkMotionTernaryFactor, testJacobians) {

    gtsam::Pose3 H(gtsam::Rot3::Rodrigues(-0.1, 0.2, 0.25),
                            gtsam::Point3(0.05, -0.10, 0.20));
    gtsam::Pose3 HPerturbed = utils::perturbWithNoise<gtsam::Pose3>(H, 0.3);


   gtsam::Point3 P1(0.4, 1.0, 0.8);
   gtsam::Point3 P2 = H * P1;

    auto noise = gtsam::noiseModel::Isotropic::Sigma(3u, 0.1);

    LandmarkMotionTernaryFactor factor(0, 1, 2,noise);

    gtsam::Matrix H1, H2, H3;
    gtsam::Vector error = factor.evaluateError(P1, P2, HPerturbed, H1, H2, H3);

    //now do numerical jacobians
    gtsam::Matrix numerical_H1 =
            gtsam::numericalDerivative31<gtsam::Vector3, gtsam::Point3, gtsam::Point3, gtsam::Pose3>(
                std::bind(&LandmarkMotionTernaryFactor::evaluateError, &factor,
                std::placeholders::_1, std::placeholders::_2, std::placeholders::_3, boost::none, boost::none, boost::none),
            P1, P2, HPerturbed);

   gtsam::Matrix numerical_H2 =
            gtsam::numericalDerivative32<gtsam::Vector3, gtsam::Point3, gtsam::Point3, gtsam::Pose3>(
                std::bind(&LandmarkMotionTernaryFactor::evaluateError, &factor,
                std::placeholders::_1, std::placeholders::_2, std::placeholders::_3, boost::none, boost::none, boost::none),
            P1, P2, HPerturbed);

    gtsam::Matrix numerical_H3 =
            gtsam::numericalDerivative33<gtsam::Vector3, gtsam::Point3, gtsam::Point3, gtsam::Pose3>(
                std::bind(&LandmarkMotionTernaryFactor::evaluateError, &factor,
                std::placeholders::_1, std::placeholders::_2, std::placeholders::_3, boost::none, boost::none, boost::none),
            P1, P2, HPerturbed);

    EXPECT_TRUE(gtsam::assert_equal(H1, numerical_H1));
    EXPECT_TRUE(gtsam::assert_equal(H2, numerical_H2));
    EXPECT_TRUE(gtsam::assert_equal(H3, numerical_H3));

}

TEST(LandmarkMotionTernaryFactor, testZeroError) {

    gtsam::Pose3 H(gtsam::Rot3::Rodrigues(-0.1, 0.2, 0.25),
                            gtsam::Point3(0.05, -0.10, 0.20));


   gtsam::Point3 P1(0.4, 1.0, 0.8);
   gtsam::Point3 P2 = H * P1;

    auto noise = gtsam::noiseModel::Isotropic::Sigma(3u, 0.1);

    LandmarkMotionTernaryFactor factor(0, 1, 2,noise);

    gtsam::Matrix H1, H2, H3;
    gtsam::Vector error = factor.evaluateError(P1, P2, H, H1, H2, H3);
    EXPECT_TRUE(gtsam::assert_equal(gtsam::Point3(0, 0, 0), error, 1e-4));


}
```

## File: backend/test_landmark_quadric_factor.cc
```
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/factors/LandmarkQuadricFactor.hpp"

#include <glog/logging.h>
#include <gtest/gtest.h>

using namespace dyno;

TEST(LandmarkQuadricFactor, residualBasicWithPointOnEllipse) {

    //test when radii = (3, 3, 3) (so denom = 9)
    //and then we construct a point m = 1, 2, 2 = 1 + 4 + 4 = 9
    gtsam::Point3 m_local(1, 2, 2);
    gtsam::Vector3 P(3, 3, 3);
    gtsam::Pose3 I = gtsam::Pose3::Identity();

    const gtsam::Vector1 error = LandmarkQuadricFactor::residual(
        m_local, I, P
    );
    EXPECT_TRUE(gtsam::assert_equal(error, gtsam::Vector1{0.0}));


}

TEST(LandmarkQuadricFactor, residualOnSphere) {

    //plug into ellipoide equation
    //matches distance to center of sphere + 1
    gtsam::Point3 m_local(6, 6, 3);
    gtsam::Vector3 P(3, 3, 3);
    gtsam::Pose3 I = gtsam::Pose3::Identity();

    const gtsam::Vector1 error = LandmarkQuadricFactor::residual(
        m_local, I, P
    );
    EXPECT_TRUE(gtsam::assert_equal(error, gtsam::Vector1{8.0}));


}
```

## File: backend/test_rgbd_backend.cc
```
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "internal/simulator.hpp"
#include "internal/helpers.hpp"

#include "dynosam/backend/BackendPipeline.hpp"
#include "dynosam/backend/FactorGraphTools.hpp"
#include "dynosam/backend/RGBDBackendModule.hpp"

#include <gtsam/linear/GaussianEliminationTree.h>
#include <gtsam/nonlinear/ISAM2-impl.h>

#include "dynosam/factors/LandmarkMotionTernaryFactor.hpp"
#include <gtsam/nonlinear/LevenbergMarquardtOptimizer.h>

#include <gtsam_unstable/slam/PoseToPointFactor.h>


#include <glog/logging.h>
#include <gtest/gtest.h>
#include <exception>
#include <vector>
#include <iterator>


TEST(RGBDBackendModule, constructSimpleGraph) {

    //make camera with a constant motion model starting at zero
    dyno_testing::ScenarioBody::Ptr camera = std::make_shared<dyno_testing::ScenarioBody>(
        std::make_unique<dyno_testing::ConstantMotionBodyVisitor>(
            gtsam::Pose3::Identity(),
            //motion only in x
            gtsam::Pose3(gtsam::Rot3::Identity(), gtsam::Point3(0.1, 0, 0))
        )
    );
    //needs to be at least 3 overlap so we can meet requirements in graph
    //TODO: how can we do 1 point but with lots of overlap (even infinity overlap?)
    dyno_testing::RGBDScenario scenario(
        camera,
        std::make_shared<dyno_testing::SimpleStaticPointsGenerator>(6, 3)
    );

    //add one obect
    const size_t num_points = 3;
    dyno_testing::ObjectBody::Ptr object1 = std::make_shared<dyno_testing::ObjectBody>(
        std::make_unique<dyno_testing::ConstantMotionBodyVisitor>(
            gtsam::Pose3(gtsam::Rot3::Identity(), gtsam::Point3(10, 0, 0)),
            //motion only in x
            gtsam::Pose3(gtsam::Rot3::Identity(), gtsam::Point3(0.2, 0, 0))
        ),
        std::make_unique<dyno_testing::ConstantObjectPointsVisitor>(num_points)
    );

    dyno_testing::ObjectBody::Ptr object2 = std::make_shared<dyno_testing::ObjectBody>(
        std::make_unique<dyno_testing::ConstantMotionBodyVisitor>(
            gtsam::Pose3(gtsam::Rot3::Identity(), gtsam::Point3(10, 0, 0)),
            //motion only in x
            gtsam::Pose3(gtsam::Rot3::Identity(), gtsam::Point3(0.2, 0, 0))
        ),
        std::make_unique<dyno_testing::ConstantObjectPointsVisitor>(num_points)
    );

    scenario.addObjectBody(1, object1);
    // scenario.addObjectBody(2, object2);

    dyno::Map3d2d::Ptr map = dyno::Map3d2d::create();
    // std::shared_ptr<dyno::IncrementalOptimizer<dyno::Landmark>> optimizer = std::make_shared<dyno::IncrementalOptimizer<dyno::Landmark>>();
    // std::shared_ptr<dyno_testing::DummyOptimizer<dyno::Landmark>> optimizer = std::make_shared<dyno_testing::DummyOptimizer<dyno::Landmark>>();
    // std::shared_ptr<dyno::ISAMOptimizer<dyno::Landmark>> optimizer = std::make_shared<dyno::ISAMOptimizer<dyno::Landmark>>();
    dyno::RGBDBackendModule backend(
        dyno::BackendParams().useLogger(false),
        map,
        dyno_testing::makeDefaultCameraPtr(),
        dyno::RGBDBackendModule::UpdaterType::MotionInWorld
        );

    for(size_t i = 0; i < 10; i++) {
        auto output = scenario.getOutput(i);

        std::stringstream ss;
        ss << output->T_world_camera_ << "\n";
        ss << dyno::container_to_string(output->static_landmarks_);

        // LOG(INFO) << ss.str();
        backend.spinOnce(output);

        LOG(INFO) << "Spun backend";

        // //Is this the full graph?
        // gtsam::NonlinearFactorGraph full_graph = backend.getMap()->getGraph();
        // gtsam::Values values = backend.smoother_->getLinearizationPoint();

        //get variables in this frame
        // const auto& map = backend.getMap();
        // const auto frame_k_node = map->getFrame(i);
        // const auto dyn_lmk_nodes = frame_k_node->dynamic_landmarks;
        // const auto object_nodes = frame_k_node->objects_seen;

        // gtsam::KeyVector dyn_lmks_this_frame;

        // for(const auto& dyn_lmk_node : dyn_lmk_nodes) {
        //     gtsam::Key key = dyn_lmk_node->makeDynamicKey(i);
        //     if(map->exists(key)) {
        //         dyn_lmks_this_frame.push_back(key);
        //     }
        // }

        // dyn_lmks_this_frame.push_back(map->getFrame(i)->makePoseKey());

        // gtsam::KeyVector object_motions_this_frame;

        // for(const auto& node : object_nodes) {
        //     const gtsam::Key key = frame_k_node->makeObjectMotionKey(node->getId());
        //     if(map->exists(key)) {
        //         dyn_lmks_this_frame.push_back(key);
        //     }
        // }


        // gtsam::VariableIndex affectedFactorsVarIndex(full_graph);
        // gtsam::Ordering order = gtsam::Ordering::ColamdConstrainedLast(affectedFactorsVarIndex, dyn_lmks_this_frame);
        // auto linearized = full_graph.linearize(values);

        // auto bayesTree =
        //     gtsam::ISAM2JunctionTree(
        //         gtsam::GaussianEliminationTree(*linearized, affectedFactorsVarIndex, order))
        //         .eliminate(gtsam::EliminatePreferCholesky)
        //         .first;

        // LOG(INFO) << bayesTree->roots().size();
        // LOG(WARNING) << "Number nnz bayes tree " << bayesTree->roots().at(0)->calculate_nnz();
        // LOG(WARNING) << "Number nnz isam2 tree " << optimizer->getSmoother().roots().at(0)->calculate_nnz();

        // // bayesTree->saveGraph(dyno::getOutputFilePath("elimated_tree.dot"), dyno::DynoLikeKeyFormatter);
        // dyno::factor_graph_tools::saveBayesTree(*bayesTree, dyno::getOutputFilePath("elimated_tree.dot"), dyno::DynoLikeKeyFormatter);

        // const auto& dynosam2_result = optimizer->getResult();

        // gtsam::FastMap<gtsam::Key, std::string> coloured_affected_keys;
        // for(const auto& key : dynosam2_result.reeliminatedKeys) {
        //     coloured_affected_keys.insert2(key, "red");
        // }

        // // const auto& smoother = optimizer->getSmoother().bayesTree();
        // const auto& smoother = optimizer->getSmoother();

        // // smoother.calculateBestEstimate();
        // if(smoother.roots().empty()) {
        //     continue;
        // }

        // dyno::factor_graph_tools::saveBayesTree(
        //     smoother,
        //     dyno::getOutputFilePath("rgbd_bayes_tree_" + std::to_string(i) + ".dot"),
        //     dyno::DynoLikeKeyFormatter,
        //     coloured_affected_keys);

        // {
        //     gtsam::KeySet marginalize_keys = dyno::factor_graph_tools::travsersal::getLeafKeys(smoother);
        //     gtsam::PrintKeySet(marginalize_keys, "Leaf keys", dyno::DynoLikeKeyFormatter);

        //     gtsam::FastMap<gtsam::Key, std::string> coloured_affected_keys;
        //     for(const auto& key : marginalize_keys) {
        //         coloured_affected_keys.insert2(key, "blue");
        //     }

        //     dyno::factor_graph_tools::saveBayesTree(
        //         smoother,
        //         dyno::getOutputFilePath("rgbd_bayes_tree_leaf.dot"),
        //         dyno::DynoLikeKeyFormatter,
        //         coloured_affected_keys);

        // }

        // smoother.getFactorsUnsafe().saveGraph(dyno::getOutputFilePath("smoother_graph_" + std::to_string(i) + ".dot"), dyno::DynoLikeKeyFormatter);

        // backend.saveGraph("rgbd_graph_" + std::to_string(i) + ".dot");
        // backend.saveTree("rgbd_bayes_tree_" + std::to_string(i) + ".dot");
    }

    gtsam::NonlinearFactorGraph full_graph = backend.new_updater_->getGraph();
    full_graph.saveGraph(dyno::getOutputFilePath("construct_simple_graph_test.dot"), dyno::DynoLikeKeyFormatter);

    {
        const auto[delayed_values, delayed_graph] = backend.constructGraph(2, 6, true);
        delayed_graph.saveGraph(dyno::getOutputFilePath("construct_simple_delayed_graph_test_2_6.dot"), dyno::DynoLikeKeyFormatter);
    }

    {
    const auto[delayed_values, delayed_graph] = backend.constructGraph(5, 9, true);
    delayed_graph.saveGraph(dyno::getOutputFilePath("construct_simple_delayed_graph_test_5_9.dot"), dyno::DynoLikeKeyFormatter);
    }

    // gtsam::LevenbergMarquardtParams opt_params;
    // opt_params.verbosity = gtsam::NonlinearOptimizerParams::Verbosity::ERROR;
    // // opt_params.
    // try {
    //     gtsam::Values opt_values = gtsam::LevenbergMarquardtOptimizer(delayed_graph, delayed_values, opt_params).optimize();
    // }
    // catch(const gtsam::ValuesKeyDoesNotExist& e) {
    //     LOG(INFO) << "Key does not exist in the values " <<  dyno::DynoLikeKeyFormatter(e.key());
    // }


    //graph depends on optimzier used
    //dummy one will += new factors so should be the full graph
    // gtsam::NonlinearFactorGraph full_graph = optimizer->getFactors();
    // full_graph.saveGraph(dyno::getOutputFilePath("construct_simple_graph_test.dot"), dyno::DynoLikeKeyFormatter);

    // backend.saveTree();





    // auto graph = backend.getMap()->getGraph();
    // auto estimates = backend.getMap()->getValues();

    // gtsam::GaussianFactorGraph factors = *graph.linearize(estimates);
    // gtsam::VariableIndex affectedFactorsVarIndex(factors);
    // //NOTE: NOT the same as in sam2 because of the affectedFactorsVarIndex
    // //maybe we can reconstruct the affectedFactorsVarIndex by looking at the factor index's and making a factor graph
    // //from this? This needs to be the factors that are affected
    // const gtsam::Ordering ordering =
    //   gtsam::Ordering::ColamdConstrained(affectedFactorsVarIndex, gtsam::FastMap<gtsam::Key, int>{});

    // gtsam::GaussianEliminationTree etree(factors, affectedFactorsVarIndex, ordering);

    // auto bayesTree = gtsam::ISAM2JunctionTree(etree)
    //                    .eliminate(gtsam::EliminatePreferCholesky)
    //                    .first;
    // bayesTree->saveGraph(dyno::getOutputFilePath("elimated_tree.dot"), dyno::DynoLikeKeyFormatter);
    // // etree.sa();

    // auto gfg = graph.linearize(estimates);

    // gtsam::Ordering isam2_ordering(dyno::factor_graph_tools::travsersal::getEliminatonOrder(*backend.smoother_));

    // //this ordering is not right. how do we get the ordering used by the bayes tree?
    // //OH DUH, the bayes tree IS the order!!! So which tree traversal algorithm do we use?
    // //Do we not start at the roots and check the frontal variables?
    // dyno::factor_graph_tools::computeRFactor(gfg, isam2_ordering);



}




TEST(RGBDBackendModule, testCliques) {
    using namespace dyno;
    //the simplest dynamic graph
    gtsam::NonlinearFactorGraph graph;
    gtsam::Values initial;

    gtsam::Pose3 H_0_1 = gtsam::Pose3(gtsam::Rot3::Rodrigues(0.3,0.2,0.1), gtsam::Point3 (0, 1, 1));
    gtsam::Pose3 H_1_2 = gtsam::Pose3(gtsam::Rot3::Rodrigues(0.5,0.1,0.1), gtsam::Point3 (1, 1.5, 1));
    gtsam::Pose3 H_0_2 = H_0_1 * H_1_2;

    gtsam::Point3 dyn_point_1_world = gtsam::Point3(2, 1, 3);
    gtsam::Point3 dyn_point_2_world = gtsam::Point3(1, 1, 3);
    gtsam::Point3 dyn_point_3_world = gtsam::Point3(3, 0.5, 2);

    static gtsam::Point3 p0 = gtsam::Point3(1,3,4);
    static gtsam::Rot3   R0 = gtsam::Rot3::Expmap( ( gtsam::Vector(3) << 0.2,0.1,0.12 ).finished() );
    static gtsam::Point3 p1 = gtsam::Point3(1,2,1);
    static gtsam::Rot3   R1 = gtsam::Rot3::Expmap( ( gtsam::Vector(3) << 0.1,0.2,1.570796 ).finished() );
    static gtsam::Point3 p2 = gtsam::Point3(2,2,1);
    static gtsam::Rot3   R2 = gtsam::Rot3::Expmap( ( gtsam::Vector(3) << 0.2,0.3,3.141593 ).finished() );
    static gtsam::Point3 p3 = gtsam::Point3(-1,1,0);
    static gtsam::Rot3   R3 = gtsam::Rot3::Expmap( ( gtsam::Vector(3) << 0.1,0.4,4.712389 ).finished() );

    static gtsam::Pose3 pose0 = gtsam::Pose3(R0,p0);
    static gtsam::Pose3 pose1 = gtsam::Pose3(R1,p1);
    static gtsam::Pose3 pose2 = gtsam::Pose3(R2,p2);
    static gtsam::Pose3 pose3 = gtsam::Pose3(R3,p3);

    auto landmark_noise = gtsam::noiseModel::Isotropic::Sigma(3u, 10);

    //static point seen at frames 0, 1 and 2
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
            CameraPoseSymbol(0),
            StaticLandmarkSymbol(0),
            gtsam::Point3(1, 2, 3),
            landmark_noise
        );
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
            CameraPoseSymbol(1),
            StaticLandmarkSymbol(0),
            gtsam::Point3(2, 2, 3),
            landmark_noise
        );

    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
            CameraPoseSymbol(2),
            StaticLandmarkSymbol(0),
            gtsam::Point3(3, 2, 3),
            landmark_noise
        );

    //motion between frames 0 and 1
    //add motion factor for the 3 tracklet with tracklet id = 1, 2, 3
    graph.emplace_shared<LandmarkMotionTernaryFactor>(
        DynamicLandmarkSymbol(0, 1),
        DynamicLandmarkSymbol(1, 1),
        ObjectMotionSymbol(1, 1),
        landmark_noise
    );

    //motion between frames 1 and 2
    graph.emplace_shared<LandmarkMotionTernaryFactor>(
        DynamicLandmarkSymbol(1, 1),
        DynamicLandmarkSymbol(2, 1),
        ObjectMotionSymbol(1, 2),
        landmark_noise
    );

    //tracklet 2
    graph.emplace_shared<LandmarkMotionTernaryFactor>(
        DynamicLandmarkSymbol(0, 2),
        DynamicLandmarkSymbol(1, 2),
        ObjectMotionSymbol(1, 1),
        landmark_noise
    );
    //motion between frames 1 and 2
    graph.emplace_shared<LandmarkMotionTernaryFactor>(
        DynamicLandmarkSymbol(1, 2),
        DynamicLandmarkSymbol(2, 2),
        ObjectMotionSymbol(1, 2),
        landmark_noise
    );


    //tracklet 3
    graph.emplace_shared<LandmarkMotionTernaryFactor>(
        DynamicLandmarkSymbol(0, 3),
        DynamicLandmarkSymbol(1, 3),
        ObjectMotionSymbol(1, 1),
        landmark_noise
    );
    //motion between frames 1 and 2
    graph.emplace_shared<LandmarkMotionTernaryFactor>(
        DynamicLandmarkSymbol(1, 3),
        DynamicLandmarkSymbol(2, 3),
        ObjectMotionSymbol(1, 2),
        landmark_noise
    );

    //tracklet 1
    //add dynamic point obs frame 0
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
        CameraPoseSymbol(0),
        DynamicLandmarkSymbol(0, 1),
        pose0.inverse() * dyn_point_1_world,
        landmark_noise
    );

     //add dynamic point obs frame 1
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
            CameraPoseSymbol(1),
            DynamicLandmarkSymbol(1, 1),
            pose1.inverse() * H_0_1 * dyn_point_1_world,
            landmark_noise
        );

     //add dynamic point obs frame 2
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
            CameraPoseSymbol(2),
            DynamicLandmarkSymbol(2, 1),
            pose2.inverse() * H_0_2 * dyn_point_1_world,
            landmark_noise
    );

    //tracklet 2
    //add dynamic point obs frame 0
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
        CameraPoseSymbol(0),
        DynamicLandmarkSymbol(0, 2),
        pose0.inverse() * dyn_point_2_world,
        landmark_noise
    );

     //add dynamic point obs frame 1
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
            CameraPoseSymbol(1),
            DynamicLandmarkSymbol(1, 2),
            pose1.inverse() * H_0_1 * dyn_point_2_world,
            landmark_noise
        );

     //add dynamic point obs frame 2
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
            CameraPoseSymbol(2),
            DynamicLandmarkSymbol(2, 2),
            pose2.inverse() * H_0_2 * dyn_point_2_world,
            landmark_noise
    );


    //tracklet 3
    //add dynamic point obs frame 0
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
        CameraPoseSymbol(0),
        DynamicLandmarkSymbol(0, 3),
        pose0.inverse() * dyn_point_3_world,
        landmark_noise
    );

     //add dynamic point obs frame 1
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
            CameraPoseSymbol(1),
            DynamicLandmarkSymbol(1, 3),
            pose1.inverse() * H_0_1 * dyn_point_3_world,
            landmark_noise
        );

     //add dynamic point obs frame 2
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
            CameraPoseSymbol(2),
            DynamicLandmarkSymbol(2, 3),
            pose2.inverse() * H_0_2 * dyn_point_3_world,
            landmark_noise
    );


    static gtsam::SharedNoiseModel pose_model(gtsam::noiseModel::Isotropic::Sigma(6, 0.1));

    // add two poses
    graph.add(gtsam::BetweenFactor<gtsam::Pose3>(CameraPoseSymbol(0), CameraPoseSymbol(1), pose0.between(pose1), pose_model));
    graph.add(gtsam::BetweenFactor<gtsam::Pose3>(CameraPoseSymbol(1), CameraPoseSymbol(2), pose1.between(pose2), pose_model));

    // add prior on first pose
    graph.addPrior(CameraPoseSymbol(0), pose0, pose_model);

    //now add inital values

    //add static point
    initial.insert(StaticLandmarkSymbol(0), gtsam::Point3(0, 1, 1));

    //add dynamic points

    //frame 0
    initial.insert(DynamicLandmarkSymbol(0, 1), dyn_point_1_world);
    initial.insert(DynamicLandmarkSymbol(0, 2), dyn_point_2_world);
    initial.insert(DynamicLandmarkSymbol(0, 3), dyn_point_3_world);

    //frame 1
    initial.insert(DynamicLandmarkSymbol(1, 1), H_0_1 * dyn_point_1_world);
    initial.insert(DynamicLandmarkSymbol(1, 2), H_0_1 * dyn_point_2_world);
    initial.insert(DynamicLandmarkSymbol(1, 3), H_0_1 * dyn_point_3_world);

    //frame 2
    initial.insert(DynamicLandmarkSymbol(2, 1), H_0_2 * dyn_point_1_world);
    initial.insert(DynamicLandmarkSymbol(2, 2), H_0_2 * dyn_point_2_world);
    initial.insert(DynamicLandmarkSymbol(2, 3), H_0_2 * dyn_point_3_world);

    //add two motions
    initial.insert(ObjectMotionSymbol(1, 1), gtsam::Pose3::Identity());
    initial.insert(ObjectMotionSymbol(1, 2), gtsam::Pose3::Identity());

    //add three poses
    initial.insert(CameraPoseSymbol(0), pose0);
    initial.insert(CameraPoseSymbol(1), pose1);
    initial.insert(CameraPoseSymbol(2), pose2);

    dyno::NonlinearFactorGraphManager nlfgm(graph, initial);
    nlfgm.writeDynosamGraphFile(dyno::getOutputFilePath("test_graph.g2o"));

    // graph.saveGraph(dyno::getOutputFilePath("small_graph.dot"), dyno::DynoLikeKeyFormatter);
    gtsam::ISAM2BayesTree::shared_ptr bayesTree = nullptr;
    {//
        // gtsam::ISAM2 isam2;
        gtsam::VariableIndex affectedFactorsVarIndex(graph);
        gtsam::Ordering order = gtsam::Ordering::Colamd(affectedFactorsVarIndex);
        auto linearized = graph.linearize(initial);

        bayesTree =
        gtsam::ISAM2JunctionTree(
            gtsam::GaussianEliminationTree(*linearized, affectedFactorsVarIndex, order))
            .eliminate(gtsam::EliminatePreferCholesky)
            .first;

        bayesTree->saveGraph(dyno::getOutputFilePath("elimated_tree.dot"), dyno::DynoLikeKeyFormatter);
    }

    // {

    //     gtsam::ISAM2UpdateParams isam_update_params;

    //     gtsam::FastMap<gtsam::Key, int> constrainedKeys;
    //     //this includes the motions, where do we want these?
    //     //maybe BETWEEN the dynnamic keys
    //     //we want motions always on the right but impossible since parent?
    //     for(const auto& [keys, value] : initial) {
    //         constrainedKeys.insert2(keys, 1);
    //     }
    //     //put previous dynamic keys lower in the graph
    //     constrainedKeys.insert2(DynamicLandmarkSymbol(0, 1), 0);
    //     constrainedKeys.insert2(DynamicLandmarkSymbol(1, 1), 0);

    //     constrainedKeys.insert2(DynamicLandmarkSymbol(0, 2), 0);
    //     constrainedKeys.insert2(DynamicLandmarkSymbol(1, 2), 0);

    //     constrainedKeys.insert2(DynamicLandmarkSymbol(0, 2), 0);
    //     constrainedKeys.insert2(DynamicLandmarkSymbol(1, 2), 0);

    //     ///put current keys later in the graph
    //     constrainedKeys.insert2(DynamicLandmarkSymbol(2, 1), 2);
    //     constrainedKeys.insert2(DynamicLandmarkSymbol(2, 2), 2);
    //     constrainedKeys.insert2(DynamicLandmarkSymbol(2, 3), 2);

    //     isam_update_params.constrainedKeys = constrainedKeys;
    //     isam2.update(graph, initial, isam_update_params);
    // }

    // isam2.saveGraph(dyno::getOutputFilePath("small_tree_original.dot"), dyno::DynoLikeKeyFormatter);

    //add new factors to  motion
    //the simplest dynamic graph
    gtsam::NonlinearFactorGraph new_graph;
    gtsam::Values new_values;

    gtsam::Pose3 H_2_3 = gtsam::Pose3(gtsam::Rot3::Rodrigues(0.4,0.15,0.1), gtsam::Point3 (0, 3, 1));
    gtsam::Pose3 H_0_3 = H_0_2 * H_2_3;

    //add dynamic obs
    new_graph.emplace_shared<LandmarkMotionTernaryFactor>(
        DynamicLandmarkSymbol(2, 1),
        DynamicLandmarkSymbol(3, 1),
        ObjectMotionSymbol(1, 3),
        landmark_noise
    );

     new_graph.emplace_shared<LandmarkMotionTernaryFactor>(
        DynamicLandmarkSymbol(2, 2),
        DynamicLandmarkSymbol(3, 2),
        ObjectMotionSymbol(1, 3),
        landmark_noise
    );

    new_graph.emplace_shared<LandmarkMotionTernaryFactor>(
        DynamicLandmarkSymbol(2, 3),
        DynamicLandmarkSymbol(3, 3),
        ObjectMotionSymbol(1, 3),
        landmark_noise
    );

    //add pose-point constraints
    new_graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
        CameraPoseSymbol(3),
        DynamicLandmarkSymbol(3, 1),
        pose3.inverse() * H_0_3 * dyn_point_1_world,
        landmark_noise
    );
    new_graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
            CameraPoseSymbol(3),
            DynamicLandmarkSymbol(3,2),
            pose3.inverse() * H_0_3 * dyn_point_2_world,
            landmark_noise
        );

    new_graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
            CameraPoseSymbol(3),
            DynamicLandmarkSymbol(3, 3),
            pose3.inverse() * H_0_3 * dyn_point_3_world,
            landmark_noise
    );

    //add static obs
    new_graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
            CameraPoseSymbol(3),
            StaticLandmarkSymbol(0),
            gtsam::Point3(4, 2, 3),
            landmark_noise
        );

    //add odom
    new_graph.add(gtsam::BetweenFactor<gtsam::Pose3>(CameraPoseSymbol(2), CameraPoseSymbol(3), pose2.between(pose3), pose_model));

    //add initials
    new_values.insert(DynamicLandmarkSymbol(3, 1), H_0_3 * dyn_point_1_world);
    new_values.insert(DynamicLandmarkSymbol(3, 2), H_0_3 * dyn_point_2_world);
    new_values.insert(DynamicLandmarkSymbol(3, 3), H_0_3 * dyn_point_3_world);
    new_values.insert(ObjectMotionSymbol(1, 3), gtsam::Pose3::Identity());

    new_values.insert(CameraPoseSymbol(3), pose3);

    {
        gtsam::NonlinearFactorGraph full_graph(graph);
        full_graph.add_factors(new_graph);

        gtsam::Values all_values(initial);
        all_values.insert(new_values);

        gtsam::VariableIndex affectedFactorsVarIndex(full_graph);
        gtsam::Ordering order = gtsam::Ordering::ColamdConstrainedLast(affectedFactorsVarIndex, full_graph.keyVector(), true);
        auto linearized = full_graph.linearize(all_values);

        bayesTree =
        gtsam::ISAM2JunctionTree(
            gtsam::GaussianEliminationTree(*linearized, affectedFactorsVarIndex, order))
            .eliminate(gtsam::EliminatePreferCholesky)
            .first;

        bayesTree->saveGraph(dyno::getOutputFilePath("elimated_tree_1.dot"), dyno::DynoLikeKeyFormatter);
    }

    // gtsam::NonlinearFactorGraph all = graph;
    // all.add_factors(new_graph);
    // all.saveGraph(dyno::getOutputFilePath("small_graph_updated.dot"), dyno::DynoLikeKeyFormatter);

    //  {

    //     gtsam::ISAM2UpdateParams isam_update_params;

    //     gtsam::FastMap<gtsam::Key, int> constrainedKeys;
    //     //this includes the motions, where do we want these?
    //     //maybe BETWEEN the dynnamic keys
    //     //we want motions always on the right but impossible since parent?
    //     for(const auto& [keys, value] : new_values) {
    //         constrainedKeys.insert2(keys, 1);
    //     }
    //     constrainedKeys.insert2(DynamicLandmarkSymbol(2, 1), 0);
    //     constrainedKeys.insert2(DynamicLandmarkSymbol(2, 2), 0);
    //     constrainedKeys.insert2(DynamicLandmarkSymbol(2, 3), 0);

    //     ///put current keys later in the graph
    //     constrainedKeys.insert2(DynamicLandmarkSymbol(3, 1), 2);
    //     constrainedKeys.insert2(DynamicLandmarkSymbol(3, 2), 2);
    //     constrainedKeys.insert2(DynamicLandmarkSymbol(3, 3), 2);

    //     isam_update_params.constrainedKeys = constrainedKeys;
    //     isam2.update(new_graph, new_values, isam_update_params);
    // }

    // isam2.saveGraph(dyno::getOutputFilePath("small_tree_updated.dot"), dyno::DynoLikeKeyFormatter);





}


TEST(RGBDBackendModule, writeOutSimpleGraph) {
    using namespace dyno;
    //the simplest dynamic graph
    gtsam::NonlinearFactorGraph graph;
    gtsam::Values initial;

    gtsam::Pose3 H_0_1 = gtsam::Pose3(gtsam::Rot3::Rodrigues(0.3,0.2,0.1), gtsam::Point3 (0, 1, 1));
    gtsam::Pose3 H_1_2 = gtsam::Pose3(gtsam::Rot3::Rodrigues(0.5,0.1,0.1), gtsam::Point3 (1, 1.5, 1));
    gtsam::Pose3 H_0_2 = H_0_1 * H_1_2;

    gtsam::Point3 dyn_point_1_world = gtsam::Point3(2, 1, 3);
    gtsam::Point3 dyn_point_2_world = gtsam::Point3(1, 1, 3);
    gtsam::Point3 dyn_point_3_world = gtsam::Point3(3, 0.5, 2);

    static gtsam::Point3 p0 = gtsam::Point3(1,3,4);
    static gtsam::Rot3   R0 = gtsam::Rot3::Expmap( ( gtsam::Vector(3) << 0.2,0.1,0.12 ).finished() );
    static gtsam::Point3 p1 = gtsam::Point3(1,2,1);
    static gtsam::Rot3   R1 = gtsam::Rot3::Expmap( ( gtsam::Vector(3) << 0.1,0.2,1.570796 ).finished() );
    static gtsam::Point3 p2 = gtsam::Point3(2,2,1);
    static gtsam::Rot3   R2 = gtsam::Rot3::Expmap( ( gtsam::Vector(3) << 0.2,0.3,3.141593 ).finished() );
    static gtsam::Point3 p3 = gtsam::Point3(-1,1,0);
    static gtsam::Rot3   R3 = gtsam::Rot3::Expmap( ( gtsam::Vector(3) << 0.1,0.4,4.712389 ).finished() );

    static gtsam::Pose3 pose0 = gtsam::Pose3(R0,p0);
    static gtsam::Pose3 pose1 = gtsam::Pose3(R1,p1);
    static gtsam::Pose3 pose2 = gtsam::Pose3(R2,p2);
    static gtsam::Pose3 pose3 = gtsam::Pose3(R3,p3);

    auto landmark_noise = gtsam::noiseModel::Isotropic::Sigma(3u, 10);

    //static point seen at frames 0, 1 and 2
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
            CameraPoseSymbol(0),
            StaticLandmarkSymbol(0),
            gtsam::Point3(1, 2, 3),
            landmark_noise
        );
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
            CameraPoseSymbol(1),
            StaticLandmarkSymbol(0),
            gtsam::Point3(2, 2, 3),
            landmark_noise
        );

    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
            CameraPoseSymbol(2),
            StaticLandmarkSymbol(0),
            gtsam::Point3(3, 2, 3),
            landmark_noise
        );

    //motion between frames 0 and 1
    //add motion factor for the 3 tracklet with tracklet id = 1, 2, 3
    graph.emplace_shared<LandmarkMotionTernaryFactor>(
        DynamicLandmarkSymbol(0, 1),
        DynamicLandmarkSymbol(1, 1),
        ObjectMotionSymbol(1, 1),
        landmark_noise
    );

    //motion between frames 1 and 2
    graph.emplace_shared<LandmarkMotionTernaryFactor>(
        DynamicLandmarkSymbol(1, 1),
        DynamicLandmarkSymbol(2, 1),
        ObjectMotionSymbol(1, 2),
        landmark_noise
    );

    //tracklet 2
    graph.emplace_shared<LandmarkMotionTernaryFactor>(
        DynamicLandmarkSymbol(0, 2),
        DynamicLandmarkSymbol(1, 2),
        ObjectMotionSymbol(1, 1),
        landmark_noise
    );
    //motion between frames 1 and 2
    graph.emplace_shared<LandmarkMotionTernaryFactor>(
        DynamicLandmarkSymbol(1, 2),
        DynamicLandmarkSymbol(2, 2),
        ObjectMotionSymbol(1, 2),
        landmark_noise
    );


    //tracklet 3
    graph.emplace_shared<LandmarkMotionTernaryFactor>(
        DynamicLandmarkSymbol(0, 3),
        DynamicLandmarkSymbol(1, 3),
        ObjectMotionSymbol(1, 1),
        landmark_noise
    );
    //motion between frames 1 and 2
    graph.emplace_shared<LandmarkMotionTernaryFactor>(
        DynamicLandmarkSymbol(1, 3),
        DynamicLandmarkSymbol(2, 3),
        ObjectMotionSymbol(1, 2),
        landmark_noise
    );

    //tracklet 1
    //add dynamic point obs frame 0
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
        CameraPoseSymbol(0),
        DynamicLandmarkSymbol(0, 1),
        pose0.inverse() * dyn_point_1_world,
        landmark_noise
    );

     //add dynamic point obs frame 1
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
            CameraPoseSymbol(1),
            DynamicLandmarkSymbol(1, 1),
            pose1.inverse() * H_0_1 * dyn_point_1_world,
            landmark_noise
        );

     //add dynamic point obs frame 2
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
            CameraPoseSymbol(2),
            DynamicLandmarkSymbol(2, 1),
            pose2.inverse() * H_0_2 * dyn_point_1_world,
            landmark_noise
    );

    //tracklet 2
    //add dynamic point obs frame 0
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
        CameraPoseSymbol(0),
        DynamicLandmarkSymbol(0, 2),
        pose0.inverse() * dyn_point_2_world,
        landmark_noise
    );

     //add dynamic point obs frame 1
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
            CameraPoseSymbol(1),
            DynamicLandmarkSymbol(1, 2),
            pose1.inverse() * H_0_1 * dyn_point_2_world,
            landmark_noise
        );

     //add dynamic point obs frame 2
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
            CameraPoseSymbol(2),
            DynamicLandmarkSymbol(2, 2),
            pose2.inverse() * H_0_2 * dyn_point_2_world,
            landmark_noise
    );


    //tracklet 3
    //add dynamic point obs frame 0
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
        CameraPoseSymbol(0),
        DynamicLandmarkSymbol(0, 3),
        pose0.inverse() * dyn_point_3_world,
        landmark_noise
    );

     //add dynamic point obs frame 1
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
            CameraPoseSymbol(1),
            DynamicLandmarkSymbol(1, 3),
            pose1.inverse() * H_0_1 * dyn_point_3_world,
            landmark_noise
        );

     //add dynamic point obs frame 2
    graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3, Landmark>>(
            CameraPoseSymbol(2),
            DynamicLandmarkSymbol(2, 3),
            pose2.inverse() * H_0_2 * dyn_point_3_world,
            landmark_noise
    );


    static gtsam::SharedNoiseModel pose_model(gtsam::noiseModel::Isotropic::Sigma(6, 0.1));

    // add two poses
    graph.add(gtsam::BetweenFactor<gtsam::Pose3>(CameraPoseSymbol(0), CameraPoseSymbol(1), pose0.between(pose1), pose_model));
    graph.add(gtsam::BetweenFactor<gtsam::Pose3>(CameraPoseSymbol(1), CameraPoseSymbol(2), pose1.between(pose2), pose_model));

    // add prior on first pose
    graph.addPrior(CameraPoseSymbol(0), pose0, pose_model);

    //now add inital values

    //add static point
    initial.insert(StaticLandmarkSymbol(0), gtsam::Point3(0, 1, 1));

    //add dynamic points

    //frame 0
    initial.insert(DynamicLandmarkSymbol(0, 1), dyn_point_1_world);
    initial.insert(DynamicLandmarkSymbol(0, 2), dyn_point_2_world);
    initial.insert(DynamicLandmarkSymbol(0, 3), dyn_point_3_world);

    //frame 1
    initial.insert(DynamicLandmarkSymbol(1, 1), H_0_1 * dyn_point_1_world);
    initial.insert(DynamicLandmarkSymbol(1, 2), H_0_1 * dyn_point_2_world);
    initial.insert(DynamicLandmarkSymbol(1, 3), H_0_1 * dyn_point_3_world);

    //frame 2
    initial.insert(DynamicLandmarkSymbol(2, 1), H_0_2 * dyn_point_1_world);
    initial.insert(DynamicLandmarkSymbol(2, 2), H_0_2 * dyn_point_2_world);
    initial.insert(DynamicLandmarkSymbol(2, 3), H_0_2 * dyn_point_3_world);

    //add two motions
    initial.insert(ObjectMotionSymbol(1, 1), gtsam::Pose3::Identity());
    initial.insert(ObjectMotionSymbol(1, 2), gtsam::Pose3::Identity());

    //add three poses
    initial.insert(CameraPoseSymbol(0), pose0);
    initial.insert(CameraPoseSymbol(1), pose1);
    initial.insert(CameraPoseSymbol(2), pose2);

    dyno::NonlinearFactorGraphManager nlfgm(graph, initial);
    // nlfgm.writeDynosamGraphFile(dyno::getOutputFilePath("test_graph.g2o"));

    graph.saveGraph(dyno::getOutputFilePath("simple_dynamic_graph.dot"), dyno::DynoLikeKeyFormatter);
    gtsam::GaussianFactorGraph::shared_ptr linearised_graph = nlfgm.linearize();
    gtsam::JacobianFactor jacobian_factor(*linearised_graph);
    gtsam::Matrix information = jacobian_factor.information();

    LOG(INFO) << information.rows() << " " << information.cols();

    writeMatrixWithPythonFormat(information, dyno::getOutputFilePath("simple_dynamic_graph_information.txt"));



}
```

## File: backend/test_triangulation.cc
```
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Yiduo Wang (yiduo.wang@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */
#include "dynosam/backend/MonoBackendTools.hpp"
#include "dynosam/common/Types.hpp"

#include <gtsam/base/Vector.h>
#include <gtest/gtest.h>

using namespace dyno;


TEST(MonoBackendTools, triangulatePoint3Vector)
{
  gtsam::Pose3 X_world_camera_prev;
  // Use Quaternion and translation to construct the current pose
  // Around y axis, 15 degree - equivalent to 15 degree to the right in yaw
  gtsam::Pose3 X_world_camera_curr(gtsam::Rot3(0.9914449, 0.0, 0.1305262, 0.0), gtsam::Point3(0.1, 0.1, 2.0));

  // std::cout << X_world_camera_prev << "\n";
  // std::cout << X_world_camera_curr << "\n";

  gtsam::Matrix3 obj_rot = Eigen::Matrix3d::Identity();
  // Euler angle x = 5 degree, y = 10 degree, z = 15 degree, order XYZ
  obj_rot <<  0.98106026, -0.08583165,  0.17364818,
              0.12895841,  0.95833311, -0.254887,
             -0.14453543,  0.2724529,   0.95125124;

  // std::cout << obj_rot << "\n";

  gtsam::Matrix3 intrinsic = Eigen::Matrix3d::Identity();
  intrinsic(0, 0) = 320.0;
  intrinsic(1, 1) = 320.0;
  intrinsic(0, 2) = 320.0;
  intrinsic(1, 2) = 240.0;

  // std::cout << intrinsic << "\n";

  gtsam::Point3Vector points_prev, points_curr;

  // a 2*2*2 cude
  points_prev.push_back(Eigen::Vector3d(5.0, 1.0, 5.0));
  points_prev.push_back(Eigen::Vector3d(7.0, 1.0, 5.0));
  points_prev.push_back(Eigen::Vector3d(5.0, 1.0, 7.0));
  points_prev.push_back(Eigen::Vector3d(7.0, 1.0, 7.0));
  points_prev.push_back(Eigen::Vector3d(5.0, -1.0, 5.0));
  points_prev.push_back(Eigen::Vector3d(7.0, -1.0, 5.0));
  points_prev.push_back(Eigen::Vector3d(5.0, -1.0, 7.0));
  points_prev.push_back(Eigen::Vector3d(7.0, -1.0, 7.0));

  // same rotation as obj_rot
  gtsam::Pose3 H_prev_curr_world(gtsam::Rot3(0.9862359, 0.1336749, 0.0806561, 0.0544469), gtsam::Point3(2.0, 0.0, 0.0));
  // std::cout << "H_prev_curr_world\n" << H_prev_curr_world << std::endl;
  for (int i = 0; i < 8; i++){
    points_curr.push_back(H_prev_curr_world.transformFrom(points_prev[i]));
  }


  gtsam::Point2Vector observation_prev, observation_curr;

  for (int i = 0; i < 8; i++){
    gtsam::Point3 local_point_prev = intrinsic*X_world_camera_prev.transformTo(points_prev[i]);
    gtsam::Point3 local_point_curr = intrinsic*X_world_camera_curr.transformTo(points_curr[i]);

    observation_prev.push_back(Eigen::Vector2d(local_point_prev.x()/local_point_prev.z(), local_point_prev.y()/local_point_prev.z()));
    observation_curr.push_back(Eigen::Vector2d(local_point_curr.x()/local_point_curr.z(), local_point_curr.y()/local_point_curr.z()));
  }


  gtsam::Point3Vector points_world = dyno::mono_backend_tools::triangulatePoint3Vector(X_world_camera_prev, X_world_camera_curr, intrinsic,
                                                                                       observation_prev, observation_curr, obj_rot);

  gtsam::Point3Vector expected_points_world;
  // [ 3.07637456  4.69454556  2.5236143   3.96822456  3.30434543  5.0268543   2.68667343  4.19481659]
  // [ 0.61527491  0.67064937  0.50472286  0.56688922 -0.66086909 -0.71812204 -0.53733469 -0.59925951]
  // [ 3.07637456  3.35324683  3.53306001  3.96822456  3.30434543  3.59061022  3.7613428   4.19481659]
  expected_points_world.push_back(gtsam::Point3(3.07637456, 0.61527491, 3.07637456));
  expected_points_world.push_back(gtsam::Point3(4.69454556, 0.67064937, 3.35324683));
  expected_points_world.push_back(gtsam::Point3(2.5236143 , 0.50472286 , 3.53306001));
  expected_points_world.push_back(gtsam::Point3(3.96822456, 0.56688922, 3.96822456));
  expected_points_world.push_back(gtsam::Point3(3.30434543, -0.66086909, 3.30434543));
  expected_points_world.push_back(gtsam::Point3( 5.0268543, -0.71812204, 3.59061022));
  expected_points_world.push_back(gtsam::Point3(2.68667343, -0.53733469, 3.7613428));
  expected_points_world.push_back(gtsam::Point3(4.19481659, -0.59925951,4.19481659));

  EXPECT_EQ(expected_points_world.size(), points_world.size());
  for(size_t i = 0; i < expected_points_world.size(); i++) {
    EXPECT_TRUE(gtsam::assert_equal(expected_points_world.at(i), points_world.at(i), 1.0e-5));
  }
}

TEST(MonoBackendTools, triangulatePoint3VectorNonExpanded)
{
  gtsam::Pose3 X_world_camera_prev;
  // Use Quaternion and translation to construct the current pose
  // Around y axis, 15 degree - equivalent to 15 degree to the right in yaw
  gtsam::Pose3 X_world_camera_curr(gtsam::Rot3(0.9914449, 0.0, 0.1305262, 0.0), gtsam::Point3(0.1, 0.1, 2.0));

  // std::cout << X_world_camera_prev << "\n";
  // std::cout << X_world_camera_curr << "\n";

  gtsam::Matrix3 obj_rot = Eigen::Matrix3d::Identity();
  // Euler angle x = 5 degree, y = 10 degree, z = 15 degree, order XYZ
  obj_rot <<  0.98106026, -0.08583165,  0.17364818,
              0.12895841,  0.95833311, -0.254887,
             -0.14453543,  0.2724529,   0.95125124;

  // std::cout << obj_rot << "\n";

  gtsam::Matrix3 intrinsic = Eigen::Matrix3d::Identity();
  intrinsic(0, 0) = 320.0;
  intrinsic(1, 1) = 320.0;
  intrinsic(0, 2) = 320.0;
  intrinsic(1, 2) = 240.0;

  // std::cout << intrinsic << "\n";

  gtsam::Point3Vector points_prev, points_curr;

  // a 2*2*2 cude
  points_prev.push_back(Eigen::Vector3d(5.0, 1.0, 5.0));
  points_prev.push_back(Eigen::Vector3d(7.0, 1.0, 5.0));
  points_prev.push_back(Eigen::Vector3d(5.0, 1.0, 7.0));
  points_prev.push_back(Eigen::Vector3d(7.0, 1.0, 7.0));
  points_prev.push_back(Eigen::Vector3d(5.0, -1.0, 5.0));
  points_prev.push_back(Eigen::Vector3d(7.0, -1.0, 5.0));
  points_prev.push_back(Eigen::Vector3d(5.0, -1.0, 7.0));
  points_prev.push_back(Eigen::Vector3d(7.0, -1.0, 7.0));

  // same rotation as obj_rot
  gtsam::Pose3 H_prev_curr_world(gtsam::Rot3(0.9862359, 0.1336749, 0.0806561, 0.0544469), gtsam::Point3(2.0, 0.0, 0.0));
  // std::cout << "H_prev_curr_world\n" << H_prev_curr_world << std::endl;
  for (int i = 0; i < 8; i++){
    points_curr.push_back(H_prev_curr_world.transformFrom(points_prev[i]));
  }


  gtsam::Point2Vector observation_prev, observation_curr;

  for (int i = 0; i < 8; i++){
    gtsam::Point3 local_point_prev = intrinsic*X_world_camera_prev.transformTo(points_prev[i]);
    gtsam::Point3 local_point_curr = intrinsic*X_world_camera_curr.transformTo(points_curr[i]);

    observation_prev.push_back(Eigen::Vector2d(local_point_prev.x()/local_point_prev.z(), local_point_prev.y()/local_point_prev.z()));
    observation_curr.push_back(Eigen::Vector2d(local_point_curr.x()/local_point_curr.z(), local_point_curr.y()/local_point_curr.z()));
  }


  gtsam::Point3Vector points_world = dyno::mono_backend_tools::triangulatePoint3VectorNonExpanded(X_world_camera_prev, X_world_camera_curr, intrinsic,
                                                                                                  observation_prev, observation_curr, obj_rot);

  gtsam::Point3Vector expected_points_world;
  // [ 3.81828248  5.7540693   3.24108742  4.98904018  4.00304244  6.02650317  3.38370898  5.18169805]
  // [ 0.7636565   0.8220099   0.64821748  0.71272003 -0.80060849 -0.86092902 -0.6767418  -0.74024258]
  // [ 3.81828248  4.1100495   4.53752238  4.98904018  4.00304244  4.30464512  4.73719257  5.18169805]
  expected_points_world.push_back(gtsam::Point3(3.81828248, 0.7636565, 3.81828248));
  expected_points_world.push_back(gtsam::Point3(5.7540693,  0.8220099, 4.1100495));
  expected_points_world.push_back(gtsam::Point3(3.24108742, 0.64821748, 4.53752238));
  expected_points_world.push_back(gtsam::Point3(4.98904018, 0.71272003, 4.98904018));
  expected_points_world.push_back(gtsam::Point3(4.00304244, -0.80060849, 4.00304244));
  expected_points_world.push_back(gtsam::Point3(6.02650317, -0.86092902, 4.30464512));
  expected_points_world.push_back(gtsam::Point3(3.38370898, -0.6767418, 4.73719257));
  expected_points_world.push_back(gtsam::Point3(5.18169805, -0.74024258, 5.18169805));

  EXPECT_EQ(expected_points_world.size(), points_world.size());
  for(size_t i = 0; i < expected_points_world.size(); i++) {
    EXPECT_TRUE(gtsam::assert_equal(expected_points_world.at(i), points_world.at(i), 1.0e-5));
  }
}
```

## File: camera/ZEDCamera/test_ZED_camera_init.cc
```
// test_zed_camera.cc

#include <gtest/gtest.h>
#include <glog/logging.h>
#include <sl/Camera.hpp> // ZED SDK header

#include "dynosam/common/ZEDCamera.hpp" // Header for the class under test


namespace dyno {
namespace common {
namespace test {


TEST(ZEDCameraInitTest, ZC_INIT_001_LiveCameraConfigPopulation) {
    // **Test Case ID:** ZC_INIT_001
    // **Method(s) Under Test:** ZEDCamera(const ZEDCameraConfig& config), populateInitParams()
    // **Test Scenario Description:** Verify correct population of sl::InitParameters for a live camera.
    // **Key Configurations/Inputs:** Live mode, specific resolution, FPS, depth mode, IMU enabled.
    // **Expected Outcome:** zed_init_params_ member reflects the input config.
    // **Mocking Considerations:** None for this part of initialization.

    ZEDCameraConfig config;
    config.svo_file_path = ""; // Live camera
    config.resolution = sl::RESOLUTION::HD720;
    config.fps = 30;
    config.depth_mode = sl::DEPTH_MODE::PERFORMANCE;
    config.coordinate_units = sl::UNIT::METER;
    config.coordinate_system_3d = sl::COORDINATE_SYSTEM::RIGHT_HANDED_Z_UP_X_FWD;
    config.enable_imu = true;
    // Set other defaults or specific values as needed by ZEDCameraConfig
    config.sdk_verbose = 1;
    config.camera_disable_self_calib = false;
    config.enable_image_enhancement = true;
    config.open_timeout_sec = 5.0f;
    config.enable_right_side_measure = false;
    config.async_grab_camera_recovery = true;
    config.grab_compute_capping_fps = 0; // 0 for no capping based on grab
    config.enable_image_validity_check = true;


    ZEDCamera zed_cam(config);
    EXPECT_TRUE(zed_cam.open()); // Ensure camera opens successfully

    // Access public member zed_init_params_ for verification
    const sl::InitParameters& init_params = zed_cam.getCameraInitParams();

    EXPECT_EQ(init_params.camera_resolution, config.resolution);
    EXPECT_EQ(init_params.camera_fps, config.fps);
    EXPECT_EQ(init_params.depth_mode, config.depth_mode);
    EXPECT_EQ(init_params.coordinate_units, config.coordinate_units);
    EXPECT_EQ(init_params.coordinate_system, config.coordinate_system_3d);
    EXPECT_TRUE(init_params.sensors_required); // Because config.enable_imu = true

    EXPECT_EQ(init_params.sdk_verbose, config.sdk_verbose > 0);
    EXPECT_EQ(init_params.camera_disable_self_calib, config.camera_disable_self_calib);
    EXPECT_EQ(init_params.enable_image_enhancement, config.enable_image_enhancement);
    EXPECT_FLOAT_EQ(init_params.open_timeout_sec, config.open_timeout_sec);
    EXPECT_EQ(init_params.enable_right_side_measure, config.enable_right_side_measure);
    EXPECT_EQ(init_params.async_grab_camera_recovery, config.async_grab_camera_recovery);
    EXPECT_EQ(init_params.grab_compute_capping_fps, config.grab_compute_capping_fps);
    EXPECT_EQ(init_params.enable_image_validity_check, config.enable_image_validity_check);

    // For live camera, input type should not be SVO
    EXPECT_NE(static_cast<const sl::INPUT_TYPE>(init_params.input.getType()), sl::INPUT_TYPE::SVO);
}

TEST(ZEDCameraInitTest, ZC_INIT_002_SVOFileConfigPopulation) {
    // **Test Case ID:** ZC_INIT_002
    // **Method(s) Under Test:** ZEDCamera(const ZEDCameraConfig& config), populateInitParams()
    // **Test Scenario Description:** Verify correct population of sl::InitParameters for SVO.
    // **Key Configurations/Inputs:** SVO file path, svo_real_time_mode = true, IMU disabled.
    // **Expected Outcome:** zed_init_params_.input reflects SVO, sensors_required is false.
    // **Mocking Considerations:** None for this part of initialization.

    ZEDCameraConfig config;
    config.svo_file_path = "test_dummy.svo";
    config.svo_real_time_mode = true;
    config.enable_imu = false;
    // Set other defaults or specific values
    config.resolution = sl::RESOLUTION::VGA;
    config.fps = 15;
    config.depth_mode = sl::DEPTH_MODE::ULTRA;
    config.coordinate_units = sl::UNIT::MILLIMETER;
    config.coordinate_system_3d = sl::COORDINATE_SYSTEM::IMAGE;


    ZEDCamera zed_cam(config);

    const sl::InitParameters& init_params = zed_cam.getCameraInitParams();

    EXPECT_EQ(static_cast<sl::INPUT_TYPE>(init_params.input.getType()), sl::INPUT_TYPE::SVO);
    // We can't directly get the path string from sl::InputType easily without SDK interaction
    // but we trust setFromSVOFile worked. The important part is the type and svo_real_time_mode.
    EXPECT_TRUE(init_params.svo_real_time_mode);
    EXPECT_FALSE(init_params.sensors_required); // Because config.enable_imu = false

    EXPECT_EQ(init_params.camera_resolution, config.resolution);
    EXPECT_EQ(init_params.camera_fps, config.fps);
    EXPECT_EQ(init_params.depth_mode, config.depth_mode);
    EXPECT_EQ(init_params.coordinate_units, config.coordinate_units);
    EXPECT_EQ(init_params.coordinate_system, config.coordinate_system_3d);
}

TEST(ZEDCameraInitTest, ZC_INIT_003_BooleanFlagsPopulation) {
    // **Test Case ID:** ZC_INIT_003
    // **Method(s) Under Test:** ZEDCamera(const ZEDCameraConfig& config), populateInitParams()
    // **Test Scenario Description:** Verify populateInitParams handles various boolean flags correctly.
    // **Key Configurations/Inputs:** Specific boolean flags set to non-default values.
    // **Expected Outcome:** Corresponding fields in zed_init_params_ match the config.
    // **Mocking Considerations:** None for this part of initialization.

    ZEDCameraConfig config;
    // Default some non-boolean values to ensure the test focuses on booleans
    config.svo_file_path = "";
    config.resolution = sl::RESOLUTION::HD1080;
    config.fps = 60;

    // Set boolean flags to specific values
    config.camera_disable_self_calib = true;
    config.enable_image_enhancement = false;
    config.async_grab_camera_recovery = false;
    config.enable_right_side_measure = true;
    config.grab_compute_capping_fps = 15;
    config.open_timeout_sec = 3.5f;
    config.enable_image_validity_check = false; // Test this specific one

    ZEDCamera zed_cam(config);

    const sl::InitParameters& init_params = zed_cam.getCameraInitParams();

    EXPECT_EQ(init_params.camera_disable_self_calib, config.camera_disable_self_calib);
    EXPECT_EQ(init_params.enable_image_enhancement, config.enable_image_enhancement);
    EXPECT_EQ(init_params.async_grab_camera_recovery, config.async_grab_camera_recovery);
    EXPECT_EQ(init_params.enable_right_side_measure, config.enable_right_side_measure);
    EXPECT_EQ(init_params.grab_compute_capping_fps, config.grab_compute_capping_fps);
    EXPECT_FLOAT_EQ(init_params.open_timeout_sec, config.open_timeout_sec);
    EXPECT_EQ(init_params.enable_image_validity_check, config.enable_image_validity_check);
}


} // namespace test
} // namespace common
} // namespace dyno
// int main(int argc, char** argv) {
//     ::testing::InitGoogleTest(&argc, argv);
//     google::InitGoogleLogging(argv[0]);
//     return RUN_ALL_TESTS();
// }
```

## File: camera/test_camera_params.cc
```
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/common/CameraParams.hpp"
#include "internal/helpers.hpp"



#include <config_utilities/parsing/yaml.h>

#include <gtsam/geometry/Cal3DS2.h>

#include <gflags/gflags.h>
#include <glog/logging.h>
#include <gtest/gtest.h>

#include <thread>

DECLARE_string(test_data_path);

using namespace dyno;

TEST(testCameraParamss, basicConstructionCal3DS2)
{
  // Intrinsics.
  const std::vector<double> intrinsics_expected = { 458.654, 457.296, 367.215, 248.375 };
  //   // Distortion coefficients.
  const std::vector<double> distortion_expected = { -0.28340811, 0.07395907, 0.00019359, 1.76187114e-05 };

  const cv::Size size_expected(752, 480);

  const std::string expected_distortion_model = "radtan";

  // Sensor extrinsics wrt. the body-frame.
  gtsam::Rot3 R_expected(0.0148655429818, -0.999880929698, 0.00414029679422, 0.999557249008, 0.0149672133247,
                         0.025715529948, -0.0257744366974, 0.00375618835797, 0.999660727178);
  gtsam::Point3 T_expected(-0.0216401454975, -0.064676986768, 0.00981073058949);
  gtsam::Pose3 pose_expected(R_expected, T_expected);

  CameraParams params(intrinsics_expected, distortion_expected, size_expected, expected_distortion_model,
                     pose_expected);

  EXPECT_EQ(size_expected.width, params.ImageWidth());
  EXPECT_EQ(size_expected.height, params.ImageHeight());

  // for (int c = 0u; c < 4u; c++)
  // {
  //   EXPECT_DOUBLE_EQ(intrinsics_expected[c], params.intrinsics_[c]);
  // }
  EXPECT_DOUBLE_EQ(intrinsics_expected[0], params.getCameraMatrix().at<double>(0, 0));
  EXPECT_DOUBLE_EQ(intrinsics_expected[1], params.getCameraMatrix().at<double>(1, 1));
  EXPECT_DOUBLE_EQ(intrinsics_expected[2], params.getCameraMatrix().at<double>(0, 2));
  EXPECT_DOUBLE_EQ(intrinsics_expected[3], params.getCameraMatrix().at<double>(1, 2));

  EXPECT_DOUBLE_EQ(intrinsics_expected[0], params.fx());
  EXPECT_DOUBLE_EQ(intrinsics_expected[1], params.fy());
  EXPECT_DOUBLE_EQ(intrinsics_expected[2], params.cu());
  EXPECT_DOUBLE_EQ(intrinsics_expected[3], params.cv());
  // //   EXPECT_EQ(cam_params.intrinsics_.size(), 4u);
  gtsam::Cal3DS2 gtsam_calib = params.constructGtsamCalibration<gtsam::Cal3DS2>();

  EXPECT_DOUBLE_EQ(intrinsics_expected[0], gtsam_calib.fx());
  EXPECT_DOUBLE_EQ(intrinsics_expected[1], gtsam_calib.fy());
  EXPECT_DOUBLE_EQ(0u, gtsam_calib.skew());
  EXPECT_DOUBLE_EQ(intrinsics_expected[2], gtsam_calib.px());
  EXPECT_DOUBLE_EQ(intrinsics_expected[3], gtsam_calib.py());

  EXPECT_TRUE(assert_equal(pose_expected, params.getExtrinsics()));

  for (int c = 0u; c < 4u; c++)
  {
    EXPECT_DOUBLE_EQ(distortion_expected[c], params.getDistortionCoeffs().at<double>(c));
  }
  EXPECT_EQ(params.getDistortionCoeffs().rows, 1u);
  EXPECT_EQ(params.getDistortionCoeffs().cols, 4u);
  EXPECT_DOUBLE_EQ(distortion_expected[0], gtsam_calib.k1());
  EXPECT_DOUBLE_EQ(distortion_expected[1], gtsam_calib.k2());
  EXPECT_DOUBLE_EQ(distortion_expected[2], gtsam_calib.p1());
  EXPECT_DOUBLE_EQ(distortion_expected[3], gtsam_calib.p2());
}


TEST(testCameraParams, parseYAML) {
  // CameraParams cam_params = CameraParams::fromYamlFile(getTestDataPath() + "/sensor.yaml");
  auto cam_params = config::fromYamlFile<CameraParams>(getTestDataPath() + "/sensor.yaml");

  // Frame rate.
  const double frame_rate_expected = 1.0 / 20.0;
  // EXPECT_DOUBLE_EQ(frame_rate_expected, cam_params.frame_rate_);

  // Image size.
  const cv::Size size_expected(752, 480);
  EXPECT_EQ(size_expected.width, cam_params.ImageWidth());
  EXPECT_EQ(size_expected.height, cam_params.ImageHeight());

  // Intrinsics.
  const std::vector<double> intrinsics_expected = {
      458.654, 457.296, 367.215, 248.375};
  EXPECT_DOUBLE_EQ(intrinsics_expected[0], cam_params.fx());
  EXPECT_DOUBLE_EQ(intrinsics_expected[1], cam_params.fy());
  EXPECT_DOUBLE_EQ(intrinsics_expected[2], cam_params.cu());
  EXPECT_DOUBLE_EQ(intrinsics_expected[3], cam_params.cv());

  EXPECT_DOUBLE_EQ(intrinsics_expected[0], cam_params.getCameraMatrix().at<double>(0, 0));
  EXPECT_DOUBLE_EQ(intrinsics_expected[1], cam_params.getCameraMatrix().at<double>(1, 1));
  EXPECT_DOUBLE_EQ(intrinsics_expected[2], cam_params.getCameraMatrix().at<double>(0, 2));
  EXPECT_DOUBLE_EQ(intrinsics_expected[3], cam_params.getCameraMatrix().at<double>(1, 2));
  gtsam::Cal3DS2 gtsam_calib = cam_params.constructGtsamCalibration<gtsam::Cal3DS2>();

  EXPECT_DOUBLE_EQ(intrinsics_expected[0], gtsam_calib.fx());
  EXPECT_DOUBLE_EQ(intrinsics_expected[1], gtsam_calib.fy());
  EXPECT_DOUBLE_EQ(0u, gtsam_calib.skew());
  EXPECT_DOUBLE_EQ(intrinsics_expected[2], gtsam_calib.px());
  EXPECT_DOUBLE_EQ(intrinsics_expected[3], gtsam_calib.py());

  // Sensor extrinsics wrt. the body-frame.
  gtsam::Rot3 R_expected(0.0148655429818,
                         -0.999880929698,
                         0.00414029679422,
                         0.999557249008,
                         0.0149672133247,
                         0.025715529948,
                         -0.0257744366974,
                         0.00375618835797,
                         0.999660727178);
  gtsam::Point3 T_expected(-0.0216401454975, -0.064676986768, 0.00981073058949);
  gtsam::Pose3 pose_expected(R_expected, T_expected);
  EXPECT_TRUE(assert_equal(pose_expected, cam_params.getExtrinsics()));

  // Distortion coefficients.
  const std::vector<double> distortion_expected = {
      -0.28340811, 0.07395907, 0.00019359, 1.76187114e-05};
  for (size_t c = 0u; c < 4u; c++) {
    EXPECT_DOUBLE_EQ(distortion_expected[c],
                     cam_params.getDistortionCoeffs().at<double>(c));
  }
  EXPECT_EQ(cam_params.getDistortionCoeffs().rows, 1u);
  EXPECT_EQ(cam_params.getDistortionCoeffs().cols, 4u);
  EXPECT_DOUBLE_EQ(distortion_expected[0], gtsam_calib.k1());
  EXPECT_DOUBLE_EQ(distortion_expected[1], gtsam_calib.k2());
  EXPECT_DOUBLE_EQ(distortion_expected[2], gtsam_calib.p1());
  EXPECT_DOUBLE_EQ(distortion_expected[3], gtsam_calib.p2());
}

TEST(testCameraParamss, convertDistortionVectorToMatrix)
{
  std::vector<double> distortion_coeffs;

  // 4 distortion params
  distortion_coeffs = { 1.0, -2.0, 1.3, 10 };
  cv::Mat distortion_coeffs_mat;
  CameraParams::convertDistortionVectorToMatrix(distortion_coeffs, &distortion_coeffs_mat);
  EXPECT_EQ(distortion_coeffs_mat.cols, distortion_coeffs.size());
  EXPECT_EQ(distortion_coeffs_mat.rows, 1u);
  for (size_t i = 0u; i < distortion_coeffs.size(); i++)
  {
    EXPECT_EQ(distortion_coeffs_mat.at<double>(0, i), distortion_coeffs.at(i));
  }

  // 5 distortion params
  distortion_coeffs = { 1, 1.2f, 3u, 4l, 5.34 };  //! randomize types as well
  CameraParams::convertDistortionVectorToMatrix(distortion_coeffs, &distortion_coeffs_mat);
  EXPECT_EQ(distortion_coeffs_mat.cols, distortion_coeffs.size());
  EXPECT_EQ(distortion_coeffs_mat.rows, 1u);
  for (size_t i = 0u; i < distortion_coeffs.size(); i++)
  {
    EXPECT_EQ(distortion_coeffs_mat.at<double>(0u, i), distortion_coeffs.at(i));
  }

  // n distortion params
  distortion_coeffs = { 1.0, 1.2, 3.2, 4.3, 5.34, 10203, 1818.9, 1.9 };
  CameraParams::convertDistortionVectorToMatrix(distortion_coeffs, &distortion_coeffs_mat);
  EXPECT_EQ(distortion_coeffs_mat.cols, distortion_coeffs.size());
  EXPECT_EQ(distortion_coeffs_mat.rows, 1u);
  for (size_t i = 0u; i < distortion_coeffs.size(); i++)
  {
    EXPECT_EQ(distortion_coeffs_mat.at<double>(0u, i), distortion_coeffs.at(i));
  }
}
```

## File: camera/test_camera.cc
```
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "internal/helpers.hpp"


#include <gflags/gflags.h>
#include <glog/logging.h>
#include <gtest/gtest.h>

#include "dynosam/common/Camera.hpp"
#include "dynosam/utils/Numerical.hpp"

#include <gtsam/base/numericalDerivative.h>

using namespace dyno;

TEST(Camera, project)
{
  Landmarks lmks;
  lmks.push_back(Landmark(0.0, 0.0, 1.0));
  lmks.push_back(Landmark(0.0, 0.0, 2.0));
  lmks.push_back(Landmark(0.0, 1.0, 2.0));
  lmks.push_back(Landmark(0.0, 10.0, 20.0));
  lmks.push_back(Landmark(1.0, 0.0, 2.0));

  CameraParams::IntrinsicsCoeffs intrinsics(4);
  CameraParams::DistortionCoeffs distortion(4);

  intrinsics.at(0) = 1.0;  // fx
  intrinsics.at(1) = 1.0;  // fy
  intrinsics.at(2) = 3.0;  // u0
  intrinsics.at(3) = 2.0;  // v0
  Keypoints expected_kpts;
  expected_kpts.push_back(Keypoint(intrinsics.at(2), intrinsics.at(3)));
  expected_kpts.push_back(Keypoint(intrinsics.at(2), intrinsics.at(3)));
  expected_kpts.push_back(Keypoint(3.0, 1.0 / 2.0 + 2.0));
  expected_kpts.push_back(Keypoint(3.0, 1.0 / 2.0 + 2.0));
  expected_kpts.push_back(Keypoint(1.0 / 2.0 + 3.0, 2.0));

  CameraParams camera_params(intrinsics, distortion, cv::Size(640, 480), "radtan");

  Camera camera(camera_params);

  Keypoints actual_kpts;
  EXPECT_NO_THROW(camera.project(lmks, &actual_kpts));
  dyno_testing::compareKeypoints(actual_kpts, expected_kpts);
}

TEST(Camera, backProjectSingleSimple)
{
  // Easy test first, back-project keypoint at the center of the image with
  // a given depth.
  CameraParams camera_params = dyno_testing::makeDefaultCameraParams();
  Camera camera(camera_params);

  Keypoint kpt(camera_params.cu(), camera_params.cv());
  Landmark actual_lmk;
  double depth = 2.0;
  camera.backProject(kpt, depth, &actual_lmk);

  Landmark expected_lmk(0.0, 0.0, depth);
  EXPECT_NEAR(expected_lmk.x(), actual_lmk.x(), 0.0001);
  EXPECT_NEAR(expected_lmk.y(), actual_lmk.y(), 0.0001);
  EXPECT_NEAR(expected_lmk.z(), actual_lmk.z(), 0.0001);
}

TEST(Camera, backProjectMultipleSimple)
{
  // Easy test first, back-project keypoints at the center of the image with
  // different depths.
  CameraParams camera_params = dyno_testing::makeDefaultCameraParams();
  Camera camera(camera_params);

  Keypoint kpt(camera_params.cu(), camera_params.cv());
  // Create 3 keypoints centered at image with different depths
  Keypoints kpts(3, kpt);
  Depths depths = { 2.0, 3.0, 4.5 };
  Landmarks actual_lmks;
  camera.backProject(kpts, depths, &actual_lmks);

  Landmarks expected_lmks;
  for (const auto& depth : depths)
  {
    expected_lmks.push_back(Landmark(0.0, 0.0, depth));
  }

  dyno_testing::compareLandmarks(actual_lmks, expected_lmks);
}

TEST(Camera, backProjectSingleTopLeft)
{
  // Back-project keypoint at the center of the image with a given depth.
  CameraParams::IntrinsicsCoeffs intrinsics(4);
  CameraParams::DistortionCoeffs distortion(4);

  double fx = 30.9 / 2.2;
  double fy = 12.0 / 23.0;
  double cu = 390.8;
  double cv = 142.2;

  intrinsics.at(0) = fx;  // fx
  intrinsics.at(1) = fy;  // fy
  intrinsics.at(2) = cu;  // u0
  intrinsics.at(3) = cv;  // v0
  CameraParams camera_params(intrinsics, distortion, cv::Size(640, 480), "radtan");

  Camera camera(camera_params);

  Landmark actual_lmk;
  double depth = 2.0;
  Keypoint kpt(0.0, 0.0);  // Top-left corner
  camera.backProject(kpt, depth, &actual_lmk);

  Landmark expected_lmk(depth / fx * (-cu), depth / fy * (-cv), depth);
  EXPECT_NEAR(expected_lmk.x(), actual_lmk.x(), 0.0001);
  EXPECT_NEAR(expected_lmk.y(), actual_lmk.y(), 0.0001);
  EXPECT_NEAR(expected_lmk.z(), actual_lmk.z(), 0.0001);
}

TEST(Camera, backProjectToZ)
{
  CameraParams camera_params = dyno_testing::makeDefaultCameraParams();
  Camera camera(camera_params);

  Keypoint kpt(camera_params.cu()/2.0, camera_params.cv()/2.0);
  double depth = 2.0;

  Landmark actual_lmk;
  camera.backProject(kpt, depth, &actual_lmk);

  const double Z = actual_lmk(2);
  Landmark z_projected_lmk;
  camera.backProjectFromZ(kpt, Z, &z_projected_lmk);

  //check that the keypoint is the same as the actual one (we just change the Z)
  //but the "measurement" shold remain the same
  Keypoint calculated_kp;
  camera.project(z_projected_lmk, &calculated_kp);
  EXPECT_TRUE(gtsam::assert_equal(kpt, calculated_kp));


}


TEST(Camera, backProjectToZJacobian)
{
  CameraParams camera_params = dyno_testing::makeDefaultCameraParams();
  Camera camera(camera_params);

  Keypoint kpt(camera_params.cu()/2.0, camera_params.cv()/2.0);
  double depth = 2.0;

  Landmark actual_lmk;
  camera.backProject(kpt, depth, &actual_lmk);
  const double Z = actual_lmk(2);


  gtsam::Pose3 pose(gtsam::Rot3::Rodrigues(0,2,3),gtsam::Point3(1,2,0));


  auto numerical_deriv_func =[&camera](const gtsam::Vector3& uvz, const gtsam::Pose3& X_world) -> gtsam::Vector3 {
    Landmark lmk;
    camera.backProjectFromZ(gtsam::Point2(uvz(0), uvz(1)), uvz(2), &lmk, X_world);
    return lmk;
  };


  //construct 3x1 vector of input to satisfy the matrix structure of the problem we want to sove
  gtsam::Vector3 input(kpt(0), kpt(1), Z);
  //numericalDerivative21 -> 2 function arguments, derivative w.r.t uvz
  gtsam::Matrix33 numerical_J = gtsam::numericalDerivative21<gtsam::Vector3, gtsam::Vector3, const gtsam::Pose3&>(
    numerical_deriv_func, input, pose);

  Landmark lmk; //unused
  gtsam::Matrix33 analytical_J;
  camera.backProjectFromZ(kpt, Z, &lmk, pose, analytical_J);

  EXPECT_TRUE(gtsam::assert_equal(numerical_J, analytical_J));

}
```

## File: core_utils/test_code_concepts.cc
```
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include <glog/logging.h>
#include <gtest/gtest.h>
#include <exception>
#include <opencv4/opencv2/opencv.hpp>

#include <type_traits>

#include "dynosam/visualizer/ColourMap.hpp"
#include "dynosam/utils/OpenCVUtils.hpp"
#include "dynosam/logger/Logger.hpp"
#include "dynosam/backend/FactorGraphTools.hpp"
#include "dynosam/backend/BackendDefinitions.hpp"

#include <optional>
#include <atomic>

#include <gtsam/geometry/Pose3.h>
#include <gtsam/geometry/Cal3_S2Stereo.h>
#include <gtsam/nonlinear/Values.h>
#include <gtsam/nonlinear/utilities.h>
#include <gtsam/nonlinear/NonlinearEquality.h>
#include <gtsam/nonlinear/NonlinearFactorGraph.h>
#include <gtsam/nonlinear/LevenbergMarquardtOptimizer.h>
#include <gtsam/inference/Symbol.h>
#include <gtsam/slam/StereoFactor.h>
#include <gtsam/slam/dataset.h>

#include <gtsam_unstable/slam/PoseToPointFactor.h>
#include <gtsam/base/treeTraversal-inst.h>

#include <gtsam/nonlinear/ISAM2.h>
#include <gtsam/base/Matrix.h>

#include <eigen3/unsupported/Eigen/MatrixFunctions>
#include <eigen3/unsupported/Eigen/KroneckerProduct>


#include <string>
#include <fstream>
#include <iostream>


std::vector<gtsam::Point3> createPoints() {

  // Create the set of ground-truth landmarks
  std::vector<gtsam::Point3> points;
  points.push_back(gtsam::Point3(10.0,10.0,10.0));
  points.push_back(gtsam::Point3(-10.0,10.0,10.0));
  points.push_back(gtsam::Point3(-10.0,-10.0,10.0));
  points.push_back(gtsam::Point3(10.0,-10.0,10.0));
  points.push_back(gtsam::Point3(10.0,10.0,-10.0));
  points.push_back(gtsam::Point3(-10.0,10.0,-10.0));
  points.push_back(gtsam::Point3(-10.0,-10.0,-10.0));
  points.push_back(gtsam::Point3(10.0,-10.0,-10.0));

  return points;
}

/* ************************************************************************* */
std::vector<gtsam::Pose3> createPoses(
            const gtsam::Pose3& init = gtsam::Pose3(gtsam::Rot3::Ypr(M_PI/2,0,-M_PI/2), gtsam::Point3(30, 0, 0)),
            const gtsam::Pose3& delta = gtsam::Pose3(gtsam::Rot3::Ypr(0,-M_PI/4,0), gtsam::Point3(sin(M_PI/4)*30, 0, 30*(1-sin(M_PI/4)))),
            int steps = 8) {

  // Create the set of ground-truth poses
  // Default values give a circular trajectory, radius 30 at pi/4 intervals, always facing the circle center
  std::vector<gtsam::Pose3> poses;
  int i = 1;
  poses.push_back(init);
  for(; i < steps; ++i) {
    poses.push_back(poses[i-1].compose(delta));
  }

  return poses;
}

cv::Mat GetSquareImage( const cv::Mat& img, int target_width = 500 )
{
    int width = img.cols,
       height = img.rows;

    cv::Mat square = cv::Mat::zeros( target_width, target_width, img.type() );

    int max_dim = ( width >= height ) ? width : height;
    float scale = ( ( float ) target_width ) / max_dim;
    cv::Rect roi;
    if ( width >= height )
    {
        roi.width = target_width;
        roi.x = 0;
        roi.height = height * scale;
        roi.y = ( target_width - roi.height ) / 2;
    }
    else
    {
        roi.y = 0;
        roi.height = target_width;
        roi.width = width * scale;
        roi.x = ( target_width - roi.width ) / 2;
    }

    cv::resize( img, square( roi ), roi.size() );

    return square;
}

//we need compose, inverse (and thats it?)

template<typename T>
struct indexed_transform_traits;

//actually just want this for Pose operations, i guess? Do I both generalising?
// this should happily cover all Eigen stuff as well :)
template<typename T, class = typename  std::enable_if<is_gtsam_value_v<T>>>
struct GtsamIndexedTransformTraits {

    static T Inverse(const T& t) {
        return gtsam::traits<T>::Inverse(t);
    }

    static T Compose(const T& g, const T& h) {
        return gtsam::traits<T>::Compose(g, h);
    }


};
template<typename T>
struct indexed_transform_traits : GtsamIndexedTransformTraits<T> {};


/**
 * @brief Pose class that add the 3-indexed notation from the "Pose changes
 * from a different point of view" paper
 *
 */
template<typename T>
class IndexedTransform {
public:
  using This = IndexedTransform<T>;

  // Top left index representing observing frame
  std::string tl;
  // Bottom left index, representing from frame
  std::string bl{};
  // Bottom right index, representing to frame
  std::string br{};
  // Data
  T transform;

  IndexedTransform(
    const std::string& top_left,
    const std::string& bottom_left,
    const std::string& bottom_right,
    const T& t)
    :   tl(top_left), bl(bottom_left), br(bottom_right), transform(t) {}

  const std::string& Origin() const { return tl; }
  const std::string& From() const { return bl; }
  const std::string& To() const { return br; }

  /**
   * @brief Returns true if the captured data represents a pose or a motion.
   * As per Equation 10 and 11 we can, if the top left and bottom left notation is the same,
   * it can be re-written in the ususal two index form
   *
   * @return true
   * @return false
   */
  inline bool isCoordinateTransform() const {
    return tl == bl;
  }

  operator T() { return transform; }

  This compose(const This& t) const {
    // T new_transform = transform * t.transform;
    T new_transform = indexed_transform_traits<T>::Compose(transform, t.transform);

    //if this is relative (left hand side), both must be relative
    if(isCoordinateTransform()) {
        if (!t.isCoordinateTransform()) {
            throw std::runtime_error("");
        }
        else {
            //check that the coordinates match
            if(br != t.tl) {
                //throw exception
                throw std::runtime_error("");
            }

            //transform is good, update the bottom right frame (to frame)
            std::string new_br = t.br;
            return IndexedTransform(
                tl,
                bl,
                new_br,
                new_transform
            );
        }
    }
    else {
        //we are global motion
        //if this is global (left hand side), the RHS (t) can be either relative
        //or global as long as the transforms match up
        //t's origin must match this (LHS) origin
        //t's to frame must match this (LHS) from frame
        if((t.Origin() != Origin()) || (t.To() != From())) {
             throw std::runtime_error("");
        }

        if(t.isCoordinateTransform()) {
            std::string new_bl = t.From();
            //this should match the origin
            assert(new_bl == Origin());
            std::string new_br = t.To();
            return IndexedTransform(
                Origin(),
                new_bl,
                new_br,
                new_transform
            );
        }
        else {
            std::string new_bl = t.From();
            std::string new_br = From();
            return IndexedTransform(
                Origin(),
                new_bl,
                new_br,
                new_transform
            );

        }
    }


  }

  This operator*(const This& t) const {
    return compose(t);
  }

  This decompose(const This& t) const {
    return inverse().compose(t);
  }

  //update this one?
  //maybe an invert and inverse (invert operates on this one)
  This inverse() const {
    This tmp = *this;
    tmp.transform = indexed_transform_traits<T>::Inverse(transform);

    //always swap bottom index's
    std::swap(tmp.bl, tmp.br);

    //coordinate transform, this we also update the top left with the new bottom left
    if(isCoordinateTransform()) {
        tmp.tl = tmp.bl;
    }
    return tmp;
  }

};

TEST(CodeConcepts, threeIndexNotationRelativeInverse) {
    gtsam::Pose3 A_B(gtsam::Rot3::Rodrigues(0.1,-0.2,0.01),gtsam::Point3(0.5,3,1));
    IndexedTransform<gtsam::Pose3> a_b("a", "a", "b", A_B);
    EXPECT_TRUE(a_b.isCoordinateTransform());

    auto b_a = a_b.inverse();
    EXPECT_EQ(b_a.Origin(), "b");
    EXPECT_EQ(b_a.From(), "b");
    EXPECT_EQ(b_a.To(), "a");

    EXPECT_EQ(a_b.Origin(), "a");
    EXPECT_EQ(a_b.From(), "a");
    EXPECT_EQ(a_b.To(), "b");


    EXPECT_TRUE(b_a.isCoordinateTransform());
    EXPECT_TRUE(gtsam::assert_equal(A_B.inverse(), b_a.transform));

}

TEST(CodeConcepts, threeIndexNotationAbsoluteInverse) {
    gtsam::Pose3 W_AB(gtsam::Rot3::Rodrigues(0.1,-0.2,0.01),gtsam::Point3(0.5,3,1));
    IndexedTransform<gtsam::Pose3> w_ab("w", "a", "b", W_AB);
    EXPECT_FALSE(w_ab.isCoordinateTransform());

    auto w_ba = w_ab.inverse();
    EXPECT_EQ(w_ba.Origin(), "w");
    EXPECT_EQ(w_ba.From(), "b");
    EXPECT_EQ(w_ba.To(), "a");

    EXPECT_EQ(w_ab.Origin(), "w");
    EXPECT_EQ(w_ab.From(), "a");
    EXPECT_EQ(w_ab.To(), "b");


    EXPECT_FALSE(w_ba.isCoordinateTransform());
    EXPECT_TRUE(gtsam::assert_equal(W_AB.inverse(), w_ba.transform));

}


TEST(CodeConcepts, threeIndexNotationRelativeComposition) {
    gtsam::Pose3 A(gtsam::Rot3::Rodrigues(0.3,2,3),gtsam::Point3(1,2,2));
    gtsam::Pose3 A_B(gtsam::Rot3::Rodrigues(0.1,-0.2,0.01),gtsam::Point3(0.5,3,1));
    IndexedTransform<gtsam::Pose3> a("w", "w", "a", A);
    IndexedTransform<gtsam::Pose3> a_b("a", "a", "b", A_B);

    auto result = a * a_b;
    EXPECT_EQ(result.Origin(), "w");
    EXPECT_EQ(result.From(), "w");
    EXPECT_EQ(result.To(), "b");
    EXPECT_TRUE(gtsam::assert_equal(A * A_B, result.transform));
}


//previsitor
struct Node {
    void operator()(
        const boost::shared_ptr<gtsam::ISAM2Clique>& clique) {
            auto conditional = clique->conditional();
            //it is FACTOR::const_iterator
            for(auto it = conditional->beginFrontals(); it != conditional->endFrontals(); it++) {
                LOG(INFO) << dyno::DynoLikeKeyFormatter(*it);
            }
        }
};


// TEST(CodeConcepts, getISAM2Ordering) {
//     using namespace gtsam;
//     Cal3_S2::shared_ptr K(new Cal3_S2(50.0, 50.0, 0.0, 50.0, 50.0));

//   // Define the camera observation noise model, 1 pixel stddev
//   auto measurementNoise = noiseModel::Isotropic::Sigma(2, 1.0);

//   // Create the set of ground-truth landmarks
//   std::vector<Point3> points = createPoints();

//   // Create the set of ground-truth poses
//   std::vector<Pose3> poses = createPoses();

//   // Create an iSAM2 object. Unlike iSAM1, which performs periodic batch steps
//   // to maintain proper linearization and efficient variable ordering, iSAM2
//   // performs partial relinearization/reordering at each step. A parameter
//   // structure is available that allows the user to set various properties, such
//   // as the relinearization threshold and type of linear solver. For this
//   // example, we we set the relinearization threshold small so the iSAM2 result
//   // will approach the batch result.
//   ISAM2Params parameters;
//   parameters.relinearizeThreshold = 0.01;
//   parameters.relinearizeSkip = 1;
//   ISAM2 isam(parameters);

//   // Create a Factor Graph and Values to hold the new data
//   NonlinearFactorGraph graph;
//   Values initialEstimate;

//   // Loop over the poses, adding the observations to iSAM incrementally
//   for (size_t i = 0; i < poses.size(); ++i) {
//     // Add factors for each landmark observation
//     for (size_t j = 0; j < points.size(); ++j) {
//       PinholeCamera<Cal3_S2> camera(poses[i], *K);
//       Point2 measurement = camera.project(points[j]);
//       graph.emplace_shared<GenericProjectionFactor<Pose3, Point3, Cal3_S2> >(
//           measurement, measurementNoise, Symbol('x', i), Symbol('l', j), K);
//     }

//     // Add an initial guess for the current pose
//     // Intentionally initialize the variables off from the ground truth
//     static Pose3 kDeltaPose(Rot3::Rodrigues(-0.1, 0.2, 0.25),
//                             Point3(0.05, -0.10, 0.20));
//     initialEstimate.insert(Symbol('x', i), poses[i] * kDeltaPose);

//     // If this is the first iteration, add a prior on the first pose to set the
//     // coordinate frame and a prior on the first landmark to set the scale Also,
//     // as iSAM solves incrementally, we must wait until each is observed at
//     // least twice before adding it to iSAM.
//     if (i == 0) {
//       // Add a prior on pose x0, 30cm std on x,y,z and 0.1 rad on roll,pitch,yaw
//       static auto kPosePrior = noiseModel::Diagonal::Sigmas(
//           (Vector(6) << Vector3::Constant(0.1), Vector3::Constant(0.3))
//               .finished());
//       graph.addPrior(Symbol('x', 0), poses[0], kPosePrior);

//       // Add a prior on landmark l0
//       static auto kPointPrior = noiseModel::Isotropic::Sigma(3, 0.1);
//       graph.addPrior(Symbol('l', 0), points[0], kPointPrior);

//       // Add initial guesses to all observed landmarks
//       // Intentionally initialize the variables off from the ground truth
//       static Point3 kDeltaPoint(-0.25, 0.20, 0.15);
//       for (size_t j = 0; j < points.size(); ++j)
//         initialEstimate.insert<Point3>(Symbol('l', j), points[j] + kDeltaPoint);

//     } else {
//       // Update iSAM with the new factors
//       isam.update(graph, initialEstimate);
//       // Each call to iSAM2 update(*) performs one iteration of the iterative
//       // nonlinear solver. If accuracy is desired at the expense of time,
//       // update(*) can be called additional times to perform multiple optimizer
//       // iterations every step.
//       isam.update();
//       Values currentEstimate = isam.calculateEstimate();

//       // Clear the factor graph and values for the next iteration
//       graph.resize(0);
//       initialEstimate.clear();
//     }
//   }

//     // isam.saveGraph(dyno::getOutputFilePath("test_bayes_tree.dot"));

//     std::function<void(const boost::shared_ptr<gtsam::ISAM2Clique>&)> node_func = Node();
//     // dyno::factor_graph_tools::travsersal::depthFirstTraversalEliminiationOrder(isam, node_func);
//     LOG(INFO) << dyno::container_to_string(dyno::factor_graph_tools::travsersal::getEliminatonOrder(isam));
//     // Data rootdata;
//     // Node preVisitor;
//     // no_op postVisitor;
//     // gtsam::treeTraversal::DepthFirstForest(isam, rootdata, preVisitor, postVisitor);

// }

// //https://github.com/zhixy/SolveAXXB/blob/master/axxb/conventionalaxxbsvdsolver.cc
// TEST(CodeConcepts, sylvesterEquation) {
//     // Eigen::MatrixXd A;
//     // A.resize(4, 4);
//     // A << 1, 0, 2, 3,
//     //      4, 1, 0, 2,
//     //      0, 5, 5, 6,
//     //      1, 7, 9, 0;

//     gtsam::Pose3 L_k_1 = dyno::utils::createRandomAroundIdentity<gtsam::Pose3>(0.1);
//     gtsam::Pose3 L_k_1_H_k = dyno::utils::createRandomAroundIdentity<gtsam::Pose3>(0.03);

//     //L_k = L_K_1 * ^{L_k}_kH_{k-1}
//     gtsam::Pose3 L_k = L_k_1 * L_k_1_H_k ;
//     gtsam::Pose3 W_k_1_H_k = L_k_1 * L_k_1_H_k * L_k_1.inverse();



//     Eigen::MatrixXd A = W_k_1_H_k.matrix();
//     Eigen::MatrixXd B = L_k_1_H_k.matrix();

//     //1 as we only have one A here
//     Eigen::MatrixXd m = Eigen::MatrixXd::Zero(12*1,12);
//     Eigen::VectorXd b = Eigen::VectorXd::Zero(12*1);

//     Eigen::Matrix3d Ra = A.topLeftCorner(3,3);
//     Eigen::Vector3d Ta = A.topRightCorner(3,1);
//     Eigen::Matrix3d Rb = B.topLeftCorner(3,3);
//     Eigen::Vector3d Tb = B.topRightCorner(3,1);

//     m.block<9,9>(12*0,0) = Eigen::MatrixXd::Identity(9,9) - Eigen::kroneckerProduct(Ra,Rb);
//     m.block<3,9>(12*0+9,0) = Eigen::kroneckerProduct(Eigen::MatrixXd::Identity(3,3),Tb.transpose());
//     m.block<3,3>(12*0+9,9) = Eigen::MatrixXd::Identity(3,3) - Ra;
//     b.block<3,1>(12*0+9,0) = Ta;

//     // //different to skew in gtsam...
//     // auto skew =[](const Eigen::Vector3d& u) {
//     //     Eigen::Matrix3d u_hat = Eigen::MatrixXd::Zero(3,3);
//     //     u_hat(0,1) = u(2);
//     //     u_hat(1,0) = -u(2);
//     //     u_hat(0,2) = -u(1);
//     //     u_hat(2,0) = u(1);
//     //     u_hat(1,2) = u(0);
//     //     u_hat(2,1) = -u(0);
//     //     return u_hat;
//     // };
//     // Eigen::Matrix3d Ta_skew = skew(Ta);
//     // // m.block<3,9>(12*0+9,0) = Eigen::kroneckerProduct(Eigen::MatrixXd::Identity(3,3),Tb.transpose());
//     // // m.block<3,3>(12*0+9,9) = Eigen::MatrixXd::Identity(3,3) - Ra;
//     // // b.block<3,1>(12*0+9,0) = Ta;
//     // m.block<3,9>(12*0+9,0) = Eigen::kroneckerProduct(Ta_skew,Tb.transpose());
//     // m.block<3,3>(12*0+9,9) = Ta_skew - Ta_skew*Ra;

//     // Eigen::JacobiSVD<Eigen::MatrixXd> svd( m, Eigen::ComputeFullV | Eigen::ComputeFullU );
//     // CHECK(svd.computeV())<<"fail to compute V";

//     Eigen::Matrix<double, 12, 1> x = m.bdcSvd(Eigen::ComputeThinU | Eigen::ComputeThinV).solve(b);
//     Eigen::Matrix3d R = Eigen::Map< Eigen::Matrix<double, 3, 3, Eigen::RowMajor> >(x.data()); //row major

//     LOG(INFO) << x;

//     Eigen::JacobiSVD<Eigen::MatrixXd> svd(R, Eigen::ComputeThinU | Eigen::ComputeThinV);
//     gtsam::Matrix44 handeyetransformation = gtsam::Matrix44::Identity();
//     handeyetransformation.topLeftCorner(3,3) = svd.matrixU() * svd.matrixV().transpose();
//     handeyetransformation.topRightCorner(3,1) = x.block<3,1>(9,0);

//     handeyetransformation.topRightCorner(3,1) = x.block<3,1>(9,0);


//     // Eigen::Matrix3d R_alpha;
//     // R_alpha.row(0) = svd.matrixV().block<3,1>(0,11).transpose();
//     // R_alpha.row(1) = svd.matrixV().block<3,1>(3,11).transpose();
//     // R_alpha.row(2) = svd.matrixV().block<3,1>(6,11).transpose();
//     // //double a = std::fabs(R_alpha.determinant());
//     // //double alpha = R_alpha.determinant()/(pow(std::fabs(R_alpha.determinant()),4./3.));
//     // double det = R_alpha.determinant();
//     // double alpha = std::pow(std::abs(det),4./3.)/det;
//     // Eigen::HouseholderQR<Eigen::Matrix3d> qr(R_alpha/alpha);

//     // gtsam::Matrix44 handeyetransformation = gtsam::Matrix44::Identity();
//     // Eigen::Matrix3d Q = qr.householderQ();
//     // Eigen::Matrix3d Rwithscale = alpha*Q.transpose()*R_alpha;
//     // Eigen::Vector3d R_diagonal = Rwithscale.diagonal();
//     // for(int i=0;i<3;i++)
//     // {
//     //     handeyetransformation.block<3,1>(0,i) = int(R_diagonal(i)>=0?1:-1)*Q.col(i);
//     // }

//     // handeyetransformation.topRightCorner(3,1) = svd.matrixV().block<3,1>(9,11)/alpha;

//     // LOG(INFO) << A;
//     // // LOG(INFO) << B;
//     // LOG(INFO) << handeyetransformation;


//     // // gtsam::Matrix44 calc_C = A*handeyetransformation - handeyetransformation * B;
//     LOG(INFO) << L_k_1;

//      LOG(INFO) << A*handeyetransformation;
//     LOG(INFO) << handeyetransformation * B;

//     LOG(INFO) << handeyetransformation;



//     // Eigen::RealSchur<Eigen::MatrixXd> SchurA(A);
//     // Eigen::MatrixXd R = SchurA.matrixT();
//     // Eigen::MatrixXd U = SchurA.matrixU();


//     // // Eigen::MatrixXd B = -A.transpose();
//     // Eigen::MatrixXd B;
//     // B.resize(2, 2);
//     // B << 0, -1,
//     //      1, 0;

//     // Eigen::RealSchur<Eigen::MatrixXd> SchurB(B);
//     // Eigen::MatrixXd S = SchurB.matrixT();
//     // Eigen::MatrixXd V = SchurB.matrixU();

//     // //C
//     // // Eigen::MatrixXd I_33 =  gtsam::Matrix33::Zero();
//     // Eigen::MatrixXd C;
//     // C.resize(4, 2);
//     // C << 1, 0,
//     //      2, 0,
//     //      0, 3,
//     //      1, 1;

//     // Eigen::MatrixXd F = (U.adjoint() * C) * V;

//     // Eigen::MatrixXd Y = Eigen::internal::matrix_function_solve_triangular_sylvester(R, S, F);

//     // Eigen::MatrixXd X = (U * Y) * V.adjoint();
//     // LOG(INFO) << "X= " << X;

//     // Eigen::MatrixXd C_calc = A * X + X * B;
//     // LOG(INFO) << "C calc= " << C_calc;

// }


TEST(CodeConcepts, drawInformationMatrix) {
    gtsam::Values initial_estimate;
    gtsam::NonlinearFactorGraph graph;
    const auto model = gtsam::noiseModel::Isotropic::Sigma(3, 1);

    using namespace gtsam::symbol_shorthand;


    // //smaller example
    // gtsam::Pose3 first_pose;
    // graph.emplace_shared<gtsam::NonlinearEquality<gtsam::Pose3> >(X(1), gtsam::Pose3());

    // // create factor noise model with 3 sigmas of value 1
    // // const auto model = gtsam::noiseModel::Isotropic::Sigma(3, 1);
    // // create stereo camera calibration object with .2m between cameras
    // const gtsam::Cal3_S2Stereo::shared_ptr K(
    //     new gtsam::Cal3_S2Stereo(1000, 1000, 0, 320, 240, 0.2));

    // //create and add stereo factors between first pose (key value 1) and the three landmarks
    // // graph.emplace_shared<gtsam::GenericStereoFactor<gtsam::Pose3,gtsam::Point3> >(gtsam::StereoPoint2(520, 480, 440), model, 1, 3, K);
    // // graph.emplace_shared<gtsam::GenericStereoFactor<gtsam::Pose3,gtsam::Point3> >(gtsam::StereoPoint2(120, 80, 440), model, 1, 4, K);
    // // graph.emplace_shared<gtsam::GenericStereoFactor<gtsam::Pose3,gtsam::Point3> >(gtsam::StereoPoint2(320, 280, 140), model, 1, 5, K);

    // // //create and add stereo factors between second pose and the three landmarks
    // // graph.emplace_shared<gtsam::GenericStereoFactor<gtsam::Pose3,gtsam::Point3> >(gtsam::StereoPoint2(570, 520, 490), model, 2, 3, K);
    // // graph.emplace_shared<gtsam::GenericStereoFactor<gtsam::Pose3,gtsam::Point3> >(gtsam::StereoPoint2(70, 20, 490), model, 2, 4, K);
    //  graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3,gtsam::Point3> >(X(1), L(1), gtsam::Point3(1, 1, 5), model);
    // graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3,gtsam::Point3> >(X(1), L(2),  gtsam::Point3(1, 1, 5), model);
    // graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3,gtsam::Point3> >(X(1), L(3), gtsam::Point3::Identity(), model);

    // //create and add stereo factors between second pose and the three landmarks
    // graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3,gtsam::Point3> >(X(2),L(1), gtsam::Point3::Identity(), model);
    // graph.emplace_shared<gtsam::PoseToPointFactor<gtsam::Pose3,gtsam::Point3> >(X(2), L(2), gtsam::Point3::Identity(), model);
    // // graph.emplace_shared<gtsam::GenericStereoFactor<gtsam::Pose3,gtsam::Point3> >(gtsam::StereoPoint2(320, 270, 115), model, 2, 5, K);

    // // create Values object to contain initial estimates of camera poses and
    // // landmark locations

    // // create and add iniital estimates
    // initial_estimate.insert(X(1), first_pose);
    // initial_estimate.insert(X(2), gtsam::Pose3(gtsam::Rot3(), gtsam::Point3(0.1, -0.1, 1.1)));
    // initial_estimate.insert(L(1), gtsam::Point3(1, 1, 5));
    // initial_estimate.insert(L(2), gtsam::Point3(-1, 1, 5));
    // initial_estimate.insert(L(3), gtsam::Point3(1, -0.5, 5));

    // std::string calibration_loc = gtsam::findExampleDataFile("VO_calibration.txt");
    // std::string pose_loc = gtsam::findExampleDataFile("VO_camera_poses_large.txt");
    // std::string factor_loc = gtsam::findExampleDataFile("VO_stereo_factors_large.txt");

    // // read camera calibration info from file
    // // focal lengths fx, fy, skew s, principal point u0, v0, baseline b
    // double fx, fy, s, u0, v0, b;
    // std::ifstream calibration_file(calibration_loc.c_str());
    // std::cout << "Reading calibration info" << std::endl;
    // calibration_file >> fx >> fy >> s >> u0 >> v0 >> b;

    // // create stereo camera calibration object
    // const gtsam::Cal3_S2Stereo::shared_ptr K(new gtsam::Cal3_S2Stereo(fx, fy, s, u0, v0, b));

    // std::ifstream pose_file(pose_loc.c_str());
    // std::cout << "Reading camera poses" << std::endl;
    // int pose_id;
    // gtsam::MatrixRowMajor m(4, 4);
    // // read camera pose parameters and use to make initial estimates of camera
    // // poses
    // while (pose_file >> pose_id) {
    //     for (int i = 0; i < 16; i++) {
    //         pose_file >> m.data()[i];
    //     }
    //     initial_estimate.insert(gtsam::Symbol('x', pose_id), gtsam::Pose3(m));
    // }

    // // camera and landmark keys
    // size_t x, l;

    // // pixel coordinates uL, uR, v (same for left/right images due to
    // // rectification) landmark coordinates X, Y, Z in camera frame, resulting from
    // // triangulation
    // double uL, uR, v, X, Y, Z;
    // std::ifstream factor_file(factor_loc.c_str());
    // std::cout << "Reading stereo factors" << std::endl;
    // // read stereo measurement details from file and use to create and add
    // // GenericStereoFactor objects to the graph representation
    // while (factor_file >> x >> l >> uL >> uR >> v >> X >> Y >> Z) {
    //     graph.emplace_shared<gtsam::GenericStereoFactor<gtsam::Pose3, gtsam::Point3> >(
    //         gtsam::StereoPoint2(uL, uR, v), model, gtsam::Symbol('x', x), gtsam::Symbol('l', l), K);
    //     // if the landmark variable included in this factor has not yet been added
    //     // to the initial variable value estimate, add it
    //     if (!initial_estimate.exists(gtsam::Symbol('l', l))) {
            // gtsam::Pose3 camPose = initial_estimate.at<gtsam::Pose3>(gtsam::Symbol('x', x));
    //         // transformFrom() transforms the input Point3 from the camera pose space,
    //         // camPose, to the global space
    //         gtsam::Point3 worldPoint = camPose.transformFrom(gtsam::Point3(X, Y, Z));
    //         initial_estimate.insert(gtsam::Symbol('l', l), worldPoint);
    //     }
    // }

    // auto gfg = graph.linearize(initial_estimate);
    // // gtsam::Matrix jacobian = gfg->jacobian().first;

    // gtsam::Ordering natural_ordering = gtsam::Ordering::Natural(*gfg);
    // gtsam::JacobianFactor jf(*gfg, natural_ordering);
    // gtsam::Matrix J = jf.jacobian().first;

    // {
    //     gtsam::Values pose_only_values;
    //     pose_only_values.insert(0, gtsam::Pose3::Identity());
    //     pose_only_values.insert(1, gtsam::Pose3::Identity());
    //     gtsam::NonlinearFactorGraph pose_only_graph;
    //     pose_only_graph.emplace_shared<gtsam::PriorFactor<gtsam::Pose3>>(0, gtsam::Pose3::Identity(), gtsam::noiseModel::Isotropic::Sigma(6, 1));
    //     pose_only_graph.emplace_shared<gtsam::PriorFactor<gtsam::Pose3>>(1, gtsam::Pose3::Identity(), gtsam::noiseModel::Isotropic::Sigma(6, 1));


    //     gtsam::Ordering natural_ordering_pose = gtsam::Ordering::Natural(pose_only_graph);
    //     auto gfg_pose = pose_only_graph.linearize(pose_only_values);
    //     gtsam::JacobianFactor jf_pose(*gfg_pose, natural_ordering_pose);

    //         // auto size = natural_ordering.size();
    //         // auto size1 = jf_pose.cols() * 6;
    //     //cols should be num variables * 6 as each pose has dimenstions variables
    //     EXPECT_EQ(natural_ordering_pose.size() * 6,(jf_pose.getA().cols()));

    //     //this just a view HORIZONTALLY, vertically it is the entire matrix
    //     const gtsam::VerticalBlockMatrix::constBlock Ablock = jf_pose.getA(jf_pose.find(0));
    //     //in this example there should be 2? blocks
    //     // LOG(INFO) << "Num blocks " << Ablock.nBlocks();
    //     LOG(INFO) << Ablock;

    //     for(gtsam::Key key : natural_ordering_pose) {
    //     //this will have rows = J.rows(), cols = dimension of the variable
    //     const gtsam::VerticalBlockMatrix::constBlock Ablock = jf_pose.getA(jf_pose.find(key));
    //     const size_t var_dimensions = Ablock.cols();cv::Vec3b(255, 0, 0);
    //     EXPECT_EQ(Ablock.rows(), jf_pose.rows());

    //     LOG(INFO) << "Start Col " << Ablock.startCol() << " Start row " << Ablock.startRow() << " dims " << var_dimensions;
    //     //eachs start col shoudl be dim(key) apart
    //     // for (int i = 0; i < J.rows(); ++i) {
    //     //     for (int j = 0; j < J.cols(); ++j) {
    //     //         if (std::fabs(J(i, j)) > 1e-15) {
    //     //             // make non zero elements blue
    //     //             J_img.at<cv::Vec3b>(i, j) = cv::Vec3b(255, 0, 0);
    //     //         }
    //     //     }
    //     // }

    //     }

    // }

    // graph.saveGraph(dyno::getOutputFilePath("test_graph.dot"));

    // LOG(INFO) << natural_ordering.size();
    // LOG(INFO) << J.cols();

    // cv::Mat J_img(cv::Size(J.cols(), J.rows()), CV_8UC3, cv::Scalar(255, 255, 255));

    // std::vector<std::pair<gtsam::Key, cv::Mat>> column_blocks;
    // LOG(INFO) << J_img.cols;
    // for(gtsam::Key key : natural_ordering) {
    //     //this will have rows = J.rows(), cols = dimension of the variable
    //     // LOG(INFO) << key;
    //     const gtsam::VerticalBlockMatrix::constBlock Ablock = jf.getA(jf.find(key));
    //     const size_t var_dimensions = Ablock.cols();
    //     EXPECT_EQ(Ablock.rows(), J.rows());

    //     // LOG(INFO) << "Start Col " << Ablock.startCol() << " Start row " << Ablock.startRow() << " dims " << var_dimensions;



    //     for (int i = 0; i < J.rows(); ++i) {
    //         for (int j = Ablock.startCol(); j < (Ablock.startCol() + var_dimensions); ++j) {
    //             ASSERT_LT(j, J_img.cols);
    //             ASSERT_LT(i, J_img.rows);
    //             if (std::fabs(J(i, j)) > 1e-15) {
    //                 // make non zero elements blue
    //                 const auto colour = dyno::ColourMap::getObjectColour((int)var_dimensions);
    //                 J_img.at<cv::Vec3b>(i, j) =  cv::Vec3b(colour[0], colour[1], colour[2]);
    //                 // J_img.at<cv::Vec3b>(i, j) = cv::Vec3b(255, 0, 0);
    //                 // J_img.at<cv::Vec3b>(i, j)[2] = colour[2];
    //             }
    //         }
    //     }

    //     cv::Mat b_img(cv::Size(var_dimensions, Ablock.rows()), CV_8UC3, cv::Scalar(255, 255, 255));
    //     for (int i = 0; i < Ablock.rows(); ++i) {
    //         for (int j = 0; j < var_dimensions; ++j) {
    //             // ASSERT_LT(j, J_img.cols);
    //             // ASSERT_LT(i, J_img.rows);
    //             if (std::fabs(Ablock(i, j)) > 1e-15) {
    //                 // make non zero elements blue
    //                 const auto colour = dyno::ColourMap::getObjectColour((int)var_dimensions);
    //                 b_img.at<cv::Vec3b>(i, j) =  cv::Vec3b(colour[0], colour[1], colour[2]);
    //                 // J_img.at<cv::Vec3b>(i, j) = cv::Vec3b(255, 0, 0);
    //                 // J_img.at<cv::Vec3b>(i, j)[2] = colour[2];
    //             }
    //         }
    //     }

    //     column_blocks.push_back(std::make_pair(key, b_img));

    // }

    // LOG(INFO) << column_blocks.size();

    // cv::Size desired_size(480, 480);

    // const int current_cols = J.cols();
    // const int desired_cols = desired_size.width;
    // const int desired_rows = desired_size.height;

    // double ratio = (double)desired_cols/(double)current_cols;
    // LOG(INFO) << ratio;
    // cv::Mat concat_column_blocks;
    // // cv::Mat concat_column_blocks = column_blocks.at(0);
    // cv::Mat labels;

    // for(size_t i = 1; i < column_blocks.size(); i++) {
    //     gtsam::Key key = column_blocks.at(i).first;
    //     cv::Mat current_block = column_blocks.at(i).second;
    //     int scaled_cols = ratio * (int)current_block.cols;
    //     cv::resize(current_block, current_block, cv::Size(scaled_cols, desired_rows), 0, 0, cv::INTER_NEAREST);

    //     //draw text info
    //     int baseline=0;
    //     // cv::Size textSize = cv::getTextSize(gtsam::DefaultKeyFormatter(key),cv::FONT_HERSHEY_PLAIN,1,1,&baseline);

    //     const int text_box_height = 30;
    //     cv::Mat text_box(cv::Size(current_block.cols, text_box_height), CV_8UC3, cv::Scalar(255, 255, 255));

    //     constexpr static double kFontScale = 0.5;
    //     constexpr static int kFontFace = cv::FONT_HERSHEY_SIMPLEX;
    //     constexpr static int kThickness = 2;
    //     //draw text mid way in box
    //     cv::putText(text_box, gtsam::DefaultKeyFormatter(key), cv::Point(text_box.cols/2, text_box.rows/2), kFontFace, kFontScale, cv::Scalar(0, 0, 0), kThickness);

    //     // cv::imshow("text", text_box);
    //     // cv::waitKey(0);

    //     cv::vconcat(text_box, current_block, current_block);

    //     if(i == 1) {
    //         cv::Mat first_block = column_blocks.at(0).second;
    //         gtsam::Key key_0 = column_blocks.at(0).first;

    //         int first_scaled_cols = ratio * first_block.cols;
    //         cv::resize(first_block, first_block, cv::Size(first_scaled_cols, desired_rows), 0, 0, cv::INTER_NEAREST);

    //         //add text
    //         cv::Mat text_box_first(cv::Size(first_block.cols, text_box_height), CV_8UC3, cv::Scalar(255, 255, 255));
    //         cv::putText(text_box, gtsam::DefaultKeyFormatter(key_0), cv::Point(text_box_first.cols/2, text_box_first.rows/2), kFontFace, kFontScale, cv::Scalar(0, 0, 0), kThickness);

    //         //concat text box on top of jacobian block
    //         cv::vconcat(text_box, first_block, first_block);
    //         //now concat with next jacobian block
    //         cv::hconcat(first_block, current_block, concat_column_blocks);

    //     }
    //     else {
    //         //concat current jacobian block with other blocks
    //         cv::hconcat(concat_column_blocks, current_block, concat_column_blocks);
    //     }

    //     //if not last add vertical line
    //     if(i < column_blocks.size() - 1) {
    //         cv::Mat vert_line(cv::Size(2, concat_column_blocks.rows), CV_8UC3, cv::Scalar(0, 0, 0));
    //         //concat blocks with vertial line
    //         cv::hconcat(concat_column_blocks, vert_line, concat_column_blocks);
    //         // concat_column_blocks = dyno::utils::concatenateImagesHorizontally(concat_column_blocks, vert_line);
    //     }


    //     // //all blocks should have the same number of rows
    //     // double scaled_cols =

    //     // //add vertical line between each variable block
    //     // cv::Mat vert_line(cv::Size(2, concat_column_blocks.rows), CV_8UC3, cv::Scalar(0, 0, 0));
    //     // concat_column_blocks = dyno::utils::concatenateImagesHorizontally(concat_column_blocks, vert_line);
    // }

    // dyno::NonlinearFactorGraphManager nlfg(graph, initial_estimate);

    // cv::Mat J_1 = nlfg.drawBlockJacobian(gtsam::Ordering::NATURAL, dyno::factor_graph_tools::DrawBlockJacobiansOptions::makeDynoSamOptions());

    // for (int i = 0; i < J.rows(); ++i) {
    //     for (int j = 0; j < J.cols(); ++j) {
    //         if (std::fabs(J(i, j)) > 1e-15) {
    //             // make non zero elements blue
    //             J_img.at<cv::Vec3b>(i, j) = cv::Vec3b(255, 0, 0);
    //         }
    //     }
    // }

    // gtsam::Matrix I = jf.information();
    // cv::Mat I_img(cv::Size(I.rows(), I.cols()), CV_8UC3, cv::Scalar(255, 255, 255));
    // for (int i = 0; i < I.rows(); ++i) {
    //     for (int j = 0; j < I.cols(); ++j) {
    //         if (std::fabs(I(i, j)) > 1e-15) {
    //             // make non zero elements blue
    //             I_img.at<cv::Vec3b>(i, j) = cv::Vec3b(255, 0, 0);
    //         }
    //     }
    // }
    // LOG(INFO) << dyno::to_string(concat_column_blocks.size());
    // //resize for viz
    // cv::resize(concat_column_blocks, concat_column_blocks, cv::Size(480, 480), 0, 0, cv::INTER_NEAREST);
    // cv::resize(J_img, J_img, cv::Size(480, 480), 0, 0, cv::INTER_NEAREST);
    // // // cv::resize(I_img, I_img, cv::Size(480, 480), 0, 0, cv::INTER_CUBIC);

    // cv::imshow("Jacobian", concat_column_blocks);
    // cv::imshow("J orig", J_img);
    // cv::imshow("J_1", J_1);
    cv::waitKey(0);

}


TEST(CodeConcepts, testModifyOptionalString) {

    auto modify = [](std::optional<std::reference_wrapper<std::string>> string) {
        if(string) {
           string->get() = "udpated";
        }
    };

    std::string input = "before";
    modify(input);

    EXPECT_EQ(input, "udpated");
}
```

## File: core_utils/test_csv.cc
```
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/utils/CsvParser.hpp"

#include <glog/logging.h>
#include <gtest/gtest.h>

using namespace dyno;

TEST(CsvHeader, testZeroConstruction) {
    CsvHeader header;
    EXPECT_EQ(header.size(), 0u);
}

TEST(CsvHeader, testConstruction) {


    CsvHeader header("1", "2", "3");
    EXPECT_EQ(header.size(), 3u);

    EXPECT_EQ(header.at(0), "1");
    EXPECT_EQ(header.at(1), "2");
    EXPECT_EQ(header.at(2), "3");

}

TEST(CsvHeader, testToString) {
    CsvHeader header("1", "2", "3");
    const std::string header_string = "1,2,3";
    EXPECT_EQ(header.toString(","), header_string);

}

TEST(CsvHeader, testToStringOneHeader) {
    CsvHeader header("1");
    const std::string header_string = "1";
    EXPECT_EQ(header.toString(), header_string);

}

TEST(CsvReader, testReadSimpleRowNoHeader) {
    CsvReader::Row row;

    EXPECT_EQ(row.size(), 0);

    std::stringstream ss;
    ss << "jesse, viorela,yiduo,  mik";

    row << ss;
    EXPECT_EQ(row.size(), 4);
    EXPECT_EQ(row[0], "jesse");
    EXPECT_EQ(row[1], "viorela");
    EXPECT_EQ(row[2], "yiduo");
    EXPECT_EQ(row[3], "mik");
}

TEST(CsvReader, testReadSimpleIterator) {

    std::stringstream ss;
    ss << "jesse, viorela,yiduo,  mik\n";
    ss << "nic, jack,will,ryan";

    CsvReader csv_reader(ss);
    auto it = csv_reader.begin();
    CsvReader::Row row = *it;
    EXPECT_EQ(row.size(), 4);
    EXPECT_EQ(row[0], "jesse");
    EXPECT_EQ(row[1], "viorela");
    EXPECT_EQ(row[2], "yiduo");
    EXPECT_EQ(row[3], "mik");

    it++;
    row = *it;
    EXPECT_EQ(row.size(), 4);
    EXPECT_EQ(row[0], "nic");
    EXPECT_EQ(row[1], "jack");
    EXPECT_EQ(row[2], "will");
    EXPECT_EQ(row[3], "ryan");

}


TEST(CsvWriter, testInvalidHeaderConstruction) {
    EXPECT_THROW({CsvWriter(CsvHeader{});}, InvalidCsvHeaderException);
}



TEST(CsvWriter, testBasicWrite) {
    CsvHeader header("frame_id", "timestamp");
    CsvWriter writer(header);

    writer << 0 << 1.1;
    std::stringstream ss;
    writer.write(ss);

    const std::string expected_write = "frame_id,timestamp\n0,1.1";
    EXPECT_EQ(ss.str(), expected_write);
}


TEST(CsvWriter, testWriteMultiLines) {
    CsvHeader header("frame_id", "timestamp");
    CsvWriter writer(header);

    writer << 0 << 1.1 << 1 << 1.3;
    std::stringstream ss;
    writer.write(ss);

    const std::string expected_write = "frame_id,timestamp\n0,1.1\n1,1.3";
    EXPECT_EQ(ss.str(), expected_write);
}
```

## File: core_utils/test_histogram.cc
```
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include <glog/logging.h>
#include <gtest/gtest.h>
#include <exception>
#include <limits>

#include "dynosam/utils/Histogram.hpp"

using namespace dyno;

TEST(Histogram, testConstructors) {

    Histogram hist(bh::make_histogram(bh::axis::regular<>(6, -1.0, 2.0)));
    Histogram hist1(bh::make_histogram(bh::axis::regular<>(6, -1.0, 2.0), bh::axis::variable<>({0, 1, 3, 5, 7})));
    Histogram hist2(bh::make_histogram(bh::axis::variable<>({0.0, 1.0, 3.0, 5.0, 7.0, std::numeric_limits<double>::max()})));

    Histogram hist3(bh::make_histogram(bh::axis::variable<>({0.0, 1.0, 3.0, 5.0, 7.0, std::numeric_limits<double>::max()})));

    EXPECT_EQ(hist.histogram_.rank(), 1);
    EXPECT_EQ(hist1.histogram_.rank(), 2);

    auto data = {-0.5, 1.1, 0.3, 1.7, 3.2, 4.0, 4.6, 4.0,7.0, 9.0, 8.1};
    std::for_each(data.begin(), data.end(), std::ref(hist2.histogram_));

    auto data1 = {1.2, 3.3, 4.0};
    std::for_each(data1.begin(), data1.end(), std::ref(hist3.histogram_));

    LOG(INFO) << hist3.toString();

    hist3.histogram_ += hist2.histogram_;
    LOG(INFO) << hist3.toString();
     LOG(INFO) << hist2.toString();


    using json = nlohmann::json;
    json hist_json;
    to_json(hist_json, hist2);
    LOG(INFO) << hist_json;

}
```

## File: core_utils/test_numerical.cc
```
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include <glog/logging.h>
#include <gtest/gtest.h>

#include "dynosam/utils/Numerical.hpp"

using namespace dyno;


TEST(Numerical, testChi_squared_quantile) {
    //from scipy.state.chi2
    //chi2.ppf(0.99, 6) = 16.811893829770927

    static constexpr auto r1 =  16.811893829770927;
    EXPECT_DOUBLE_EQ(chi_squared_quantile(6, 0.99), r1);

    // >>> chi2.ppf(0.5, 3)
    // 2.3659738843753377
    static constexpr auto r2 =  2.3659738843753377;
    EXPECT_DOUBLE_EQ(chi_squared_quantile(3, 0.5), r2);


}
```

## File: core_utils/test_structured_containers.cc
```
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "dynosam/common/StructuredContainers.hpp"
#include "dynosam/common/Types.hpp"


#include <glog/logging.h>
#include <gtest/gtest.h>
#include <exception>
#include <vector>
#include <iterator>

using namespace dyno;

using IntFilterIterator = internal::filter_iterator<std::vector<int>>;

/// @brief make definition for testing
template<>
struct std::iterator_traits<IntFilterIterator> : public dyno::internal::filter_iterator_detail<IntFilterIterator::pointer> {};


// //iterator is the pointer type... this is weird naming but is what the c++ standard does
// template<typename _Iterator>
// struct _filter_iterator_detail {
//     //! naming conventions to match those required by iterator traits
//     using value_type = typename std::iterator_traits<_Iterator>::value_type;
//     using reference = typename std::iterator_traits<_Iterator>::reference;
//     using pointer = typename std::iterator_traits<_Iterator>::pointer;
//     using difference_type = typename std::iterator_traits<_Iterator>::difference_type;
//     using iterator_category = std::forward_iterator_tag; //i guess? only forward is defined (++iter) right now
// };

// //in this case iter is the actual iterator (so _Container::iterator or _container::const_iterator)
// template<typename _Iter, typename _Container>
// struct _filter_iterator : public _filter_iterator_detail<typename _Iter::pointer> {

//     using BaseDetail = _filter_iterator_detail<typename _Iter::pointer>;
//     using iterator = _Iter;
//     using typename BaseDetail::value_type;
//     using typename BaseDetail::reference;
//     using typename BaseDetail::pointer;
//     using typename BaseDetail::difference_type;
//     using typename BaseDetail::iterator_category;

//     _filter_iterator(_Container& container,_Iter it) : container_(container), it_(it) {}

//     _Container& container_;
//     iterator it_;

//     reference operator*() { return *it_; }
//     reference operator->() { return *it_; }

//     bool operator==(const _filter_iterator& other) const {
//         return it_ == other.it_;
//     }
//     bool operator!=(const _filter_iterator& other) const { return it_ != other.it_; }

//     bool operator==(const iterator& other) const {
//         return it_ == other;
//     }
//     bool operator!=(const iterator& other) const { return it_ != other; }

//     _filter_iterator& operator++() {
//         // do {
//         //     ++it_;
//         // }
//         // while(is_invalid());
//         ++it_;
//         return *this;
//     }


//     //allows the iterator to be used as a enhanced for loop
//     _filter_iterator begin() { return _filter_iterator(container_, container_.begin()); }
//     _filter_iterator end() { return _filter_iterator(container_, container_.end()); }

//     const _filter_iterator begin() const { return _filter_iterator(container_, container_.begin()); }
//     const _filter_iterator end() const { return _filter_iterator(container_, container_.end()); }


// };

// TEST(FilterIterator, basic) {

//     using T = std::shared_ptr<int>;

//     std::vector<T> v;
//     _filter_iterator<std::vector<T>::iterator, std::vector<T>> fi(v, v.begin());

//     for(std::shared_ptr<int> i : fi) {}


//     const _filter_iterator<std::vector<T>::const_iterator, std::vector<T>> fi_c(v, v.cbegin());
//     for(std::shared_ptr<int> i : fi_c) {}
// }

TEST(FilterIterator, testNormalIteration) {

    std::vector<int> v = {1, 2, 3, 4, 5};
    internal::filter_iterator<std::vector<int>> v_iter(v, [](const int&) -> bool { return true; });

    EXPECT_EQ(*v_iter, 1);
    ++v_iter;
    EXPECT_EQ(*v_iter, 2);
    ++v_iter;
    EXPECT_EQ(*v_iter, 3);
    ++v_iter;
    EXPECT_EQ(*v_iter, 4);
    ++v_iter;
    EXPECT_EQ(*v_iter, 5);
    ++v_iter;
    EXPECT_EQ(v_iter, v.end());
}

TEST(FilterIterator, testConditionalIteratorWithValidStartingIndex) {

    //start with valid element at v(0)
    std::vector<int> v = {2, 3, 4, 5};
    //true on even numbers
    internal::filter_iterator<std::vector<int>> v_iter(v, [](const int& v) -> bool { return v % 2 == 0; });

    EXPECT_EQ(*v_iter, 2);
    ++v_iter;
    EXPECT_EQ(*v_iter, 4);
    ++v_iter;
    EXPECT_EQ(v_iter, v.end());
}

TEST(FilterIterator, testConditionalIteratorAsLoop) {

    //start with valid element at v(0)
    std::vector<int> v = {2, 3, 4, 5};
    //true on even numbers
    internal::filter_iterator<std::vector<int>> v_iter(v, [](const int& v) -> bool { return v % 2 == 0; });

    int index = 0;
    for(const int& i : v_iter) {
        if(index == 0) {
            EXPECT_EQ(i, 2);
        }
        else if(index == 1) {
            EXPECT_EQ(i, 4);
        }
        else {
            FAIL() << "Should not get here";
        }

        index++;

    }

    EXPECT_EQ(index, 2); //2 iterations only!!!
}


TEST(FilterIterator, testConditionalIteratorWithInvalidStartingIndex) {

    //start with valid element at v(0)
    std::vector<int> v = {1, 2, 3, 4, 5, 6};
    //true on even numbers
    internal::filter_iterator<std::vector<int>> v_iter(v, [](const int& v) -> bool { return v % 2 == 0; });

    EXPECT_EQ(*v_iter, 2);
    ++v_iter;
    EXPECT_EQ(*v_iter, 4);
    ++v_iter;
    EXPECT_EQ(*v_iter, 6);
    EXPECT_NE(v_iter, v.end());
    ++v_iter;
    EXPECT_EQ(v_iter, v.end());
}


TEST(FilterIterator, testStdDistance) {

    {
        std::vector<int> v = {1, 2, 3, 4, 5, 6};
        //true on even numbers
        internal::filter_iterator<std::vector<int>> v_iter(v, [](const int& v) -> bool { return v % 2 == 0; });
        EXPECT_EQ(std::distance(v_iter.begin(), v_iter.end()), 3);
    }

    {
        //start with valid element at v(0)
        std::vector<int> v = {1, 2, 3};
        //true on even numbers
        internal::filter_iterator<std::vector<int>> v_iter(v, [](const int& v) -> bool { return v % 2 == 0; });
        EXPECT_EQ(std::distance(v_iter.begin(), v_iter.end()), 1);
    }


     {
        //start with valid element at v(0)
        std::vector<int> v = {2, 2, 2};
        //true on even numbers
        internal::filter_iterator<std::vector<int>> v_iter(v, [](const int& v) -> bool { return v % 2 == 0; });
        EXPECT_EQ(std::distance(v_iter.begin(), v_iter.end()), 3);
    }
}
```

## File: core_utils/test_types.cc
```
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include <dynosam/utils/JsonUtils.hpp>
#include <filesystem>
#include <nlohmann/json.hpp>  //for gt packet seralize tests

#include "dynosam/common/Exceptions.hpp"
#include "dynosam/common/GroundTruthPacket.hpp"
#include "dynosam/frontend/FrontendInputPacket.hpp"
#include "dynosam/frontend/vision/Feature.hpp"
#include "dynosam/logger/Logger.hpp"
#include "dynosam/utils/Statistics.hpp"
#include "dynosam/utils/Variant.hpp"
#include "internal/helpers.hpp"

using namespace dyno;

#include <glog/logging.h>
#include <gmock/gmock.h>
#include <gtest/gtest.h>

// //custom type with dyno::to_string defined. Must be inside dyno namespace
// namespace dyno {
//     struct CustomToString {};
// }

// template<>
// std::string dyno::to_string(const CustomToString&) {
//     return "custom_to_string";
// }

// TEST(IOTraits, testToString) {

//     EXPECT_EQ(traits<decltype(4)>::ToString(4), "4");
//     EXPECT_EQ(traits<CustomToString>::ToString(CustomToString{}),
//     "custom_to_string");
// }

// TEST(Exceptions, testExceptionStream) {
//     EXPECT_THROW({ExceptionStream::Create<DynosamException>();},
//     std::runtime_error); EXPECT_NO_THROW({ExceptionStream::Create();});
// }

// TEST(Exceptions, testExceptionStreamMessage) {
//     //would be preferable to use gmock like
//     //Throws<std::runtime_error>(Property(&std::runtime_error::what,
//     //      HasSubstr("message"))));
//     //but currently issues getting the gmock library to be found...
//     // try {
//     //     ExceptionStream::Create<std::runtime_error>() << "A message";
//     // }
//     // catch(const std::runtime_error& expected) {
//     //     EXPECT_EQ(std::string(expected.what()), "A message");
//     // }
//     // catch(...) {
//     //     FAIL() << "An excpetion was thrown but it was not
//     std::runtime_error";
//     // }
//     // FAIL() << "Exception should be thrown but was not";
//     ExceptionStream::Create<std::runtime_error>() << "A message";
// }

// // TEST(Exceptions, testBasicThrow) {
// //     checkAndThrow(false);
// //     // EXPECT_THROW({checkAndThrow(false);}, DynosamException);
// //     // EXPECT_NO_THROW({checkAndThrow(true);});
// // }

TEST(GtsamUtils, isGtsamValueType) {
  EXPECT_TRUE(is_gtsam_value_v<gtsam::Pose3>);
  EXPECT_TRUE(is_gtsam_value_v<gtsam::Point3>);
  // EXPECT_FALSE(is_gtsam_value_v<double>);
  EXPECT_FALSE(is_gtsam_value_v<ImageType::RGBMono>);
}

TEST(VariantTypes, isVariant) {
  using Var = std::variant<int, std::string>;
  EXPECT_TRUE(is_variant_v<Var>);
  EXPECT_FALSE(is_variant_v<int>);
}

TEST(VariantTypes, variantContains) {
  using Var = std::variant<int, std::string>;
  // for some reason EXPECT_TRUE doenst work?
  //  EXPECT_TRUE(isvariantmember_v<int, Var>);
  bool r = is_variant_member_v<int, Var>;
  EXPECT_EQ(r, true);

  r = is_variant_member_v<std::string, Var>;
  EXPECT_EQ(r, true);

  r = is_variant_member_v<double, Var>;
  EXPECT_EQ(r, false);
}

TEST(ImageType, testRGBMonoValidation) {
  {
    // invalid type
    cv::Mat input(cv::Size(50, 50), CV_64F);
    EXPECT_THROW({ ImageType::RGBMono::validate(input); },
                 InvalidImageTypeException);
  }

  {
    // okay type
    cv::Mat input(cv::Size(50, 50), CV_8UC1);
    EXPECT_NO_THROW({ ImageType::RGBMono::validate(input); });
  }

  {
    // okay type
    cv::Mat input(cv::Size(50, 50), CV_8UC3);
    EXPECT_NO_THROW({ ImageType::RGBMono::validate(input); });
  }
}

TEST(ImageType, testDepthValidation) {
  {
    // okay type
    cv::Mat input(cv::Size(50, 50), CV_64F);
    EXPECT_NO_THROW({ ImageType::Depth::validate(input); });
  }

  {
    // invalud type
    cv::Mat input(cv::Size(50, 50), CV_8UC3);
    EXPECT_THROW({ ImageType::Depth::validate(input); },
                 InvalidImageTypeException);
  }
}

TEST(ImageType, testOpticalFlowValidation) {
  {
    // okay type
    cv::Mat input(cv::Size(50, 50), CV_32FC2);
    EXPECT_NO_THROW({ ImageType::OpticalFlow::validate(input); });
  }

  {
    // invalud type
    cv::Mat input(cv::Size(50, 50), CV_8UC3);
    EXPECT_THROW({ ImageType::OpticalFlow::validate(input); },
                 InvalidImageTypeException);
  }
}

TEST(ImageType, testSemanticMaskValidation) {
  // TODO:
}

TEST(ImageType, testMotionMaskValidation) {
  // TODO:
}

TEST(ImageContainerSubset, testBasicSubsetContainer) {
  cv::Mat depth(cv::Size(25, 25), CV_64F);
  uchar* depth_ptr = depth.data;

  cv::Mat optical_flow(cv::Size(25, 25), CV_32FC2);
  uchar* optical_flow_ptr = optical_flow.data;
  ImageContainerSubset<ImageType::Depth, ImageType::OpticalFlow> ics{
      ImageWrapper<ImageType::Depth>(depth),
      ImageWrapper<ImageType::OpticalFlow>(optical_flow)};

  cv::Mat retrieved_depth = ics.get<ImageType::Depth>();
  cv::Mat retrieved_of = ics.get<ImageType::OpticalFlow>();

  EXPECT_EQ(retrieved_depth.data, depth_ptr);
  EXPECT_EQ(retrieved_of.data, optical_flow_ptr);

  EXPECT_EQ(depth.size(), retrieved_depth.size());
  EXPECT_EQ(optical_flow.size(), retrieved_of.size());
}

TEST(ImageContainerSubset, testSubsetContainerExists) {
  cv::Mat depth(cv::Size(25, 25), CV_64F);
  cv::Mat optical_flow(cv::Size(25, 25), CV_32FC2);
  ImageContainerSubset<ImageType::Depth, ImageType::OpticalFlow,
                       ImageType::RGBMono>
      ics{ImageWrapper<ImageType::Depth>(depth),
          ImageWrapper<ImageType::OpticalFlow>(optical_flow),
          ImageWrapper<ImageType::RGBMono>()};

  EXPECT_TRUE(ics.exists<ImageType::Depth>());
  EXPECT_TRUE(ics.exists<ImageType::OpticalFlow>());

  EXPECT_FALSE(ics.exists<ImageType::RGBMono>());
}

TEST(ImageContainerSubset, testBasicMakeSubset) {
  cv::Mat depth(cv::Size(50, 50), CV_64F);
  cv::Mat optical_flow(cv::Size(50, 50), CV_32FC2);
  uchar* optical_flow_ptr = optical_flow.data;
  ImageContainerSubset<ImageType::Depth, ImageType::OpticalFlow> ics{
      ImageWrapper<ImageType::Depth>(depth),
      ImageWrapper<ImageType::OpticalFlow>(optical_flow)};

  EXPECT_EQ(ics.index<ImageType::OpticalFlow>(), 1u);

  ImageContainerSubset<ImageType::OpticalFlow> subset =
      ics.makeSubset<ImageType::OpticalFlow>();
  EXPECT_EQ(subset.index<ImageType::OpticalFlow>(), 0u);

  cv::Mat retrieved_of = ics.get<ImageType::OpticalFlow>();
  cv::Mat subset_retrieved_of = subset.get<ImageType::OpticalFlow>();

  EXPECT_EQ(retrieved_of.data, optical_flow_ptr);
  EXPECT_EQ(subset_retrieved_of.data, optical_flow_ptr);
}

TEST(ImageContainerSubset, testSafeGet) {
  cv::Mat depth(cv::Size(25, 25), CV_64F);
  cv::Mat optical_flow(cv::Size(25, 25), CV_32FC2);
  ImageContainerSubset<ImageType::Depth, ImageType::OpticalFlow,
                       ImageType::RGBMono>
      ics{ImageWrapper<ImageType::Depth>(depth),
          ImageWrapper<ImageType::OpticalFlow>(optical_flow),
          ImageWrapper<ImageType::RGBMono>()};

  cv::Mat tmp;
  EXPECT_FALSE(ics.safeGet<ImageType::RGBMono>(tmp));

  // tmp shoudl now be the same as the depth ptr
  EXPECT_TRUE(ics.safeGet<ImageType::Depth>(tmp));
  EXPECT_EQ(depth.data, tmp.data);
  EXPECT_EQ(depth.size(), tmp.size());

  // tmp shoudl now be the same as the optical flow ptr
  EXPECT_TRUE(ics.safeGet<ImageType::OpticalFlow>(tmp));
  EXPECT_EQ(optical_flow.data, tmp.data);
  EXPECT_EQ(optical_flow.size(), tmp.size());
}

TEST(ImageContainerSubset, testSafeClone) {
  cv::Mat depth(cv::Size(50, 50), CV_64F);
  cv::Mat optical_flow(cv::Size(50, 50), CV_32FC2);
  ImageContainerSubset<ImageType::Depth, ImageType::OpticalFlow,
                       ImageType::RGBMono>
      ics{ImageWrapper<ImageType::Depth>(depth),
          ImageWrapper<ImageType::OpticalFlow>(optical_flow),
          ImageWrapper<ImageType::RGBMono>()};

  cv::Mat tmp;
  EXPECT_FALSE(ics.cloneImage<ImageType::RGBMono>(tmp));
  EXPECT_TRUE(tmp.empty());

  EXPECT_TRUE(ics.cloneImage<ImageType::Depth>(tmp));
  EXPECT_NE(depth.data, tmp.data);  // no longer the same data
  EXPECT_EQ(depth.size(), tmp.size());

  EXPECT_TRUE(ics.cloneImage<ImageType::OpticalFlow>(tmp));
  EXPECT_NE(optical_flow.data, tmp.data);  // no longer the same data
  EXPECT_EQ(optical_flow.size(), tmp.size());
}

TEST(ImageContainer, testImageContainerIndexing) {
  EXPECT_EQ(ImageContainer::Index<ImageType::RGBMono>(), 0u);
  EXPECT_EQ(ImageContainer::Index<ImageType::Depth>(), 1u);
  EXPECT_EQ(ImageContainer::Index<ImageType::OpticalFlow>(), 2u);
  EXPECT_EQ(ImageContainer::Index<ImageType::SemanticMask>(), 3u);
  EXPECT_EQ(ImageContainer::Index<ImageType::MotionMask>(), 4u);
}

TEST(ImageContainer, CreateRGBDSemantic) {
  cv::Mat rgb(cv::Size(50, 50), CV_8UC1);
  cv::Mat depth(cv::Size(50, 50), CV_64F);
  cv::Mat optical_flow(cv::Size(50, 50), CV_32FC2);
  cv::Mat semantic_mask(cv::Size(50, 50), CV_32SC1);
  ImageContainer::Ptr rgbd_semantic = ImageContainer::Create(
      0u, 0u, ImageWrapper<ImageType::RGBMono>(rgb),
      ImageWrapper<ImageType::Depth>(depth),
      ImageWrapper<ImageType::OpticalFlow>(optical_flow),
      ImageWrapper<ImageType::SemanticMask>(semantic_mask));

  EXPECT_TRUE(rgbd_semantic->hasSemanticMask());
  EXPECT_TRUE(rgbd_semantic->hasDepth());
  EXPECT_FALSE(rgbd_semantic->hasMotionMask());
  EXPECT_FALSE(rgbd_semantic->isMonocular());
}

TEST(ImageContainer, CreateRGBDSemanticWithInvalidSizes) {
  cv::Mat rgb(cv::Size(25, 25), CV_8UC1);
  cv::Mat depth(cv::Size(50, 50), CV_64F);
  cv::Mat optical_flow(cv::Size(50, 50), CV_32FC2);
  cv::Mat semantic_mask(cv::Size(50, 50), CV_32SC1);
  EXPECT_THROW(
      {
        ImageContainer::Create(
            0u, 0u, ImageWrapper<ImageType::RGBMono>(rgb),
            ImageWrapper<ImageType::Depth>(depth),
            ImageWrapper<ImageType::OpticalFlow>(optical_flow),
            ImageWrapper<ImageType::SemanticMask>(semantic_mask));
      },
      ImageContainerConstructionException);
}

TEST(FeatureContainer, basicAdd) {
  FeatureContainer fc;
  EXPECT_EQ(fc.size(), 0u);
  EXPECT_FALSE(fc.exists(1));

  Feature f;
  f.trackletId(1);

  fc.add(f);
  EXPECT_EQ(fc.size(), 1u);
  EXPECT_TRUE(fc.exists(1));

  // this implicitly tests map access
  auto fr = fc.getByTrackletId(1);
  EXPECT_TRUE(fr != nullptr);
  EXPECT_EQ(*fr, f);
}

TEST(FeatureContainer, basicRemove) {
  FeatureContainer fc;
  EXPECT_EQ(fc.size(), 0u);

  Feature f;
  f.trackletId(1);

  fc.add(f);
  EXPECT_EQ(fc.size(), 1u);

  fc.remove(1);
  EXPECT_FALSE(fc.exists(1));

  auto fr = fc.getByTrackletId(1);
  EXPECT_TRUE(fr == nullptr);
}

TEST(FeatureContainer, testVectorLikeIteration) {
  FeatureContainer fc;

  for (size_t i = 0; i < 10u; i++) {
    Feature f;
    f.trackletId(i);
    fc.add(f);
  }

  int count = 0;
  for (const auto& i : fc) {
    EXPECT_TRUE(i != nullptr);
    count++;
  }

  EXPECT_EQ(count, 10);
  count = 0;

  fc.remove(0);
  fc.remove(1);

  for (const auto& i : fc) {
    EXPECT_TRUE(i != nullptr);
    EXPECT_TRUE(i->trackletId() != 0 || i->trackletId() != 1);
    count++;
  }

  EXPECT_EQ(count, 8);
}

TEST(FeatureContainer, testusableIterator) {
  FeatureContainer fc;

  for (size_t i = 0; i < 10u; i++) {
    Feature f;
    f.trackletId(i);
    fc.add(f);
    EXPECT_TRUE(f.usable());
  }

  {
    auto usable_iterator = fc.beginUsable();
    EXPECT_EQ(std::distance(usable_iterator.begin(), usable_iterator.end()),
              10);
  }

  fc.markOutliers({3});
  fc.markOutliers({4});

  {
    auto usable_iterator = fc.beginUsable();
    EXPECT_EQ(std::distance(usable_iterator.begin(), usable_iterator.end()), 8);

    for (const auto& i : usable_iterator) {
      EXPECT_TRUE(i->trackletId() != 3 || i->trackletId() != 4);
    }
  }
}

TEST(Feature, checkInvalidState) {
  Feature f;
  EXPECT_TRUE(f.inlier());
  EXPECT_FALSE(f.usable());  // inlier initally but invalid tracking label

  f.trackletId(10);
  EXPECT_TRUE(f.usable());

  f.markInvalid();
  EXPECT_FALSE(f.usable());

  f.trackletId(10u);
  EXPECT_TRUE(f.usable());

  f.markOutlier();
  EXPECT_FALSE(f.usable());
}

TEST(Feature, checkDepth) {
  Feature f;
  EXPECT_FALSE(f.hasDepth());

  f.depth(12.0);
  EXPECT_TRUE(f.hasDepth());
}

TEST(GroundTruthInputPacket, findAssociatedObjectWithIdx) {
  ObjectPoseGT obj01;
  obj01.frame_id_ = 0;
  obj01.object_id_ = 1;

  ObjectPoseGT obj02;
  obj02.frame_id_ = 0;
  obj02.object_id_ = 2;

  ObjectPoseGT obj03;
  obj03.frame_id_ = 0;
  obj03.object_id_ = 3;

  ObjectPoseGT obj11;
  obj11.frame_id_ = 1;
  obj11.object_id_ = 1;

  ObjectPoseGT obj12;
  obj12.frame_id_ = 1;
  obj12.object_id_ = 2;

  GroundTruthInputPacket packet_0;
  packet_0.frame_id_ = 0;
  packet_0.object_poses_.push_back(obj01);
  packet_0.object_poses_.push_back(obj02);
  packet_0.object_poses_.push_back(obj03);

  GroundTruthInputPacket packet_1;
  packet_1.frame_id_ = 1;
  // put in out of order compared to packet_1
  packet_1.object_poses_.push_back(obj12);
  packet_1.object_poses_.push_back(obj11);

  size_t obj_idx, obj_other_idx;
  EXPECT_TRUE(
      packet_0.findAssociatedObject(1, packet_1, obj_idx, obj_other_idx));

  EXPECT_EQ(obj_idx, 0);
  EXPECT_EQ(obj_other_idx, 1);

  EXPECT_TRUE(
      packet_0.findAssociatedObject(2, packet_1, obj_idx, obj_other_idx));

  EXPECT_EQ(obj_idx, 1);
  EXPECT_EQ(obj_other_idx, 0);

  // object 3 is not in packet_1
  EXPECT_FALSE(
      packet_0.findAssociatedObject(3, packet_1, obj_idx, obj_other_idx));
}

TEST(GroundTruthInputPacket, findAssociatedObjectWithPtr) {
  ObjectPoseGT obj01;
  obj01.frame_id_ = 0;
  obj01.object_id_ = 1;

  ObjectPoseGT obj02;
  obj02.frame_id_ = 0;
  obj02.object_id_ = 2;

  ObjectPoseGT obj03;
  obj03.frame_id_ = 0;
  obj03.object_id_ = 3;

  ObjectPoseGT obj11;
  obj11.frame_id_ = 1;
  obj11.object_id_ = 1;

  ObjectPoseGT obj12;
  obj12.frame_id_ = 1;
  obj12.object_id_ = 2;

  GroundTruthInputPacket packet_0;
  packet_0.frame_id_ = 0;
  packet_0.object_poses_.push_back(obj01);
  packet_0.object_poses_.push_back(obj02);
  packet_0.object_poses_.push_back(obj03);

  GroundTruthInputPacket packet_1;
  packet_1.frame_id_ = 1;
  // put in out of order compared to packet_1
  packet_1.object_poses_.push_back(obj12);
  packet_1.object_poses_.push_back(obj11);

  ObjectPoseGT* obj;
  const ObjectPoseGT* obj_other;
  EXPECT_TRUE(packet_0.findAssociatedObject(2, packet_1, &obj, &obj_other));

  EXPECT_TRUE(obj != nullptr);
  EXPECT_TRUE(obj_other != nullptr);

  EXPECT_EQ(obj->object_id_, 2);
  EXPECT_EQ(obj_other->object_id_, 2);

  EXPECT_EQ(obj->frame_id_, 0);
  EXPECT_EQ(obj_other->frame_id_, 1);
}

TEST(SensorTypes, MeasurementWithCovarianceConstructionEmpty) {
  MeasurementWithCovariance<Landmark> measurement;
  EXPECT_FALSE(measurement.hasModel());
}

TEST(SensorTypes, MeasurementWithCovarianceConstructionMeasurement) {
  Landmark lmk(10, 12.4, 0.001);
  MeasurementWithCovariance<Landmark> measurement(lmk);
  EXPECT_FALSE(measurement.hasModel());
  EXPECT_EQ(measurement.measurement(), lmk);
}

TEST(SensorTypes, MeasurementWithCovarianceConstructionMeasurementAndSigmas) {
  Landmark lmk(10, 12.4, 0.001);
  gtsam::Vector3 sigmas;
  sigmas << 0.1, 0.2, 0.3;
  MeasurementWithCovariance<Landmark> measurement(lmk, sigmas);
  EXPECT_TRUE(measurement.hasModel());
  EXPECT_EQ(measurement.measurement(), lmk);

  MeasurementWithCovariance<Landmark>::Covariance cov =
      measurement.covariance();

  MeasurementWithCovariance<Landmark>::Covariance expected_cov =
      sigmas.array().pow(2).matrix().asDiagonal();
  EXPECT_TRUE(gtsam::assert_equal(expected_cov, cov));
}

TEST(SensorTypes, MeasurementWithCovarianceConstructionMeasurementAndCov) {
  Landmark lmk(10, 12.4, 0.001);
  MeasurementWithCovariance<Landmark>::Covariance expected_cov;
  expected_cov << 0.1, 0, 0, 0, 0.2, 0, 0, 0, 0.4;
  MeasurementWithCovariance<Landmark> measurement(lmk, expected_cov);
  EXPECT_TRUE(measurement.hasModel());
  EXPECT_EQ(measurement.measurement(), lmk);

  MeasurementWithCovariance<Landmark>::Covariance cov =
      measurement.covariance();
  EXPECT_TRUE(gtsam::assert_equal(expected_cov, cov));
}

TEST(JsonIO, ReferenceFrameValue) {
  ReferenceFrameValue<gtsam::Pose3> ref_frame(gtsam::Pose3::Identity(),
                                              ReferenceFrame::GLOBAL);

  using json = nlohmann::json;
  json j = ref_frame;

  auto ref_frame_load = j.template get<ReferenceFrameValue<gtsam::Pose3>>();
  // TODO: needs equals operator
  //  EXPECT_EQ(kp_load, kp);
}

TEST(JsonIO, ObjectPoseGTIO) {
  ObjectPoseGT object_pose_gt;

  object_pose_gt.frame_id_ = 0;
  object_pose_gt.object_id_ = 1;
  object_pose_gt.L_camera_ = gtsam::Pose3::Identity();
  object_pose_gt.L_world_ = gtsam::Pose3::Identity();
  object_pose_gt.prev_H_current_L_ = gtsam::Pose3::Identity();

  using json = nlohmann::json;
  json j = object_pose_gt;

  auto object_pose_gt_2 = j.template get<ObjectPoseGT>();
  EXPECT_EQ(object_pose_gt, object_pose_gt_2);
}

TEST(JsonIO, MeasurementWithCovSigmas) {
  using json = nlohmann::json;
  Landmark lmk(10, 12.4, 0.001);
  MeasurementWithCovariance<Landmark>::Covariance expected_cov;
  expected_cov << 0.1, 0, 0, 0, 0.2, 0, 0, 0, 0.4;
  MeasurementWithCovariance<Landmark> measurement(lmk, expected_cov);
  json j = measurement;
  auto measurements_load =
      j.template get<MeasurementWithCovariance<Landmark>>();
  EXPECT_TRUE(gtsam::assert_equal(measurements_load, measurement));
}

TEST(JsonIO, MeasurementWithCov) {
  using json = nlohmann::json;
  Landmark lmk(10, 12.4, 0.001);
  gtsam::Vector3 sigmas;
  sigmas << 0.1, 0.2, 0.3;
  MeasurementWithCovariance<Landmark> measurement(lmk, sigmas);

  json j = measurement;
  auto measurements_load =
      j.template get<MeasurementWithCovariance<Landmark>>();
  EXPECT_TRUE(gtsam::assert_equal(measurements_load, measurement));
}

TEST(JsonIO, MeasurementWithNoCov) {
  using json = nlohmann::json;
  Landmark lmk(10, 12.4, 0.001);
  MeasurementWithCovariance<Landmark> measurement(lmk);

  json j = measurement;
  auto measurements_load =
      j.template get<MeasurementWithCovariance<Landmark>>();
  EXPECT_TRUE(gtsam::assert_equal(measurements_load, measurement));
}

TEST(JsonIO, TrackedValueStatusKp) {
  KeypointStatus kp =
      dyno_testing::makeStatusKeypointMeasurement(4, 3, 1, Keypoint(0, 1));

  using json = nlohmann::json;
  json j = (KeypointStatus)kp;

  auto kp_load = j.template get<KeypointStatus>();
  // TODO: needs equals operator
  EXPECT_EQ(kp_load, kp);
}

TEST(JsonIO, TrackedValueStatusKps) {
  StatusKeypointVector measurements;
  for (size_t i = 0; i < 10; i++) {
    measurements.push_back(
        dyno_testing::makeStatusKeypointMeasurement(i, background_label, 0));
  }
  using json = nlohmann::json;
  json j = measurements;

  auto measurements_load = j.template get<StatusKeypointVector>();
  EXPECT_EQ(measurements, measurements);
}

TEST(JsonIO, RGBDInstanceOutputPacket) {
  auto scenario = dyno_testing::makeDefaultScenario();

  std::map<FrameId, RGBDInstanceOutputPacket> rgbd_output;

  for (size_t i = 0; i < 10; i++) {
    auto output = scenario.getOutput(i);
    rgbd_output.insert({i, *output});
  }

  using json = nlohmann::json;
  json j = rgbd_output;
  std::map<FrameId, RGBDInstanceOutputPacket> rgbd_output_loaded =
      j.template get<std::map<FrameId, RGBDInstanceOutputPacket>>();
  EXPECT_EQ(rgbd_output_loaded, rgbd_output);
}

TEST(JsonIO, GroundTruthInputPacketIO) {
  GroundTruthInputPacket gt_packet;

  using json = nlohmann::json;
  json j = gt_packet;

  auto gt_packet_2 = j.template get<GroundTruthInputPacket>();
}

TEST(JsonIO, GroundTruthPacketMapIO) {
  ObjectPoseGT obj01;
  obj01.frame_id_ = 0;
  obj01.object_id_ = 1;

  ObjectPoseGT obj02;
  obj02.frame_id_ = 0;
  obj02.object_id_ = 2;

  ObjectPoseGT obj03;
  obj03.frame_id_ = 0;
  obj03.object_id_ = 3;

  ObjectPoseGT obj11;
  obj11.frame_id_ = 1;
  obj11.object_id_ = 1;

  ObjectPoseGT obj12;
  obj12.frame_id_ = 1;
  obj12.object_id_ = 2;

  GroundTruthInputPacket packet_0;
  packet_0.frame_id_ = 0;
  packet_0.object_poses_.push_back(obj01);
  packet_0.object_poses_.push_back(obj02);
  packet_0.object_poses_.push_back(obj03);

  GroundTruthInputPacket packet_1;
  packet_1.frame_id_ = 1;
  // put in out of order compared to packet_1
  packet_1.object_poses_.push_back(obj12);
  packet_1.object_poses_.push_back(obj11);

  GroundTruthPacketMap gt_packet_map;
  gt_packet_map.insert2(0, packet_0);
  gt_packet_map.insert2(1, packet_1);

  using json = nlohmann::json;
  json j = gt_packet_map;

  auto gt_packet_map_2 = j.template get<GroundTruthPacketMap>();
  EXPECT_EQ(gt_packet_map, gt_packet_map_2);
}

TEST(JsonIO, eigenJsonIO) {
  Eigen::Matrix4d m;
  m << 1.0, 2.0, 3.0, 4.0, 11.0, 12.0, 13.0, 14.0, 21.0, 22.0, 23.0, 24.0, 31.0,
      32.0, 33.0, 34.0;
  nlohmann::json j = m;
  // std::cerr << j.dump() << std::endl;
  Eigen::Matrix4d m2 = j.get<Eigen::Matrix4d>();

  EXPECT_TRUE(gtsam::assert_equal(m, m2));
}

namespace fs = std::filesystem;
class JsonIOWithFiles : public ::testing::Test {
 public:
  JsonIOWithFiles() {}

 protected:
  virtual void SetUp() { fs::create_directory(sandbox); }
  virtual void TearDown() { fs::remove_all(sandbox); }

  const fs::path sandbox{"/tmp/sandbox_json"};
};

TEST_F(JsonIOWithFiles, testSimpleBison) {
  StatusKeypointVector measurements;
  for (size_t i = 0; i < 10; i++) {
    measurements.push_back(
        dyno_testing::makeStatusKeypointMeasurement(i, background_label, 0));
  }

  fs::path tmp_bison_path = sandbox / "simple_bison.bson";
  std::string tmp_bison_path_str = tmp_bison_path;

  JsonConverter::WriteOutJson(measurements, tmp_bison_path_str,
                              JsonConverter::Format::BSON);

  StatusKeypointVector measurements_read;
  EXPECT_TRUE(JsonConverter::ReadInJson(measurements_read, tmp_bison_path_str,
                                        JsonConverter::Format::BSON));
  EXPECT_EQ(measurements_read, measurements);
}

TEST(TrackedValueStatus, testIsTimeInvariant) {
  TrackedValueStatus<Keypoint> status_time_invariant(
      MeasurementWithCovariance<Keypoint>{Keypoint()},
      TrackedValueStatus<Keypoint>::MeaninglessFrame, 0, 0,
      ReferenceFrame::GLOBAL);

  EXPECT_TRUE(status_time_invariant.isTimeInvariant());

  TrackedValueStatus<Keypoint> status_time_variant(
      MeasurementWithCovariance<Keypoint>{Keypoint()},
      0,  // use zero
      0, 0, ReferenceFrame::GLOBAL);
  EXPECT_FALSE(status_time_variant.isTimeInvariant());
}

TEST(Statistics, testGetModules) {
  utils::StatsCollector("global_stats").IncrementOne();
  utils::StatsCollector("ns.spin").IncrementOne();
  utils::StatsCollector("ns.spin1").IncrementOne();

  EXPECT_EQ(utils::Statistics::getTagByModule(),
            std::vector<std::string>({"global_stats"}));
  EXPECT_EQ(utils::Statistics::getTagByModule("ns"),
            std::vector<std::string>({"ns.spin", "ns.spin1"}));
}
```

## File: data/sensor.yaml
```yaml
# %YAML:1.0
# General sensor definitions.
camera_id: camera

# Sensor extrinsics wrt. the body-frame.
T_BS:
  [0.0148655429818, -0.999880929698, 0.00414029679422, -0.0216401454975,
         0.999557249008, 0.0149672133247, 0.025715529948, -0.064676986768,
        -0.0257744366974, 0.00375618835797, 0.999660727178, 0.00981073058949,
         0.0, 0.0, 0.0, 1.0]

# Camera specific definitions.
rate_hz: 20
resolution:
  width: 752
  height: 480
camera_model: pinhole
intrinsics: [458.654, 457.296, 367.215, 248.375] #fu, fv, cu, cv
distortion_model: radial-tangential
distortion_coefficients: [-0.28340811, 0.07395907, 0.00019359, 1.76187114e-05]
```

## File: data/tracking_params.yaml
```yaml
orb_params:
  scale_factor: 12

use_clahe_filter: false
```

## File: dataproviders/ZEDDataProvider/test_ZED_data_provider_conversion_and_mask.cc
```
#include <gtest/gtest.h>
#include <gmock/gmock.h>
#include "dynosam/dataprovider/ZEDDataProvider.hpp"
#include "dynosam/common/ZEDCamera.hpp" // For common::ZEDCamera
#include "dynosam/common/ImageTypes.hpp"
#include "dynosam/utils/OpenCVUtils.hpp" // For cvTypeToString

// Mock for common::ZEDCamera
class MockZEDCamera : public dyno::common::ZEDCamera {
public:
    // Explicitly inherit constructors from the base class
    using dyno::common::ZEDCamera::ZEDCamera;


    MOCK_METHOD(bool, open, (), (override));
    MOCK_METHOD(void, close, (), (override));
    MOCK_METHOD(void, shutdown, (), (override));
    MOCK_CONST_METHOD(bool, isOpened, (), (override));
    MOCK_METHOD(sl::ERROR_CODE, grab, (sl::RuntimeParameters&), (override));
    MOCK_METHOD(sl::ERROR_CODE, retrieveRawImage, (sl::VIEW, sl::Mat&, sl::MEM, const sl::Resolution&), (override));
    MOCK_METHOD(sl::ERROR_CODE, retrieveRawDepth, (sl::Mat&, sl::MEM, const sl::Resolution&), (override));
    MOCK_METHOD(sl::ERROR_CODE, retrieveRawMeasure, (sl::MEASURE, sl::Mat&, sl::MEM, const sl::Resolution&), (override));
    MOCK_METHOD(sl::ERROR_CODE, retrieveRawSensorsData, (sl::SensorsData&, sl::TIME_REFERENCE), (override));
    MOCK_METHOD(bool, enableObjectDetection, (), (override));
    MOCK_METHOD(void, disableObjectDetection, (), (override));
    MOCK_CONST_METHOD(bool, isObjectDetectionEnabled, (), (override));
    MOCK_METHOD(sl::ERROR_CODE, retrieveObjects, (sl::Objects&, const sl::ObjectDetectionRuntimeParameters&), (override));
    MOCK_METHOD(bool, enableBodyTracking, (), (override));
    MOCK_METHOD(void, disableBodyTracking, (), (override));
    MOCK_CONST_METHOD(bool, isBodyTrackingEnabled, (), (override));
    MOCK_METHOD(sl::ERROR_CODE, retrieveBodies, (sl::Bodies&, const sl::BodyTrackingRuntimeParameters&), (override));
    MOCK_CONST_METHOD(sl::CameraInformation, getCameraInformation, (), (override));
    MOCK_CONST_METHOD(sl::CalibrationParameters, getCalibrationParameters, (bool), (override));
    MOCK_CONST_METHOD(bool, isSVOMode, (), (override));
    MOCK_METHOD(int, getSVONumberOfFrames, (), (override));
    MOCK_CONST_METHOD(unsigned int, getSVOFrameRate, (), (override));
    MOCK_METHOD(sl::ERROR_CODE, setSVOPosition, (int), (override));
    MOCK_CONST_METHOD(bool, isImuEnabled, (), (override)); // Added for completeness
};

// Test Fixture for ZEDDataProvider Data Conversion and Mask Generation Tests
class ZEDDataProviderConversionTest : public ::testing::Test {
protected:
    std::shared_ptr<MockZEDCamera> mock_zed_camera_;
    std::unique_ptr<dyno::ZEDDataProvider> data_provider_;
    dyno::ZEDConfig test_config_;
    sl::CameraInformation mock_cam_info_;

    void SetUp() override {
        // Default config values can be set here or overridden in tests
        test_config_.resolution = sl::RESOLUTION::HD720;
        test_config_.fps = 30;
        test_config_.depth_mode = sl::DEPTH_MODE::PERFORMANCE;
        test_config_.coordinate_units = sl::UNIT::METER;
        test_config_.coordinate_system_3d_zed = sl::COORDINATE_SYSTEM::IMAGE; // Common for image processing
        test_config_.enable_imu = false;
        test_config_.output_rgb = true; // Default to outputting RGB

        // Mock CameraInformation needed for camera_params_ setup
        mock_cam_info_.camera_configuration.resolution.width = 1280;
        mock_cam_info_.camera_configuration.resolution.height = 720;
        mock_cam_info_.camera_configuration.fps = 30;
        mock_cam_info_.camera_configuration.calibration_parameters.left_cam.fx = 500.0f;
        mock_cam_info_.camera_configuration.calibration_parameters.left_cam.fy = 500.0f;
        mock_cam_info_.camera_configuration.calibration_parameters.left_cam.cx = 640.0f;
        mock_cam_info_.camera_configuration.calibration_parameters.left_cam.cy = 360.0f;
        // Assuming no distortion for ZED rectified images
        for(int i=0; i<5; ++i) mock_cam_info_.camera_configuration.calibration_parameters.left_cam.disto[i] = 0.f;


        // Create a mock ZEDCamera with a default ZEDCameraConfig based on test_config_
        // This mock_zed_camera_ is what the ZEDDataProvider will internally use.
        // We need to ensure the ZEDDataProvider's constructor can accept a ZEDCamera* or ZEDCamera::Ptr.
        // The current ZEDDataProvider constructor creates its own ZEDCamera.
        // To test utility functions in isolation, we might not need the full ZEDDataProvider instance,
        // or we need to refactor ZEDDataProvider to allow injecting a ZEDCamera (which is better for testing).
        // For now, we'll test static or const methods that can be called without complex state.
        // The methods like createImageContainer and generateMaskFromZEDDetections are const,
        // so they can be called on an instance. We'll make a dummy instance.
        // The constructor of ZEDDataProvider creates its own ZEDCamera. This is problematic for mocking.
        // A better design would be to inject the ZEDCamera.
        // For now, we can test the static `slMat_to_cvMat` directly.
        // For `createImageContainer` and `generateMaskFromZEDDetections`, we'll need an instance of `ZEDDataProvider`.
        // We'll construct a ZEDDataProvider, but its internal zed_camera_wrapper_ won't be our mock unless we modify ZEDDataProvider.
        // Let's assume we can test the functions somewhat statically or with minimal state from the provider for now.

        // For methods that are part of ZEDDataProvider instance, we need an instance.
        // The constructor will try to open the camera. We'll mock that part.
        mock_zed_camera_ = std::make_shared<MockZEDCamera>(dyno::ZEDDataProvider::createZEDCameraConfig(test_config_));

        // We will test the static `slMat_to_cvMat` separately.
        // For instance methods, we'll need to create a `ZEDDataProvider`.
        // Its constructor creates a `common::ZEDCamera`. We can't directly inject our mock there
        // without changing `ZEDDataProvider`.
        // However, `createImageContainer` and `generateMaskFromZEDDetections` are `const`.
        // Their main dependency for *these specific tests* is `config_` and `slMat_to_cvMat`.
        // We will create a ZEDDataProvider instance and set its config.
    }

    // Helper to create a ZEDDataProvider instance with a specific config
    // This is a simplified version because the actual ZEDDataProvider constructor
    // creates its own ZEDCamera and tries to open it.
    // For these specific data conversion tests, we primarily need the config
    // and the static slMat_to_cvMat. The instance methods
    // `createImageContainer` and `generateMaskFromZEDDetections` are const.
    void initializeDataProvider(const dyno::ZEDConfig& config) {
        // This is a bit of a workaround. Ideally, ZEDCamera is injected.
        // We are testing const methods that depend mostly on config and static helpers.
        // The actual zed_camera_wrapper_ within this data_provider_ won't be our mock_zed_camera_
        // if the constructor instantiates its own.
        // However, the methods we're testing don't rely on an *opened* camera for their logic.
        data_provider_ = std::make_unique<dyno::ZEDDataProvider>(config, nullptr);
    }
};

// --- Test Cases for ZEDDataProvider: 3. Data Conversion and Mask Generation ---

TEST_F(ZEDDataProviderConversionTest, ZDP_CONV_001_slMatToCvMat_U8C4) {
    sl::Mat sl_mat(10, 10, sl::MAT_TYPE::U8_C4, sl::MEM::CPU);
    // Fill with some data
    for (int i = 0; i < 10 * 10 * 4; ++i) {
        sl_mat.getPtr<sl::uchar1>()[i] = static_cast<sl::uchar1>(i % 256);
    }

    cv::Mat cv_mat = dyno::ZEDDataProvider::slMat_to_cvMat(sl_mat);

    ASSERT_EQ(cv_mat.rows, 10);
    ASSERT_EQ(cv_mat.cols, 10);
    ASSERT_EQ(cv_mat.type(), CV_8UC4);
    for (int r = 0; r < 10; ++r) {
        for (int c = 0; c < 10; ++c) {
            for (int ch = 0; ch < 4; ++ch) {
                ASSERT_EQ(cv_mat.at<cv::Vec4b>(r, c)[ch], sl_mat.getPtr<sl::uchar1>()[(r * 10 + c) * 4 + ch]);
            }
        }
    }
}

TEST_F(ZEDDataProviderConversionTest, ZDP_CONV_002_createImageContainer_BGRAtoBGR) {
    test_config_.output_rgb = true;
    initializeDataProvider(test_config_);

    sl::Mat left_sl(720, 1280, sl::MAT_TYPE::U8_C4, sl::MEM::CPU);
    left_sl.setTo(sl::uchar4(10, 20, 30, 255)); // B, G, R, A
    sl::Mat depth_sl(720, 1280, sl::MAT_TYPE::F32_C1, sl::MEM::CPU);
    depth_sl.setTo(1.5f);

    dyno::ImageContainer::Ptr container = data_provider_->createImageContainer(
        12345.678, 0, left_sl, depth_sl, std::nullopt, std::nullopt);

    ASSERT_TRUE(container);
    const auto& rgb_mono_wrapper = container->get<dyno::ImageType::RGBMono>();
    ASSERT_FALSE(rgb_mono_wrapper.image.empty());
    ASSERT_EQ(rgb_mono_wrapper.image.type(), CV_8UC3) << "Expected BGR (CV_8UC3)";
    cv::Vec3b pixel = rgb_mono_wrapper.image.at<cv::Vec3b>(0,0);
    EXPECT_EQ(pixel[0], 10); // B
    EXPECT_EQ(pixel[1], 20); // G
    EXPECT_EQ(pixel[2], 30); // R
}

TEST_F(ZEDDataProviderConversionTest, ZDP_CONV_003_createImageContainer_BGRAtoGrayscale) {
    test_config_.output_rgb = false;
    initializeDataProvider(test_config_);

    sl::Mat left_sl(720, 1280, sl::MAT_TYPE::U8_C4, sl::MEM::CPU);
    left_sl.setTo(sl::uchar4(10, 20, 30, 255)); // B, G, R, A for a BGR-like input
    sl::Mat depth_sl(720, 1280, sl::MAT_TYPE::F32_C1, sl::MEM::CPU);
    depth_sl.setTo(1.5f);

    dyno::ImageContainer::Ptr container = data_provider_->createImageContainer(
        12345.678, 0, left_sl, depth_sl, std::nullopt, std::nullopt);

    ASSERT_TRUE(container);
    const auto& rgb_mono_wrapper = container->get<dyno::ImageType::RGBMono>();
    ASSERT_FALSE(rgb_mono_wrapper.image.empty());
    ASSERT_EQ(rgb_mono_wrapper.image.type(), CV_8UC1) << "Expected Grayscale (CV_8UC1)";
    // Approximate grayscale value for B=10, G=20, R=30 (standard OpenCV conversion: 0.299R + 0.587G + 0.114B)
    // 0.299*30 + 0.587*20 + 0.114*10 = 8.97 + 11.74 + 1.14 = 21.85
    unsigned char expected_gray = static_cast<unsigned char>(0.114 * 10 + 0.587 * 20 + 0.299 * 30);
    EXPECT_NEAR(rgb_mono_wrapper.image.at<unsigned char>(0,0), expected_gray, 1);
}

TEST_F(ZEDDataProviderConversionTest, ZDP_CONV_004_createImageContainer_DepthF32C1toCV64F) {
    initializeDataProvider(test_config_);

    sl::Mat left_sl(720, 1280, sl::MAT_TYPE::U8_C1, sl::MEM::CPU); // Dummy RGB
    left_sl.setTo(sl::uchar1(128));
    sl::Mat depth_sl(720, 1280, sl::MAT_TYPE::F32_C1, sl::MEM::CPU);
    depth_sl.setTo(2.75f);

    dyno::ImageContainer::Ptr container = data_provider_->createImageContainer(
        12345.678, 0, left_sl, depth_sl, std::nullopt, std::nullopt);

    ASSERT_TRUE(container);
    const auto& depth_wrapper = container->get<dyno::ImageType::Depth>();
    ASSERT_FALSE(depth_wrapper.image.empty());
    ASSERT_EQ(depth_wrapper.image.type(), CV_64FC1) << "Expected Depth (CV_64FC1)";
    EXPECT_DOUBLE_EQ(depth_wrapper.image.at<double>(0,0), 2.75);
}


TEST_F(ZEDDataProviderConversionTest, ZDP_MASK_001_GenerateMask_OD_Segmentation) {
    test_config_.enable_object_detection = true;
    test_config_.object_detection_enable_segmentation = true;
    initializeDataProvider(test_config_);

    sl::Objects zed_objects;
    zed_objects.object_list.resize(1);
    zed_objects.object_list[0].id = 1;
    zed_objects.object_list[0].mask.alloc(10, 10, sl::MAT_TYPE::U8_C1, sl::MEM::CPU);
    zed_objects.object_list[0].mask.setTo(sl::uchar1(0));
    for(int r=2; r<5; ++r) for(int c=2; c<5; ++c) zed_objects.object_list[0].mask.setValue(c,r,sl::uchar1(255));


    dyno::ImageType::MotionMask dummy_mask_type;
    cv::Mat generated_mask = data_provider_->generateMaskFromZEDDetections(
        zed_objects, std::nullopt, cv::Size(10, 10), dummy_mask_type);

    ASSERT_FALSE(generated_mask.empty());
    ASSERT_EQ(generated_mask.type(), CV_32SC1);
    for(int r=0; r<10; ++r) {
        for(int c=0; c<10; ++c) {
            if (r >= 2 && r < 5 && c >= 2 && c < 5) {
                EXPECT_EQ(generated_mask.at<int32_t>(r,c), 1);
            } else {
                EXPECT_EQ(generated_mask.at<int32_t>(r,c), 0);
            }
        }
    }
}

TEST_F(ZEDDataProviderConversionTest, ZDP_MASK_002_GenerateMask_OD_BoundingBox) {
    test_config_.enable_object_detection = true;
    test_config_.object_detection_enable_segmentation = false;
    initializeDataProvider(test_config_);

    sl::Objects zed_objects;
    zed_objects.object_list.resize(1);
    zed_objects.object_list[0].id = 2;
    zed_objects.object_list[0].bounding_box_2d[0] = sl::uint2(1,1); // Top-left
    zed_objects.object_list[0].bounding_box_2d[1] = sl::uint2(5,1); // Top-right
    zed_objects.object_list[0].bounding_box_2d[2] = sl::uint2(5,5); // Bottom-right
    zed_objects.object_list[0].bounding_box_2d[3] = sl::uint2(1,5); // Bottom-left

    dyno::ImageType::MotionMask dummy_mask_type;
    cv::Mat generated_mask = data_provider_->generateMaskFromZEDDetections(
        zed_objects, std::nullopt, cv::Size(10, 10), dummy_mask_type);

    ASSERT_FALSE(generated_mask.empty());
    ASSERT_EQ(generated_mask.type(), CV_32SC1);
     for(int r=0; r<10; ++r) {
        for(int c=0; c<10; ++c) {
            if (r >= 1 && r <= 5 && c >= 1 && c <= 5) { // BBox covers from (1,1) to (5,5)
                EXPECT_EQ(generated_mask.at<int32_t>(r,c), 2);
            } else {
                EXPECT_EQ(generated_mask.at<int32_t>(r,c), 0);
            }
        }
    }
}

TEST_F(ZEDDataProviderConversionTest, ZDP_MASK_003_GenerateMask_BT_Segmentation_WithODOffset) {
    test_config_.enable_body_tracking = true;
    test_config_.body_tracking_enable_segmentation = true;
    test_config_.enable_object_detection = true; // To test ID offset
    initializeDataProvider(test_config_);

    sl::Bodies zed_bodies;
    zed_bodies.body_list.resize(1);
    zed_bodies.body_list[0].id = 1; // Original body ID
    zed_bodies.body_list[0].mask.alloc(10, 10, sl::MAT_TYPE::U8_C1, sl::MEM::CPU);
    zed_bodies.body_list[0].mask.setTo(sl::uchar1(0));
    for(int r=3; r<6; ++r) for(int c=3; c<6; ++c) zed_bodies.body_list[0].mask.setValue(c,r,sl::uchar1(255));

    dyno::ImageType::MotionMask dummy_mask_type;
    cv::Mat generated_mask = data_provider_->generateMaskFromZEDDetections(
        std::nullopt, zed_bodies, cv::Size(10, 10), dummy_mask_type);

    ASSERT_FALSE(generated_mask.empty());
    ASSERT_EQ(generated_mask.type(), CV_32SC1);
    int expected_id = 1 + 1000; // ID offset
    for(int r=0; r<10; ++r) {
        for(int c=0; c<10; ++c) {
            if (r >= 3 && r < 6 && c >= 3 && c < 6) {
                EXPECT_EQ(generated_mask.at<int32_t>(r,c), expected_id);
            } else {
                EXPECT_EQ(generated_mask.at<int32_t>(r,c), 0);
            }
        }
    }
}


TEST_F(ZEDDataProviderConversionTest, ZDP_MASK_004_GenerateMask_OD_and_BT_Combined) {
    test_config_.enable_object_detection = true;
    test_config_.object_detection_enable_segmentation = false; // Use BBox for OD
    test_config_.enable_body_tracking = true;
    test_config_.body_tracking_enable_segmentation = true; // Use Seg for BT
    initializeDataProvider(test_config_);

    sl::Objects zed_objects;
    zed_objects.object_list.resize(1);
    zed_objects.object_list[0].id = 5;
    zed_objects.object_list[0].bounding_box_2d[0] = sl::uint2(0,0);
    zed_objects.object_list[0].bounding_box_2d[1] = sl::uint2(3,0);
    zed_objects.object_list[0].bounding_box_2d[2] = sl::uint2(3,3);
    zed_objects.object_list[0].bounding_box_2d[3] = sl::uint2(0,3);

    sl::Bodies zed_bodies;
    zed_bodies.body_list.resize(1);
    zed_bodies.body_list[0].id = 1; // Original body ID
    zed_bodies.body_list[0].mask.alloc(10, 10, sl::MAT_TYPE::U8_C1, sl::MEM::CPU);
    zed_bodies.body_list[0].mask.setTo(sl::uchar1(0));
    // Body mask overlaps with object BBox but should take precedence due to ID logic
    for(int r=2; r<5; ++r) for(int c=2; c<5; ++c) zed_bodies.body_list[0].mask.setValue(c,r,sl::uchar1(255));


    dyno::ImageType::MotionMask dummy_mask_type;
    cv::Mat generated_mask = data_provider_->generateMaskFromZEDDetections(
        zed_objects, zed_bodies, cv::Size(10, 10), dummy_mask_type);

    ASSERT_FALSE(generated_mask.empty());
    ASSERT_EQ(generated_mask.type(), CV_32SC1);

    int body_id_offset = 1 + 1000;
    for(int r=0; r<10; ++r) {
        for(int c=0; c<10; ++c) {
            if (r >= 2 && r < 5 && c >= 2 && c < 5) { // Body segmentation region
                EXPECT_EQ(generated_mask.at<int32_t>(r,c), body_id_offset) << "Failed at r=" << r << ", c=" << c;
            } else if (r >= 0 && r <= 3 && c >= 0 && c <= 3) { // Object BBox region (not overlapped by body)
                EXPECT_EQ(generated_mask.at<int32_t>(r,c), 5) << "Failed at r=" << r << ", c=" << c;
            } else {
                EXPECT_EQ(generated_mask.at<int32_t>(r,c), 0) << "Failed at r=" << r << ", c=" << c;
            }
        }
    }
}

TEST_F(ZEDDataProviderConversionTest, ZDP_MASK_005_GenerateMask_NoDetections) {
    test_config_.enable_object_detection = true; // Configured, but no data will be provided
    initializeDataProvider(test_config_);

    dyno::ImageType::MotionMask dummy_mask_type;
    cv::Mat generated_mask = data_provider_->generateMaskFromZEDDetections(
        std::nullopt, std::nullopt, cv::Size(10, 10), dummy_mask_type);

    ASSERT_FALSE(generated_mask.empty());
    ASSERT_EQ(generated_mask.type(), CV_32SC1);
    cv::Mat expected_zeros = cv::Mat::zeros(10, 10, CV_32SC1);
    // Check if all elements are zero
    ASSERT_EQ(cv::countNonZero(generated_mask != expected_zeros), 0);
}
```

## File: dataproviders/test_dataset_provider.cc
```
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */


#include "dynosam/dataprovider/DatasetProvider.hpp"
#include "dynosam/dataprovider/KittiDataProvider.hpp"
#include "internal/tmp_file.hpp"

#include <glog/logging.h>
#include <gtest/gtest.h>
#include <exception>

double randomGetItem(size_t idx) {
    return idx + 3.5;
}

class DoubleDataFolder : public dyno::DataFolder<double> {

public:
    DoubleDataFolder() {}

    std::string getFolderName() const override {
        return "dummy";
    }


    double getItem(size_t idx) {
        //idk some random equation to show that the value changes?
        return randomGetItem(idx);
    }
};


namespace dyno {

class DatasetProviderFixture : public ::testing::Test
{
public:
  DatasetProviderFixture()
  {
  }

protected:
  virtual void SetUp()
  {
    fs::create_directory(sandbox);
  }
  virtual void TearDown()
  {
    fs::remove_all(sandbox);
  }

  const fs::path sandbox{"/tmp/sandbox"};

};


TEST_F(DatasetProviderFixture, testDataFolderStructureConstructor) {

    DoubleDataFolder::Ptr ddf = std::make_shared<DoubleDataFolder>();
    DataFolderStructure<double> ds("some_path", ddf);

    auto retrieved_ddf = ds.getDataFolder<0>();
    EXPECT_EQ(ddf, retrieved_ddf);
}

TEST_F(DatasetProviderFixture, testDataFolderStructureGetPath) {

    DoubleDataFolder::Ptr ddf = std::make_shared<DoubleDataFolder>();
    DataFolderStructure<double> ds("/tmp/some_path", ddf);

    const std::string folder_path = ds.getAbsoluteFolderPath<0>();
    const std::string expected_folder_path = "/tmp/some_path/dummy";
    EXPECT_EQ(folder_path, expected_folder_path);
}

TEST_F(DatasetProviderFixture, testGenericDatasetLoadingWithValidationFailure) {

    DoubleDataFolder::Ptr ddf = std::make_shared<DoubleDataFolder>();


    EXPECT_THROW({GenericDataset<double>(sandbox, ddf);}, std::runtime_error);
}

TEST_F(DatasetProviderFixture, testGenericDatasetLoadingWithValidationSuccess) {

    DoubleDataFolder::Ptr ddf = std::make_shared<DoubleDataFolder>();

    const fs::path sandbox("/tmp/sandbox");

    std::ofstream{sandbox/ddf->getFolderName()}; // create regular file

    EXPECT_NO_THROW({GenericDataset<double>(sandbox, ddf);});
}


TEST_F(DatasetProviderFixture, testGenericDatasetMockLoading) {

    DoubleDataFolder::Ptr ddf = std::make_shared<DoubleDataFolder>();

    std::ofstream{sandbox/ddf->getFolderName()}; // create regular file

    GenericDataset<double> gd(sandbox, ddf);

    std::vector<double>& data = gd.getDataVector<0>();
    EXPECT_EQ(data.size(), 0u);

    gd.load(1);
    EXPECT_EQ(data.size(), 1u);
    EXPECT_EQ(data.at(0), randomGetItem(0u));

    gd.load(3);
    EXPECT_EQ(data.size(), 3u);
    EXPECT_EQ(data.at(2), randomGetItem(2u));

}

// TEST_F(DatasetProviderFixture, testOptionalGenericdataset) {
//     FunctionalDataFolder<double>::Ptr loader1 = std::make_shared<FunctionalDataFolder<double>>(
//         [=](size_t i) -> double {
//             return (double)i;
//         }
//     );

//     FunctionalDataFolder<int>::Ptr loader2 = std::make_shared<FunctionalDataFolder<int>>(
//         [=](size_t i) ->int {
//             return i;
//         }
//     );

//     GenericDataset<double, std::optional<int>> dataset_with_optional(sandbox, loader1, loader2)
// }


TEST(DynoDataset, testDummy) {

    // KittiDataLoader dd("/root/data/kitti/0000");
    // dd.spin();

}


// TEST(PlaybackGui, creation) {
//     PlaybackGui pg;

//     while(true) pg.draw();
// }



} //dyno
```

## File: internal/helpers.hpp
```
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#pragma once

#include <gtest/gtest.h>

#include <ament_index_cpp/get_package_prefix.hpp>

#include "dynosam/common/Camera.hpp"
#include "dynosam/common/Types.hpp"
#include "simulator.hpp"

/**
 * @brief gets the full path to the installation directory of the test data
 * which is expected to be at dynosam/test/data
 *
 * The full path will be the ROS install directory of this data after building
 *
 * @return std::string
 */
inline std::string getTestDataPath() {
  return ament_index_cpp::get_package_prefix("dynosam") + "/test/data";
}

namespace dyno_testing {

using namespace dyno;

inline KeypointStatus makeStatusKeypointMeasurement(
    TrackletId tracklet_id, ObjectId object_id, FrameId frame_id,
    const Keypoint& keypoint = Keypoint(), double sigma = 2.0) {
  gtsam::Vector2 kp_sigmas;
  kp_sigmas << sigma, sigma;
  MeasurementWithCovariance<Keypoint> kp_measurement(keypoint, kp_sigmas);
  return KeypointStatus(kp_measurement, frame_id, tracklet_id, object_id,
                        ReferenceFrame::LOCAL);
}

inline void compareLandmarks(const Landmarks& lmks_1, const Landmarks& lmks_2,
                             const float& tol = 1e-9) {
  ASSERT_EQ(lmks_1.size(), lmks_2.size());
  for (size_t i = 0u; i < lmks_1.size(); i++) {
    const auto& lmk_1 = lmks_1[i];
    const auto& lmk_2 = lmks_2[i];
    EXPECT_TRUE(gtsam::assert_equal(lmk_1, lmk_2, tol));
  }
}

inline void compareKeypoints(const Keypoints& lmks_1, const Keypoints& lmks_2,
                             const float& tol = 1e-9) {
  ASSERT_EQ(lmks_1.size(), lmks_2.size());
  for (size_t i = 0u; i < lmks_1.size(); i++) {
    const auto& lmk_1 = lmks_1[i];
    const auto& lmk_2 = lmks_2[i];
    EXPECT_TRUE(gtsam::assert_equal(lmk_1, lmk_2, tol));
  }
}

inline CameraParams makeDefaultCameraParams() {
  CameraParams::IntrinsicsCoeffs intrinsics(4);
  CameraParams::DistortionCoeffs distortion(4);
  intrinsics.at(0) = 721.5377;  // fx
  intrinsics.at(1) = 721.5377;  // fy
  intrinsics.at(2) = 609.5593;  // u0
  intrinsics.at(3) = 172.8540;  // v0
  return CameraParams(intrinsics, distortion, cv::Size(640, 480), "radtan");
}

inline Camera makeDefaultCamera() { return Camera(makeDefaultCameraParams()); }

inline Camera::Ptr makeDefaultCameraPtr() {
  return std::make_shared<Camera>(makeDefaultCameraParams());
}

inline dyno_testing::RGBDScenario makeDefaultScenario() {
  dyno_testing::ScenarioBody::Ptr camera =
      std::make_shared<dyno_testing::ScenarioBody>(
          std::make_unique<dyno_testing::ConstantMotionBodyVisitor>(
              gtsam::Pose3::Identity(),
              // motion only in x
              gtsam::Pose3(gtsam::Rot3::Identity(), gtsam::Point3(0.1, 0, 0))));
  // needs to be at least 3 overlap so we can meet requirements in graph
  // TODO: how can we do 1 point but with lots of overlap (even infinity
  // overlap?)
  dyno_testing::RGBDScenario scenario(
      camera,
      std::make_shared<dyno_testing::SimpleStaticPointsGenerator>(8, 3));

  // add one obect
  const size_t num_points = 3;
  dyno_testing::ObjectBody::Ptr object1 =
      std::make_shared<dyno_testing::ObjectBody>(
          std::make_unique<dyno_testing::ConstantMotionBodyVisitor>(
              gtsam::Pose3(gtsam::Rot3::Identity(), gtsam::Point3(10, 0, 0)),
              // motion only in x
              gtsam::Pose3(gtsam::Rot3::Identity(), gtsam::Point3(0.2, 0, 0))),
          std::make_unique<dyno_testing::ConstantObjectPointsVisitor>(
              num_points));

  dyno_testing::ObjectBody::Ptr object2 =
      std::make_shared<dyno_testing::ObjectBody>(
          std::make_unique<dyno_testing::ConstantMotionBodyVisitor>(
              gtsam::Pose3(gtsam::Rot3::Identity(), gtsam::Point3(10, 0, 0)),
              // motion only in x
              gtsam::Pose3(gtsam::Rot3::Identity(), gtsam::Point3(0.2, 0, 0))),
          std::make_unique<dyno_testing::ConstantObjectPointsVisitor>(
              num_points));

  scenario.addObjectBody(1, object1);
  scenario.addObjectBody(2, object2);
  return scenario;
}

}  // namespace dyno_testing
```

## File: internal/simulator.cc
```
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "simulator.hpp"


namespace dyno_testing {

} //dyno_testing
```

## File: internal/simulator.hpp
```
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#pragma once

#include <gtsam/geometry/Pose3.h>

#include "dynosam/common/Types.hpp"
#include "dynosam/frontend/RGBDInstance-Definitions.hpp"
#include "dynosam/utils/GtsamUtils.hpp"

namespace dyno_testing {

using namespace dyno;

class ScenarioBodyBase {
 public:
  DYNO_POINTER_TYPEDEFS(ScenarioBodyBase)

  virtual ~ScenarioBodyBase() {}

  virtual gtsam::Pose3 pose(FrameId frame_id) const = 0;  ///< pose at time t
  virtual gtsam::Pose3 motionWorld(
      FrameId frame_id) const = 0;  ///< motion in world frame from t-1 to t
  virtual gtsam::Pose3 motionBody(FrameId frame_id)
      const = 0;  ///< motion local frame from t-1 to t, in ^{t-1}X_{t-1}
  virtual gtsam::Pose3 motionWorldFromInitial(
      FrameId frame_id) const = 0;  ///< motion in world frame from 0 to t

  gtsam::Rot3 rotation(FrameId frame_id) const {
    return this->pose(frame_id).rotation();
  }
  gtsam::Vector3 translation(FrameId frame_id) const {
    return this->pose(frame_id).translation();
  }
};

class ScenarioBodyVisitor : public ScenarioBodyBase {
 public:
  DYNO_POINTER_TYPEDEFS(ScenarioBodyVisitor)
  virtual ~ScenarioBodyVisitor() {}

  virtual gtsam::Pose3 pose(FrameId frame_id) const = 0;  ///< pose at time t
  virtual gtsam::Pose3 motionWorld(
      FrameId frame_id) const = 0;  ///< motion in world frame from t-1 to t

  ///< motion local frame from t-1 to t, in ^{t-1}X_{t-1}
  virtual gtsam::Pose3 motionBody(FrameId frame_id) const override {
    // from t-1 to t
    const gtsam::Pose3 motion_k = motionWorld(frame_id);
    const gtsam::Pose3 pose_k = pose(frame_id);
    // TODO: check
    return pose_k.inverse() * motion_k * pose_k.inverse();
  }
  virtual gtsam::Pose3 motionWorldFromInitial(
      FrameId frame_id) const = 0;  ///< motion in world frame from 0 to t
};

class ScenarioBody : public ScenarioBodyBase {
 public:
  DYNO_POINTER_TYPEDEFS(ScenarioBody)

  ScenarioBody(ScenarioBodyVisitor::UniquePtr body_visitor)
      : body_visitor_(std::move(body_visitor)) {}

  gtsam::Pose3 pose(FrameId frame_id) const override {
    return body_visitor_->pose(frame_id);
  }
  gtsam::Pose3 motionWorld(FrameId frame_id) const override {
    return body_visitor_->motionWorld(frame_id);
  }
  gtsam::Pose3 motionBody(FrameId frame_id) const override {
    return body_visitor_->motionBody(frame_id);
  }
  gtsam::Pose3 motionWorldFromInitial(FrameId frame_id) const override {
    return body_visitor_->motionWorldFromInitial(frame_id);
  }

 protected:
  ScenarioBodyVisitor::UniquePtr body_visitor_;
};

using TrackedPoint = std::pair<TrackletId, gtsam::Point3>;
using TrackedPoints = std::vector<TrackedPoint>;

struct PointsGenerator {
  /**
   * @brief Static function to generate a unique tracklet for any generator. If
   * increment is true, the global tracklet id will be incremented
   *
   * @param increment
   * @return TrackletId
   */
  static TrackletId getTracklet(bool increment = true) {
    static TrackletId global_tracklet = 0;

    auto tracklet_id = global_tracklet;

    if (increment) global_tracklet++;
    return tracklet_id;
  }

  static TrackedPoint generateNewPoint(const gtsam::Point3& mean, double sigma,
                                       int32_t seed = 42) {
    gtsam::Point3 point = dyno::utils::perturbWithNoise(mean, sigma, seed);
    return std::make_pair(PointsGenerator::getTracklet(true), point);
  }
};

/**
 * @brief Base class that knows how to generate points given the
 * ScenarioBodyVisitor for an object
 *
 */
class ObjectPointGeneratorVisitor {
 public:
  DYNO_POINTER_TYPEDEFS(ObjectPointGeneratorVisitor)

  virtual ~ObjectPointGeneratorVisitor() = default;
  virtual TrackedPoints getPointsWorld(
      const ScenarioBodyVisitor::UniquePtr& body_visitor,
      FrameId frame_id) const = 0;
};

class StaticPointGeneratorVisitor {
 public:
  DYNO_POINTER_TYPEDEFS(StaticPointGeneratorVisitor)

  virtual ~StaticPointGeneratorVisitor() = default;
  virtual TrackedPoints getPointsWorld(FrameId frame_id) const = 0;
};

class ObjectBody : public ScenarioBody {
 public:
  DYNO_POINTER_TYPEDEFS(ObjectBody)

  // struct Params {
  //     double enters_scenario_ = 0.0;
  //     double leaves_scenario_ = std::numeric_limits<double>::max();
  // };

  ObjectBody(ScenarioBodyVisitor::UniquePtr body_visitor,
             ObjectPointGeneratorVisitor::UniquePtr points_visitor)
      : ScenarioBody(std::move(body_visitor)),
        points_visitor_(std::move(points_visitor)) {}

  virtual FrameId entersScenario() const { return 0; };
  virtual FrameId leavesScenario() const {
    return std::numeric_limits<FrameId>::max();
  };
  virtual TrackedPoints getPointsWorld(FrameId frame_id) const {
    return points_visitor_->getPointsWorld(body_visitor_, frame_id);
  };

 protected:
  ObjectPointGeneratorVisitor::UniquePtr points_visitor_;
};

// Motion and pose visotors
class ConstantMotionBodyVisitor : public ScenarioBodyVisitor {
 public:
  DYNO_POINTER_TYPEDEFS(ConstantMotionBodyVisitor)
  ConstantMotionBodyVisitor(const gtsam::Pose3& pose_0,
                            const gtsam::Pose3& motion)
      : pose_0_(pose_0), motion_(motion) {}

  virtual gtsam::Pose3 pose(FrameId frame_id) const override {
    // from Pose Changes From a Different Point of View
    return motionWorldFromInitial(frame_id) * pose_0_;
  }

  virtual gtsam::Pose3 motionWorld(FrameId) const override { return motion_; }

  // TODO: I have no idea if this is right for constant motion but whatevs...
  gtsam::Pose3 motionWorldFromInitial(FrameId frame_id) const {
    return gtsam::Pose3::Expmap(frame_id * gtsam::Pose3::Logmap(motion_));
  }

 private:
  const gtsam::Pose3 pose_0_;
  const gtsam::Pose3 motion_;
};

// Points generator visitor
class ConstantObjectPointsVisitor : public ObjectPointGeneratorVisitor {
 public:
  ConstantObjectPointsVisitor(size_t num_points) : num_points_(num_points) {}

  // TODO: this assumes that the points we get from the object are ALWAYS the
  // same and ALWAYS the same order
  //
  TrackedPoints getPointsWorld(
      const ScenarioBodyVisitor::UniquePtr& body_visitor,
      FrameId frame_id) const override {
    if (!is_init) {
      initalisePoints(body_visitor->pose(0));
    }

    TrackedPoints points_world_t;  // points in world frame at time t
    for (const auto& tracked_point : points_world_0_) {
      auto tracklet_id = tracked_point.first;
      auto point = tracked_point.second;
      points_world_t.push_back(std::make_pair(
          tracklet_id, body_visitor->motionWorldFromInitial(frame_id) * point));
    }

    return points_world_t;
  }

 private:
  void initalisePoints(const gtsam::Pose3& P0) const {
    std::mt19937 engine(42);
    std::uniform_real_distribution<double> normal(0.0, 1.0);

    for (size_t i = 0; i < num_points_; i++) {
      // generate around pose0 with a normal distrubution around the translation
      // component
      // gtsam::Point3 p(P0.x() + normal(engine), P0.y() + normal(engine),
      //           P0.z() + normal(engine));

      // points_world_0_.push_back(std::make_pair(PointsGenerator::getTracklet(true),
      // p));
      points_world_0_.push_back(
          PointsGenerator::generateNewPoint(P0.translation(), 1.0));
    }

    is_init = true;
  }

  const size_t num_points_;

  // mutable so can be changed in the initalised poitns function, which is
  // called once
  mutable TrackedPoints points_world_0_;  // points in the world frame at time 0
  mutable bool is_init{false};
};

// I think this only ever means that a point can be seen by a max o
class SimpleStaticPointsGenerator : public StaticPointGeneratorVisitor {
 public:
  SimpleStaticPointsGenerator(size_t num_points_per_frame, size_t overlap)
      : num_points_per_frame_(num_points_per_frame),
        overlap_(overlap),
        has_overlap_(overlap < num_points_per_frame) {}

  TrackedPoints getPointsWorld(FrameId frame_id) const override {
    // expect we always start at zero
    if (frame_id == 0) {
      generateNewPoints(num_points_per_frame_);
      return points_world_0_;
    } else {
      // must have at least this many points after the first (zeroth) frame
      CHECK_GE(points_world_0_.size(), num_points_per_frame_);

      CHECK(has_overlap_) << "not implemented";
      int diff = (int)num_points_per_frame_ - (int)overlap_;
      CHECK(diff > 0);
      generateNewPoints((size_t)diff);

      size_t start_i = frame_id * ((size_t)diff);
      CHECK_GT(start_i, 0);

      size_t end_i = start_i + num_points_per_frame_ - 1;
      CHECK_LT(end_i, points_world_0_.size());

      TrackedPoints points_in_window;
      for (size_t i = start_i; i <= end_i; i++) {
        points_in_window.push_back(points_world_0_.at(i));
      }

      CHECK_EQ(points_in_window.size(), num_points_per_frame_);
      return points_in_window;
    }
  }

 private:
  void generateNewPoints(size_t num_new) const {
    // points can be distributed over this distance
    constexpr double point_distance_sigma = 40;
    for (size_t i = 0; i < num_new; i++) {
      points_world_0_.push_back(PointsGenerator::generateNewPoint(
          gtsam::Point3(0, 0, 0), point_distance_sigma));
    }
  }

  const size_t num_points_per_frame_;
  const size_t overlap_;
  const bool has_overlap_;
  mutable TrackedPoints
      points_world_0_;  // all points in the world frame at time 0. This may be
                        // uppdated overtime within the getPointsWorld
};

class Scenario {
 public:
  Scenario(ScenarioBody::Ptr camera_body,
           StaticPointGeneratorVisitor::Ptr static_points_generator)
      : camera_body_(camera_body),
        static_points_generator_(static_points_generator) {}

  void addObjectBody(ObjectId object_id, ObjectBody::Ptr object_body) {
    CHECK_GT(object_id, background_label);
    object_bodies_.insert2(object_id, object_body);
  }

  gtsam::Pose3 cameraPose(FrameId frame_id) const {
    return camera_body_->pose(frame_id);
  }

  ObjectIds getObjectIds(FrameId frame_id) const {
    ObjectIds object_ids;
    for (const auto& [object_id, obj] : object_bodies_) {
      if (objectInScenario(object_id, frame_id))
        object_ids.push_back(object_id);
    }

    return object_ids;
  }

  bool objectInScenario(ObjectId object_id, FrameId frame_id) const {
    if (object_bodies_.exists(object_id)) {
      const auto& object = object_bodies_.at(object_id);

      return frame_id >= object->entersScenario() &&
             frame_id < object->leavesScenario();
    }
    return false;
  }

 protected:
  ScenarioBody::Ptr camera_body_;
  StaticPointGeneratorVisitor::Ptr static_points_generator_;
  gtsam::FastMap<ObjectId, ObjectBody::Ptr> object_bodies_;
};

class RGBDScenario : public Scenario {
 public:
  RGBDScenario(ScenarioBody::Ptr camera_body,
               StaticPointGeneratorVisitor::Ptr static_points_generator)
      : Scenario(camera_body, static_points_generator) {}

  RGBDInstanceOutputPacket::Ptr getOutput(FrameId frame_id) const {
    StatusLandmarkVector static_landmarks, dynamic_landmarks;
    StatusKeypointVector static_keypoint_measurements,
        dynamic_keypoint_measurements;

    MotionEstimateMap motions;
    const gtsam::Pose3 X_world = cameraPose(frame_id);

    // tracklets should be uniqyue but becuase we use the DynamicPointSymbol
    // they only need to be unique per frame
    for (const auto& [object_id, object] : object_bodies_) {
      if (objectInScenario(object_id, frame_id)) {
        const gtsam::Pose3 H_world_k = object->motionWorld(frame_id);
        TrackedPoints points_world = object->getPointsWorld(frame_id);

        motions.insert2(object_id,
                        dyno::ReferenceFrameValue<gtsam::Pose3>(
                            H_world_k, dyno::ReferenceFrame::GLOBAL));

        // convert to status vectors
        for (const TrackedPoint& tracked_p_world : points_world) {
          auto tracklet_id = tracked_p_world.first;
          auto p_world = tracked_p_world.second;
          const gtsam::Point3 p_camera = X_world.inverse() * p_world;

          // TODO: covariance
          MeasurementWithCovariance<Landmark> lmk_measurement(p_camera);
          auto landmark_status = dyno::LandmarkStatus::DynamicInLocal(
              lmk_measurement, frame_id, tracklet_id, object_id);

          dynamic_landmarks.push_back(landmark_status);

          // the keypoint sttatus should be unused in the RGBD case but
          // we need it to fill out the data structures
          // TODO: covariance?
          MeasurementWithCovariance<Keypoint> kp_measurement{dyno::Keypoint()};
          auto keypoint_status = dyno::KeypointStatus::DynamicInLocal(
              kp_measurement, frame_id, tracklet_id, object_id);
          dynamic_keypoint_measurements.push_back(keypoint_status);
        }
      }
    }

    // add static points
    const TrackedPoints static_points_world =
        static_points_generator_->getPointsWorld(frame_id);

    // convert to status vectors
    for (const TrackedPoint& tracked_p_world : static_points_world) {
      auto tracklet_id = tracked_p_world.first;
      auto p_world = tracked_p_world.second;
      const gtsam::Point3 p_camera = X_world.inverse() * p_world;

      // TODO: covariance
      MeasurementWithCovariance<Landmark> lmk_measurement(p_camera);
      auto landmark_status =
          dyno::LandmarkStatus::StaticInLocal(p_camera, frame_id, tracklet_id);
      static_landmarks.push_back(landmark_status);

      // the keypoint sttatus should be unused in the RGBD case but
      // we need it to fill out the data structures
      MeasurementWithCovariance<Keypoint> kp_measurement{dyno::Keypoint()};
      auto keypoint_status = dyno::KeypointStatus::StaticInLocal(
          kp_measurement, frame_id, tracklet_id);
      static_keypoint_measurements.push_back(keypoint_status);
    }

    return std::make_shared<RGBDInstanceOutputPacket>(
        static_keypoint_measurements, dynamic_keypoint_measurements,
        static_landmarks, dynamic_landmarks, X_world, frame_id, frame_id,
        motions);
  }
};

}  // namespace dyno_testing
```

## File: internal/tmp_file.cc
```
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "tmp_file.hpp"

#define BOOST_NO_CXX11_SCOPED_ENUMS
#include <boost/filesystem.hpp>
#undef BOOST_NO_CXX11_SCOPED_ENUMS

#include <fstream>
#include <iostream>
#include <mutex>
#include <cstdio>
#include <cstdlib>
#include <glog/logging.h>
#include <unistd.h>

namespace dyno_testing
{

TempFile::TempFile()
{
  relative_path = boost::filesystem::unique_path("%%%%_%%%%_%%%%_%%%%");
  // absolute_path = boost::filesystem::temp_directory_path() / relative_path; //opening temp file does not actually
  // create a file..?
  absolute_path = boost::filesystem::current_path() / relative_path;

  write_stream.open(getFilePath());

  write_stream.close();
}

TempFile::~TempFile()
{
  if (write_stream.is_open())
  {
    write_stream.close();
  }

  if (read_stream.is_open())
  {
    read_stream.close();
  }

  boost::filesystem::remove(absolute_path);
}

std::string TempFile::readLine() const
{
  const std::lock_guard<std::mutex> lg(io_mutex);
  read_stream.open(getFilePath());

  // update the read ptr to the last position
  // TODO:(jesse) ensure the buf is no greater than the length of the file
  std::filebuf* pbuf = read_stream.rdbuf();
  pbuf->pubseekpos(current_pos);

  std::string line;
  std::getline(read_stream, line);

  // set current position to the pos in the file for next read
  current_pos = pbuf->pubseekoff(0, read_stream.cur);

  // if(static_cast<uint8_t>(read_stream.peek()) != 0) {
  //     line += "\n";
  // }

  read_stream.close();
  return line;
}
void TempFile::write(const std::string& data)
{
  const std::lock_guard<std::mutex> lg(io_mutex);

  write_stream.open(getFilePath(), std::ios::app);

  write_stream << data << std::endl;
  write_stream.close();
}

}  // namespace dyno_testing
```

## File: internal/tmp_file.hpp
```
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#pragma once
#include <iostream>
#include <cstdio>
#include <cstdlib>
#define BOOST_NO_CXX11_SCOPED_ENUMS
#include <boost/filesystem.hpp>
#undef BOOST_NO_CXX11_SCOPED_ENUMS
#include <string>
#include <mutex>
#include <cstdio>
#include <cstdlib>

namespace dyno_testing {
static constexpr std::string_view endl = "\n";


class TempFile
{
public:
  TempFile();
  ~TempFile();

  /**
   * @brief Get the absolute file path
   *
   * @return std::string
   */
  inline std::string getFilePath() const
  {
    return absolute_path.native();
  }

  std::string readLine() const;
  void write(const std::string& data);

private:
  boost::filesystem::path relative_path;
  boost::filesystem::path absolute_path;
  mutable std::ofstream write_stream;
  mutable std::ifstream read_stream;

  std::FILE* tmpf;
  mutable std::mutex io_mutex;
  mutable long current_pos = 0;
};

inline std::ostream& operator<<(std::ostream& os, const TempFile& tmp_file)
{
  os << tmp_file.readLine();
  return os;
}

inline std::istream& operator>>(std::istream& is, TempFile& tmp_file)
{
  std::string input_line;
  std::getline(is, input_line);
  tmp_file.write(input_line);
  return is;
}

inline TempFile& operator<<(TempFile& tmp_file, const std::string& data)
{
  tmp_file.write(data);
  return tmp_file;
}

} // dyno_testing
```

## File: pipelines_params/test_params.cc
```
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "internal/helpers.hpp"
#include "dynosam/frontend/vision/TrackerParams.hpp"


#include <glog/logging.h>
#include <gtest/gtest.h>
#include <config_utilities/parsing/yaml.h>           // Enable fromYamlFile().



using namespace dyno;

TEST(TrackerParams, testLoadingDefault) {

    const std::string file = getTestDataPath() + "/tracking_params.yaml";
    LOG(INFO) << file;

    TrackerParams config = config::fromYamlFile<TrackerParams>(file);

    const std::string config_as_string = config::toString(config);
    std::cout << config_as_string << std::endl;

}
```

## File: pipelines_params/test_pipelines.cc
```
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */
#include "internal/simulator.hpp"
#include "internal/helpers.hpp"

#include "dynosam/utils/SafeCast.hpp"

#include "dynosam/pipeline/PipelineBase.hpp"
#include "dynosam/pipeline/PipelinePayload.hpp"
#include "dynosam/common/ModuleBase.hpp"

#include "dynosam/frontend/FrontendPipeline.hpp"
#include "dynosam/backend/RGBDBackendModule.hpp"

#include <filesystem>

#include <glog/logging.h>
#include <gtest/gtest.h>
#include <variant>
#include <type_traits>

using namespace dyno;


namespace fs = std::filesystem;
class FrontendWithFiles : public ::testing::Test
{
public:
  FrontendWithFiles()
  {
  }

protected:
  virtual void SetUp()
  {
    fs::create_directory(sandbox);
  }
  virtual void TearDown()
  {
    fs::remove_all(sandbox);
  }

  const fs::path sandbox{"/tmp/sandbox_json_backend"};

};


//TODO: why is this in the backend testing... oh well...
TEST_F(FrontendWithFiles, testLoadingFrontendWithJson) {
    using namespace dyno;
    auto scenario = dyno_testing::makeDefaultScenario();

    std::map<FrameId, RGBDInstanceOutputPacket::Ptr> rgbd_output;

    for(size_t i = 0; i < 10; i++) {
        auto output = scenario.getOutput(i);
        rgbd_output.insert({i, output});
    }

    fs::path tmp_bison_path = sandbox / "simple_bison.bson";
    std::string tmp_bison_path_str = tmp_bison_path;

    JsonConverter::WriteOutJson(rgbd_output, tmp_bison_path_str, JsonConverter::Format::BSON);

    FrontendOfflinePipeline<RGBDBackendModule::ModuleTraits> offline_backed("offline-rgbdfrontend", tmp_bison_path_str);

    ThreadsafeQueue<FrontendOutputPacketBase::ConstPtr> queue;
    offline_backed.registerOutputQueue(&queue);

     int consumed = 0;
    while(offline_backed.spinOnce()) {
        FrontendOutputPacketBase::ConstPtr base;
        bool result = queue.popBlocking(base);

        EXPECT_TRUE(result);
        EXPECT_TRUE(base != nullptr);
        RGBDInstanceOutputPacket::ConstPtr derived = safeCast<FrontendOutputPacketBase, RGBDInstanceOutputPacket>(base);
        EXPECT_TRUE(derived != nullptr);

        //this value indexed by consume will be wrong if pop blocking doesnt work
        EXPECT_EQ(*derived, *rgbd_output.at(consumed));
        consumed++;
    }

    //should spin until the pipeline is shutdown which happens when we dont have any more data
    offline_backed.spin();
    EXPECT_TRUE(offline_backed.isShutdown());
    EXPECT_EQ(consumed, 10); //we should habve processed data 10 tiuems

}



//https://www.cppstories.com/2018/09/visit-variants/
template<class... Ts> struct overload : Ts... { using Ts::operator()...; };
template<class... Ts> overload(Ts...) -> overload<Ts...>; // line not needed in C++20


// //must be T& not const T&
// template<typename T>
// struct Visitor {

//     void process(const T& t) {
//         LOG(INFO) << t;
//     }

//     void operator()(T& t) { process(t);}

// };



// template<typename T> struct is_variant : std::false_type {};

// template<typename ...Variants>
// struct is_variant<std::variant<Variants...>> : std::true_type {};

// template<typename T>
// inline constexpr bool is_variant_v=is_variant<T>::value;

// // main lookup logic of looking up a type in a list.
// //https://www.appsloveworld.com/cplus/100/22/how-do-i-check-if-an-stdvariant-can-hold-a-certain-type
// template<typename T, typename... Variants>
// struct isoneof : public std::false_type {};

// template<typename T, typename FrontVariant, typename... RestVariants>
// struct isoneof<T, FrontVariant, RestVariants...> : public
//   std::conditional<
//     std::is_same<T, FrontVariant>::value,
//     std::true_type,
//     isoneof<T, RestVariants...>
//   >::type {};

// // convenience wrapper for std::variant<>.
// template<typename T, typename Variants>
// struct isvariantmember : public std::false_type {};

// template<typename T, typename... Variants>
// struct isvariantmember<T, std::variant<Variants...>> : public isoneof<T, Variants...> {};

// template<typename T, typename... Variants>
// inline constexpr bool isvariantmember_v = isvariantmember<T, Variants...>::value;


// template<typename IPacket, typename OPacket, typename MInput = IPacket, typename MOutput = OPacket>
// class MBase {
// public:
//     using InputPacket = IPacket;
//     using OutputPacket = OPacket;
//     using ModuleInput = MInput;
//     using ModuleOutput = MOutput;

// public:
//     constexpr static bool IsInputPacketVariant = is_variant_v<IPacket>;
//     constexpr static bool IsOutputPacketVariant = is_variant_v<OutputPacket>;

//     constexpr static bool IsModuleInputVariant = is_variant_v<ModuleInput>;
//     constexpr static bool IsModuleOutputVariant = is_variant_v<ModuleOutput>;


//     //either InputPacket is a variant or the InputPacket and ModuleInput are the same
//     //and no casting is required
//     static_assert(IsInputPacketVariant || std::is_same_v<ModuleInput,InputPacket>,
//         "InputPacket specified by IPacket is not a std::variant or ModuleInput and InputPacket are not the same type");

//     static_assert(IsOutputPacketVariant || std::is_same_v<ModuleOutput,OutputPacket>,
//         "OutputPacket specified by OPacket is not a std::variant or ModuleOutput and OutputPacket are not the same type");

//     // //if IsInputPacketVariant, then ModuleInput cannot also be a variant!!
//     static_assert(!(IsInputPacketVariant && IsModuleInputVariant),
//         "InputPacket is a variant, so ModuleInput cannot also be a variant, but a type within the variant");

//     static_assert(!(IsOutputPacketVariant && IsModuleOutputVariant),
//         "OutputPacket is a variant, so ModuleOutput cannot also be a variant, but a type within the variant");

//     static_assert(isvariantmember_v<ModuleInput, InputPacket> || std::is_same_v<ModuleInput,InputPacket>,
//         "If InputPacket is a variant, ModuleInput is not a type within the variant, or ModuleInput and InputPacket types are not the same");

//     static_assert(isvariantmember_v<ModuleOutput, OutputPacket> || std::is_same_v<ModuleOutput,OutputPacket>,
//         "If InputPacket is a variant, ModuleOutput is not a type within the variant, or ModuleOutput and InputPacket types are not the same");

// public:
//     ModuleInput cast(const InputPacket& packet) const {
//         if constexpr (IsInputPacketVariant) {
//             try {
//                 //packet should be a variant the ModuleOutput is a type within the std::variant
//                 //and the static asserts guarantee that ModuleInput is also not a variant
//                 return std::get<ModuleInput>(packet);
//             }
//             catch(const std::bad_variant_access& e) {
//                 throw std::runtime_error("Bad access");
//             }
//         }
//         else {
//            //if InputPacket is not a variant, then the static asserts guarantee that InputPacket == ModuleInput;
//            return packet;
//         }
//     }




//     virtual ModuleOutput process(const ModuleInput& input) {
//         LOG(INFO) << input;
//         return ModuleOutput{};
//     }

//     OutputPacket spinOnce(const InputPacket& packet) {
//         return process(cast(packet));
//     }

// };


// using VI = std::variant<double, std::string>;

// template<typename INPUT, typename OUTPUT>
// class Module : public MBase<VI, VI, INPUT, OUTPUT> {
// public:
//     using MBase<VI, VI, INPUT, OUTPUT>::process;
// };

// class DerivedModule : public Module<double, std::string> {

// public:
//     std::string process(const double& input) override {
//         LOG(INFO) << "Input" << input;
//         return "result";
//     }
// };



// TEST(PipelineBase, checkCompilation) {
//     using VariantInput = std::variant<double, int>;
//     //IPacket is variant so we need to specify ModuleInput
//     MBase<VariantInput, double, double>{};
//     MBase<int, double, int, double>{};
//     MBase<VariantInput, double, int, double>{};
//     // MBase<VariantInput, double, int, std::string>{};
//     // MBase<VariantInput, double, VariantInput, double>{};

//     // constexpr bool r = MBase<VariantInput, double, int, double>::IsModuleInputVariantMember;
// }

// TEST(PipelineBase, moduleProcess) {
//     // using VariantInput = std::variant<double, std::string>;
//     // //IPacket is variant so we need to specify ModuleInput
//     // MBase<VariantInput, double, std::string> mb{};

//     // //this should cast the input to a double
//     // mb.spinOnce("hi");

//     DerivedModule dm;
//     dm.spinOnce("string");
// }


// TEST(PipelineBase, testWithVairant) {

//     using Var = std::variant<int, std::string>;
//     using VarPipeline = FunctionalSIMOPipelineModule<Var, NullPipelinePayload>;

//     using VarModule = VariantModule<Var, NullPipelinePayload, std::string>;

//     VarPipeline::InputQueue input_queue;
//     VarPipeline::OutputQueue output_queue;
//     VarModule var_module;
//     // Visitor<std::string> string_visitor;
//     // Visitor<int> int_visitor;

//     VarPipeline p("var_module", &input_queue,
//         [&](const VarPipeline::InputConstSharedPtr& var_ptr) -> VarPipeline::OutputConstSharedPtr {
//             // return var_module.spinOnce(var_ptr);
//             return var_module.spinOnce(var_ptr);


//         },
//         false);

//     // input_queue.push(std::make_shared<Var>((int)3));
//     input_queue.push(std::make_shared<Var>("string"));

//     p.spinOnce();
//     p.spinOnce();


// }
```

## File: thread_safety/test_threadsafe_imu_buffer.cc
```
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

/*******************************************************************************
 * MIT License
 *
 * Copyright (c) 2019 Toni Rosinol
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 *all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 *******************************************************************************/

/********************************************************************************
 Copyright 2017 Autonomous Systems Lab, ETH Zurich, Switzerland
   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at
       http://www.apache.org/licenses/LICENSE-2.0
   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
*********************************************************************************/

#include "dynosam/frontend/imu/ThreadSafeImuBuffer.hpp"
#include "dynosam/frontend/imu/Imu-Definitions.hpp"
#include "dynosam/frontend/imu/ImuMeasurements.hpp"

#include <glog/logging.h>
#include <gtest/gtest.h>

namespace dyno {

TEST(ThreadsafeImuBuffer, PopFromEmptyBuffer) {
  dyno::ThreadsafeImuBuffer buffer(-1);
  // Pop from empty buffer.
  Timestamps imu_timestamps(1, 2);
  ImuAccGyrs imu_measurements(6, 2);
  {
    dyno::ThreadsafeImuBuffer::QueryResult success =
        buffer.getImuDataBtwTimestamps(50, 100, &imu_timestamps,
                                       &imu_measurements);
    EXPECT_EQ(success,
              ThreadsafeImuBuffer::QueryResult::kDataNotYetAvailable);
    EXPECT_EQ(0u, imu_timestamps.size());
    EXPECT_EQ(0u, imu_measurements.size());
  }
  {
    dyno::ThreadsafeImuBuffer::QueryResult success =
        buffer.getImuDataBtwTimestamps(50, 100, &imu_timestamps,
                                       &imu_measurements, true);
    EXPECT_EQ(success,
              ThreadsafeImuBuffer::QueryResult::kDataNotYetAvailable);
    EXPECT_EQ(0u, imu_timestamps.size());
    EXPECT_EQ(0u, imu_measurements.size());
  }
  {
    dyno::ThreadsafeImuBuffer::QueryResult success =
        buffer.getImuDataInterpolatedUpperBorder(50, 100, &imu_timestamps,
                                                 &imu_measurements);
    EXPECT_EQ(success,
              ThreadsafeImuBuffer::QueryResult::kDataNotYetAvailable);
    EXPECT_EQ(0u, imu_timestamps.size());
    EXPECT_EQ(0u, imu_measurements.size());
  }
  {
    dyno::ThreadsafeImuBuffer::QueryResult success =
        buffer.getImuDataInterpolatedBorders(50, 100, &imu_timestamps,
                                             &imu_measurements);
    EXPECT_EQ(success,
              ThreadsafeImuBuffer::QueryResult::kDataNotYetAvailable);
    EXPECT_EQ(0u, imu_timestamps.size());
    EXPECT_EQ(0u, imu_measurements.size());
  }
}

TEST(ThreadsafeImuBuffer, LinearInterpolate) {
  ImuAccGyr y;
  dyno::ThreadsafeImuBuffer::linearInterpolate(
      10, ImuAccGyr::Constant(10.0), 20, ImuAccGyr::Constant(50.0), 15, &y);
  EXPECT_EQ(y, ImuAccGyr::Constant(30.0));
}

TEST(ThreadsafeImuBuffer, getImuDataBtwTimestamps) {
  dyno::ThreadsafeImuBuffer buffer(-1);
  buffer.addMeasurement(10, ImuAccGyr::Constant(10.0));
  buffer.addMeasurement(15, ImuAccGyr::Constant(15.0));
  buffer.addMeasurement(20, ImuAccGyr::Constant(20.0));
  buffer.addMeasurement(25, ImuAccGyr::Constant(25.0));
  buffer.addMeasurement(30, ImuAccGyr::Constant(30.0));
  buffer.addMeasurement(40, ImuAccGyr::Constant(40.0));
  buffer.addMeasurement(50, ImuAccGyr::Constant(50.0));

  Timestamps imu_timestamps;
  ImuAccGyrs imu_measurements;
  dyno::ThreadsafeImuBuffer::QueryResult result;

  // Test aligned getter.
  result = buffer.getImuDataBtwTimestamps(20, 30, &imu_timestamps,
                                          &imu_measurements);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataAvailable);
  EXPECT_EQ(imu_timestamps.cols(), 1);
  EXPECT_EQ(imu_measurements.cols(), 1);
  EXPECT_EQ(imu_timestamps(0), 25);
  EXPECT_EQ(imu_measurements.col(0)(0), 25.0);

  // Test aligned getter, but asking for lower bound
  result = buffer.getImuDataBtwTimestamps(20, 30, &imu_timestamps,
                                          &imu_measurements, true);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataAvailable);
  EXPECT_EQ(imu_timestamps.cols(), 2);
  EXPECT_EQ(imu_measurements.cols(), 2);
  EXPECT_EQ(imu_timestamps(0), 20);
  EXPECT_EQ(imu_timestamps(1), 25);
  EXPECT_EQ(imu_measurements.col(0)(0), 20.0);
  EXPECT_EQ(imu_measurements.col(1)(0), 25.0);

  // Test unaligned getter (no lower/upper-interpolation).
  result = buffer.getImuDataBtwTimestamps(19, 31, &imu_timestamps,
                                          &imu_measurements);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataAvailable);
  EXPECT_EQ(imu_timestamps.cols(), 3);
  EXPECT_EQ(imu_measurements.cols(), 3);
  EXPECT_EQ(imu_timestamps(0), 20);
  EXPECT_EQ(imu_timestamps(1), 25);
  EXPECT_EQ(imu_timestamps(2), 30);
  EXPECT_EQ(imu_measurements.col(0)(0), 20.0);
  EXPECT_EQ(imu_measurements.col(1)(0), 25.0);
  EXPECT_EQ(imu_measurements.col(2)(0), 30.0);

  // Test unaligned getter but asking for lower bound.
  result = buffer.getImuDataBtwTimestamps(19, 31, &imu_timestamps,
                                          &imu_measurements, true);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataAvailable);
  EXPECT_EQ(imu_timestamps.cols(), 3);
  EXPECT_EQ(imu_measurements.cols(), 3);
  EXPECT_EQ(imu_timestamps(0), 20);
  EXPECT_EQ(imu_timestamps(1), 25);
  EXPECT_EQ(imu_timestamps(2), 30);
  EXPECT_EQ(imu_measurements.col(0)(0), 20.0);
  EXPECT_EQ(imu_measurements.col(1)(0), 25.0);
  EXPECT_EQ(imu_measurements.col(2)(0), 30.0);

  // Fail: query out of upper bound.
  result = buffer.getImuDataBtwTimestamps(40, 51, &imu_timestamps,
                                          &imu_measurements);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataNotYetAvailable);
  result = buffer.getImuDataBtwTimestamps(60, 61, &imu_timestamps,
                                          &imu_measurements);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataNotYetAvailable);

  // Fail: query out of lower bound.
  result = buffer.getImuDataBtwTimestamps(-1, 20, &imu_timestamps,
                                          &imu_measurements);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataNeverAvailable);
  result = buffer.getImuDataBtwTimestamps(-20, -10, &imu_timestamps,
                                          &imu_measurements);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataNeverAvailable);

  // Query in between two values: return nothing.
  result = buffer.getImuDataBtwTimestamps(21, 24, &imu_timestamps,
                                          &imu_measurements);
  EXPECT_EQ(result, dyno::ThreadsafeImuBuffer::QueryResult::
                        kTooFewMeasurementsAvailable);
  EXPECT_EQ(imu_timestamps.cols(), 0);
  EXPECT_EQ(imu_measurements.cols(), 0);
  result = buffer.getImuDataBtwTimestamps(21, 24, &imu_timestamps,
                                          &imu_measurements, true);
  EXPECT_EQ(result, dyno::ThreadsafeImuBuffer::QueryResult::
                        kTooFewMeasurementsAvailable);
  EXPECT_EQ(imu_timestamps.cols(), 0);
  EXPECT_EQ(imu_measurements.cols(), 0);

  // Query right between two values: return nothing.
  result = buffer.getImuDataBtwTimestamps(20, 25, &imu_timestamps,
                                          &imu_measurements);
  EXPECT_EQ(result, dyno::ThreadsafeImuBuffer::QueryResult::
                        kTooFewMeasurementsAvailable);
  EXPECT_EQ(imu_timestamps.cols(), 0);
  EXPECT_EQ(imu_measurements.cols(), 0);

  // Query right between two values but ask for lower bound: return lower bound.
  result = buffer.getImuDataBtwTimestamps(20, 25, &imu_timestamps,
                                          &imu_measurements, true);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataAvailable);
  EXPECT_EQ(imu_timestamps.cols(), 1);
  EXPECT_EQ(imu_measurements.cols(), 1);
  EXPECT_EQ(imu_timestamps(0), 20);
  EXPECT_EQ(imu_measurements.col(0)(0), 20.0);
}

TEST(ThreadsafeImuBuffer, getImuDataInterpolatedBorders) {
  dyno::ThreadsafeImuBuffer buffer(-1);
  buffer.addMeasurement(10, ImuAccGyr::Constant(10.0));
  buffer.addMeasurement(15, ImuAccGyr::Constant(15.0));
  buffer.addMeasurement(20, ImuAccGyr::Constant(20.0));
  buffer.addMeasurement(25, ImuAccGyr::Constant(25.0));
  buffer.addMeasurement(30, ImuAccGyr::Constant(30.0));
  buffer.addMeasurement(40, ImuAccGyr::Constant(40.0));
  buffer.addMeasurement(50, ImuAccGyr::Constant(50.0));

  Timestamps imu_timestamps;
  ImuAccGyrs imu_measurements;
  dyno::ThreadsafeImuBuffer::QueryResult result;

  // Test aligned getter (no-interpolation, only border values).
  result = buffer.getImuDataInterpolatedBorders(20, 30, &imu_timestamps,
                                                &imu_measurements);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataAvailable);
  EXPECT_EQ(imu_timestamps.cols(), 3);
  EXPECT_EQ(imu_measurements.cols(), 3);
  EXPECT_EQ(imu_timestamps(0), 20);
  EXPECT_EQ(imu_timestamps(1), 25);
  EXPECT_EQ(imu_timestamps(2), 30);
  EXPECT_EQ(imu_measurements.col(0)(0), 20.0);
  EXPECT_EQ(imu_measurements.col(1)(0), 25.0);
  EXPECT_EQ(imu_measurements.col(2)(0), 30.0);

  // Test aligned getter (no-interpolation).
  result = buffer.getImuDataInterpolatedBorders(20, 40, &imu_timestamps,
                                                &imu_measurements);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataAvailable);
  EXPECT_EQ(imu_timestamps.cols(), 4);
  EXPECT_EQ(imu_measurements.cols(), 4);
  EXPECT_EQ(imu_timestamps(0), 20);
  EXPECT_EQ(imu_timestamps(1), 25);
  EXPECT_EQ(imu_timestamps(2), 30);
  EXPECT_EQ(imu_timestamps(3), 40);
  EXPECT_EQ(imu_measurements.col(0)(0), 20.0);
  EXPECT_EQ(imu_measurements.col(1)(0), 25.0);
  EXPECT_EQ(imu_measurements.col(2)(0), 30.0);
  EXPECT_EQ(imu_measurements.col(3)(0), 40.0);

  // Test unaligned getter (lower/upper-interpolation).
  result = buffer.getImuDataInterpolatedBorders(19, 21, &imu_timestamps,
                                                &imu_measurements);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataAvailable);
  EXPECT_EQ(imu_timestamps.cols(), 3);
  EXPECT_EQ(imu_measurements.cols(), 3);
  EXPECT_EQ(imu_timestamps(0), 19);
  EXPECT_EQ(imu_timestamps(1), 20);
  EXPECT_EQ(imu_timestamps(2), 21);
  EXPECT_EQ(imu_measurements.col(0)(0), 19.0);
  EXPECT_EQ(imu_measurements.col(1)(0), 20.0);
  EXPECT_EQ(imu_measurements.col(2)(0), 21.0);

  // Fail: query out of upper bound.
  result = buffer.getImuDataInterpolatedBorders(40, 51, &imu_timestamps,
                                                &imu_measurements);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataNotYetAvailable);
  result = buffer.getImuDataInterpolatedBorders(60, 61, &imu_timestamps,
                                                &imu_measurements);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataNotYetAvailable);

  // Fail: query out of lower bound.
  result = buffer.getImuDataInterpolatedBorders(-1, 20, &imu_timestamps,
                                                &imu_measurements);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataNeverAvailable);
  result = buffer.getImuDataInterpolatedBorders(-20, -10, &imu_timestamps,
                                                &imu_measurements);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataNeverAvailable);

  // Query between two values: return the border values.
  result = buffer.getImuDataInterpolatedBorders(21, 29, &imu_timestamps,
                                                &imu_measurements);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataAvailable);
  EXPECT_EQ(imu_timestamps.cols(), 3);
  EXPECT_EQ(imu_measurements.cols(), 3);
  EXPECT_EQ(imu_timestamps(0), 21);
  EXPECT_EQ(imu_timestamps(1), 25);
  EXPECT_EQ(imu_timestamps(2), 29);
  EXPECT_EQ(imu_measurements.col(0)(0), 21.0);
  EXPECT_EQ(imu_measurements.col(1)(0), 25.0);
  EXPECT_EQ(imu_measurements.col(2)(0), 29.0);
}

TEST(ThreadsafeImuBuffer, getImuDataInterpolatedUpperBorder) {
  dyno::ThreadsafeImuBuffer buffer(-1);
  buffer.addMeasurement(10, ImuAccGyr::Constant(10.0));
  buffer.addMeasurement(15, ImuAccGyr::Constant(15.0));
  buffer.addMeasurement(20, ImuAccGyr::Constant(20.0));
  buffer.addMeasurement(25, ImuAccGyr::Constant(25.0));
  buffer.addMeasurement(30, ImuAccGyr::Constant(30.0));
  buffer.addMeasurement(40, ImuAccGyr::Constant(40.0));
  buffer.addMeasurement(50, ImuAccGyr::Constant(50.0));

  Timestamps imu_timestamps;
  ImuAccGyrs imu_measurements;
  dyno::ThreadsafeImuBuffer::QueryResult result;

  // Test aligned getter (no-interpolation).
  result = buffer.getImuDataInterpolatedUpperBorder(20, 40, &imu_timestamps,
                                                    &imu_measurements);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataAvailable);
  EXPECT_EQ(imu_timestamps.cols(), 4);
  EXPECT_EQ(imu_measurements.cols(), 4);
  EXPECT_EQ(imu_timestamps(0), 20);
  EXPECT_EQ(imu_timestamps(1), 25);
  EXPECT_EQ(imu_timestamps(2), 30);
  EXPECT_EQ(imu_timestamps(3), 40);
  EXPECT_EQ(imu_measurements.col(0)(0), 20.0);
  EXPECT_EQ(imu_measurements.col(1)(0), 25.0);
  EXPECT_EQ(imu_measurements.col(2)(0), 30.0);
  EXPECT_EQ(imu_measurements.col(3)(0), 40.0);

  // Test unaligned getter (only upper-interpolation).
  result = buffer.getImuDataInterpolatedUpperBorder(19, 21, &imu_timestamps,
                                                    &imu_measurements);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataAvailable);
  EXPECT_EQ(imu_timestamps.cols(), 2);
  EXPECT_EQ(imu_measurements.cols(), 2);
  EXPECT_EQ(imu_timestamps(0), 20);
  EXPECT_EQ(imu_timestamps(1), 21);
  EXPECT_EQ(imu_measurements.col(0)(0), 20.0);
  EXPECT_EQ(imu_measurements.col(1)(0), 21.0);

  // Fail: query out of upper bound.
  result = buffer.getImuDataInterpolatedUpperBorder(40, 51, &imu_timestamps,
                                                    &imu_measurements);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataNotYetAvailable);
  result = buffer.getImuDataInterpolatedUpperBorder(60, 61, &imu_timestamps,
                                                    &imu_measurements);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataNotYetAvailable);

  // Fail: query out of lower bound.
  result = buffer.getImuDataInterpolatedUpperBorder(9, 20, &imu_timestamps,
                                                    &imu_measurements);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataNeverAvailable);
  result = buffer.getImuDataInterpolatedUpperBorder(-20, -10, &imu_timestamps,
                                                    &imu_measurements);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataNeverAvailable);

  // Query in between two values: return too few measurements available.
  // even if asked to interpolate, there are no measurements in between
  // given timestamps.
  result = buffer.getImuDataInterpolatedUpperBorder(21, 24, &imu_timestamps,
                                                    &imu_measurements);
  EXPECT_EQ(result, dyno::ThreadsafeImuBuffer::QueryResult::
                        kTooFewMeasurementsAvailable);
  EXPECT_EQ(imu_timestamps.cols(), 0);
  EXPECT_EQ(imu_measurements.cols(), 0);

  // Query with only one value inside interval:
  // return the interpolated border value for upper border only,
  // and one measurement
  result = buffer.getImuDataInterpolatedUpperBorder(21, 29, &imu_timestamps,
                                                    &imu_measurements);
  EXPECT_EQ(result,
            dyno::ThreadsafeImuBuffer::QueryResult::kDataAvailable);
  EXPECT_EQ(imu_timestamps.cols(), 2);
  EXPECT_EQ(imu_measurements.cols(), 2);
  EXPECT_EQ(imu_timestamps(0), 25);
  EXPECT_EQ(imu_timestamps(1), 29);
  EXPECT_EQ(imu_measurements.col(0)(0), 25.0);
  EXPECT_EQ(imu_measurements.col(1)(0), 29.0);
}

//// Disabled because CppUnitTests does not support Expect DEATH.
// TEST(ThreadsafeImuBuffer, DeathOnAddDataNotIncreasingTimestamp) {
//  dyno::ThreadsafeImuBuffer buffer(-1);
//
//  ImuAccGyr imu_measurement;
//  imu_measurement.setRandom();
//
//  buffer.addMeasurement(0u, imu_measurement);
//  buffer.addMeasurement(10u, imu_measurement);
//  //EXPECT_DEATH(buffer.addMeasurement(9u, imu_measurement), "^");
//}

TEST(ThreadsafeImuBuffer, TestAddMeasurements) {
  const size_t kNumMeasurements = 10;
  dyno::ThreadsafeImuBuffer buffer(-1);

  // Create IMU measurements and fill buffer.
  Timestamps imu_timestamps_groundtruth(1, kNumMeasurements);
  ImuAccGyrs imu_measurements_groundtruth(6, kNumMeasurements);

  for (size_t idx = 0; idx < kNumMeasurements; ++idx) {
    Timestamp timestamp = static_cast<Timestamp>(idx * 4.3);
    ImuAccGyr imu_measurement;
    imu_measurement.setConstant(idx);
    imu_timestamps_groundtruth(idx) = timestamp;
    imu_measurements_groundtruth.col(idx) = imu_measurement;
  }
  buffer.addMeasurements(imu_timestamps_groundtruth,
                         imu_measurements_groundtruth);
}

}  // namespace dyno
```

## File: thread_safety/thread_safe_queue_tests.cc
```
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include <chrono>
#include <iostream>
#include <memory>
#include <string>
#include <thread>
#include <vector>

#include <glog/logging.h>
#include <gtest/gtest.h>

#include "dynosam/pipeline/ThreadSafeQueue.hpp"


void consumer(ThreadsafeQueue<std::string>& q,  // NOLINT
              const std::atomic_bool& kill_switch)
{
  while (!kill_switch)
  {
    std::string msg = "No msg!";
    if (q.popBlocking(msg))
    {
      VLOG(1) << "Got msg: " << msg << '\n';
    }
  }
  q.shutdown();
}

void producer_milliseconds(ThreadsafeQueue<std::string>& q,  // NOLINT
                           const std::atomic_bool& kill_switch, int delay)
{
  while (!kill_switch)
  {
    std::this_thread::sleep_for(std::chrono::milliseconds(delay));
    q.push("Hello World!");
  }
  q.shutdown();
}

void producer(ThreadsafeQueue<std::string>& q,  // NOLINT
              const std::atomic_bool& kill_switch)
{
  while (!kill_switch)
  {
    std::this_thread::sleep_for(std::chrono::milliseconds(200));
    q.push("Hello World!");
  }
  q.shutdown();
}

void blockingProducer(ThreadsafeQueue<std::string>& q,  // NOLINT
                      const std::atomic_bool& kill_switch)
{
  while (!kill_switch)
  {
    std::this_thread::sleep_for(std::chrono::milliseconds(20));
    q.pushBlockingIfFull("Hello World!", 5);
  }
  q.shutdown();
}

/* ************************************************************************* */
TEST(testThreadsafeQueue, popBlocking_by_reference)
{
  ThreadsafeQueue<std::string> q;
  std::thread p([&] {
    q.push("Hello World!");
    q.push("Hello World 2!");
  });
  std::string s;
  q.popBlocking(s);
  EXPECT_EQ(s, "Hello World!");
  q.popBlocking(s);
  EXPECT_EQ(s, "Hello World 2!");
  q.shutdown();
  EXPECT_FALSE(q.popBlocking(s));
  EXPECT_EQ(s, "Hello World 2!");

  // Leave some time for p to finish its work.
  std::this_thread::sleep_for(std::chrono::milliseconds(100));
  EXPECT_TRUE(p.joinable());
  p.join();
}

/* ************************************************************************* */
TEST(testThreadsafeQueue, popBlocking_by_shared_ptr)
{
  ThreadsafeQueue<std::string> q;
  std::thread p([&] {
    q.push("Hello World!");
    q.push("Hello World 2!");
  });
  std::shared_ptr<std::string> s = q.popBlocking();
  EXPECT_EQ(*s, "Hello World!");
  auto s2 = q.popBlocking();
  EXPECT_EQ(*s2, "Hello World 2!");
  q.shutdown();
  EXPECT_EQ(q.popBlocking(), nullptr);

  // Leave some time for p to finish its work.
  std::this_thread::sleep_for(std::chrono::milliseconds(100));
  EXPECT_TRUE(p.joinable());
  p.join();
}

/* ************************************************************************* */
TEST(testThreadsafeQueue, push)
{
  ThreadsafeQueue<std::string> q;
  std::thread p([&] {
    q.push(std::string("Hello World!"));
    std::string s = "Hello World 2!";
    q.push(s);
  });
  std::shared_ptr<std::string> s = q.popBlocking();
  EXPECT_EQ(*s, "Hello World!");
  auto s2 = q.popBlocking();
  EXPECT_EQ(*s2, "Hello World 2!");
  q.shutdown();
  EXPECT_EQ(q.popBlocking(), nullptr);

  // Leave some time for p to finish its work.
  std::this_thread::sleep_for(std::chrono::milliseconds(100));
  EXPECT_TRUE(p.joinable());
  p.join();
}

TEST(testThreadsafeQueue, pointerContainer)
{
  ThreadsafeQueue<std::shared_ptr<std::string>> q;
  std::thread p([&] {
    q.push(std::make_shared<std::string>("Hello World!"));
    auto s = std::make_shared<std::string>("Hello World 2!");
    q.push(s);
  });

  // wow this is gross...
  std::shared_ptr<std::shared_ptr<std::string>> s = q.popBlocking();
  EXPECT_EQ(**s, "Hello World!");
  auto s2 = q.popBlocking();
  EXPECT_EQ(**s2, "Hello World 2!");
  q.shutdown();
  EXPECT_EQ(q.popBlocking(), nullptr);

  // Leave some time for p to finish its work.
  std::this_thread::sleep_for(std::chrono::milliseconds(100));
  EXPECT_TRUE(p.joinable());
  p.join();
}

/* ************************************************************************* */
TEST(testThreadsafeQueue, pushBlockingIfFull)
{
  // Here we test only its nominal push behavior, not the blocking behavior
  ThreadsafeQueue<std::string> q;
  std::thread p([&] {
    q.pushBlockingIfFull(std::string("Hello World!"), 2);
    std::string s = "Hello World 2!";
    q.pushBlockingIfFull(s, 2);
  });
  std::shared_ptr<std::string> s = q.popBlocking();
  EXPECT_EQ(*s, "Hello World!");
  auto s2 = q.popBlocking();
  EXPECT_EQ(*s2, "Hello World 2!");
  q.shutdown();
  EXPECT_EQ(q.popBlocking(), nullptr);

  // Leave some time for p to finish its work.
  std::this_thread::sleep_for(std::chrono::milliseconds(100));
  EXPECT_TRUE(p.joinable());
  p.join();
}

/* ************************************************************************* */
TEST(testThreadsafeQueue, producer_consumer)
{
  ThreadsafeQueue<std::string> q;
  std::atomic_bool kill_switch(false);
  std::thread c(consumer, std::ref(q), std::ref(kill_switch));
  std::thread p(producer, std::ref(q), std::ref(kill_switch));

  std::this_thread::sleep_for(std::chrono::milliseconds(200));

  VLOG(1) << "Shutdown queue.\n";
  q.shutdown();

  std::this_thread::sleep_for(std::chrono::milliseconds(200));

  VLOG(1) << "Resume queue.\n";
  q.resume();

  std::this_thread::sleep_for(std::chrono::milliseconds(200));

  VLOG(1) << "Joining threads.\n";
  kill_switch = true;
  c.join();
  p.join();
  VLOG(1) << "Threads joined.\n";
}

/* ************************************************************************* */
TEST(testThreadsafeQueue, blocking_producer)
{
  ThreadsafeQueue<std::string> q;
  std::atomic_bool kill_switch(false);
  std::thread p(blockingProducer, std::ref(q), std::ref(kill_switch));

  // Give plenty of time to the blockingProducer to fill-in completely the queue
  // and be blocked.
  std::this_thread::sleep_for(std::chrono::milliseconds(200));

  VLOG(1) << "Shutdown queue.\n";
  q.shutdown();

  std::this_thread::sleep_for(std::chrono::milliseconds(200));

  VLOG(1) << "Resume queue.\n";
  q.resume();

  std::this_thread::sleep_for(std::chrono::milliseconds(200));

  VLOG(1) << "Joining thread.\n";
  q.shutdown();
  kill_switch = true;
  p.join();
  VLOG(1) << "Thread joined.\n";

  // Need to resume the queue to be able to pop...
  q.resume();

  // Expect non-empty queue.
  EXPECT_TRUE(!q.empty());
  size_t queue_size = 0;
  while (!q.empty())
  {
    std::string output;
    EXPECT_TRUE(q.pop(output));
    EXPECT_EQ(output, "Hello World!");
    ++queue_size;
  }
  // Expect the size of the queue to be the maximum size of the queue
  EXPECT_EQ(queue_size, 5u);
}

/* ************************************************************************* */
TEST(testThreadsafeQueue, stress_test)
{
  ThreadsafeQueue<std::string> q;
  std::atomic_bool kill_switch(false);
  std::vector<std::thread> cs;
  for (size_t i = 0; i < 10; i++)
  {
    // Create 10 consumers.
    cs.push_back(std::thread(consumer, std::ref(q), std::ref(kill_switch)));
  }
  std::vector<std::thread> ps;
  for (size_t i = 0; i < 10; i++)
  {
    // Create 10 producers.
    ps.push_back(std::thread(producer, std::ref(q), std::ref(kill_switch)));
  }
  std::vector<std::thread> blocking_ps;
  for (size_t i = 0; i < 10; i++)
  {
    // Create 10 producers.
    ps.push_back(std::thread(blockingProducer, std::ref(q), std::ref(kill_switch)));
  }

  std::this_thread::sleep_for(std::chrono::milliseconds(200));

  VLOG(1) << "Shutdown queue.\n";
  q.shutdown();

  std::this_thread::sleep_for(std::chrono::milliseconds(200));

  VLOG(1) << "Resume queue.\n";
  q.resume();

  std::this_thread::sleep_for(std::chrono::milliseconds(200));

  VLOG(1) << "Joining threads.\n";
  kill_switch = true;
  for (size_t i = 0; i < cs.size(); i++)
  {
    cs[i].join();
  }
  for (size_t i = 0; i < ps.size(); i++)
  {
    ps[i].join();
  }
  for (size_t i = 0; i < blocking_ps.size(); i++)
  {
    blocking_ps[i].join();
  }
  VLOG(1) << "Threads joined.\n";
}

TEST(testThreadsafeQueue, limited_queue_size)
{
  ThreadsafeQueue<std::string> q(5);
  std::atomic_bool kill_switch(false);
  std::thread p(producer_milliseconds, std::ref(q), std::ref(kill_switch), 10);

  // Give plenty of time to the producer_milliseconds to fill-in completely the queue
  // and be blocked.
  std::this_thread::sleep_for(std::chrono::milliseconds(200));
  // Expect the size of the queue to be the maximum size of the queue
  EXPECT_EQ(q.size(), 5u);

  q.shutdown();
  kill_switch = true;
  p.join();
}
```

## File: thread_safety/thread_safe_temporal_buffer_test.cc
```
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include <chrono>
#include <thread>

#include <gflags/gflags.h>
#include <glog/logging.h>
#include <gtest/gtest.h>

#include "dynosam/pipeline/ThreadSafeTemporalBuffer.hpp"

namespace dyno
{
struct TestData
{
  explicit TestData(Timestamp time) : timestamp(time)
  {
  }
  TestData() = default;

  Timestamp timestamp;
};

TEST(ThreadsafeTemporalBuffer, testEqualsOverloading)
{
  ThreadsafeTemporalBuffer<TestData> buffer_1(-1);
  ThreadsafeTemporalBuffer<TestData> buffer_2(100);

  EXPECT_EQ(buffer_1.bufferLength(), -1);
  EXPECT_EQ(buffer_2.bufferLength(), 100);

  buffer_1.addValue(0, TestData(0));
  buffer_1.addValue(1, TestData(1));
  buffer_1.addValue(2, TestData(2));

  EXPECT_EQ(buffer_1.size(), 3u);
  EXPECT_EQ(buffer_2.size(), 0u);

  buffer_2 = buffer_1;
  EXPECT_EQ(buffer_1.size(), 3u);
  EXPECT_EQ(buffer_2.size(), 3u);

  EXPECT_EQ(buffer_1.bufferLength(), -1);
  EXPECT_EQ(buffer_2.bufferLength(), -1);

  // change buffer1 and buffer 2 should not change
  buffer_1.addValue(3, TestData(3));
  EXPECT_EQ(buffer_1.size(), 4u);
  EXPECT_EQ(buffer_2.size(), 3u);
}

TEST(ThreadsafeTemporalBuffer, testLagSizeInfinite)
{
  ThreadsafeTemporalBuffer<TestData> buffer_(-1);
  buffer_.addValue(0, TestData(0));
  EXPECT_EQ(buffer_.size(), 1u);
  buffer_.addValue(1271839713, TestData(1271839713));
  EXPECT_EQ(buffer_.size(), 2u);
  buffer_.addValue(10000, TestData(10000));
  EXPECT_EQ(buffer_.size(), 3u);
}

TEST(ThreadsafeTemporalBuffer, testLagSize10)
{
  ThreadsafeTemporalBuffer<TestData> buffer_(10);
  buffer_.addValue(0, TestData(0));

  const double kMaxDelta = 0.01;
  TestData retrieved_item;
  // check that we can retrieve this value now so wer can be sure we cannot retrieve it later
  EXPECT_TRUE(buffer_.getNearestValueToTime(0, kMaxDelta, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 0);

  EXPECT_EQ(buffer_.size(), 1u);
  buffer_.addValue(4.3, TestData(4.3));
  EXPECT_EQ(buffer_.size(), 2u);
  buffer_.addValue(9.9, TestData(9.9));
  EXPECT_EQ(buffer_.size(), 3u);

  // add time past the buffer -> this data should stay in the buffer but push out the 0th value
  buffer_.addValue(10.3, TestData(10.3));
  EXPECT_EQ(buffer_.size(), 3u);
  EXPECT_FALSE(buffer_.getNearestValueToTime(0, kMaxDelta, &retrieved_item));
  EXPECT_TRUE(buffer_.getNearestValueToTime(10.2999, kMaxDelta, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 10.3);
}

class ThreadsafeTemporalBufferFixture : public ::testing::Test
{
public:
  ThreadsafeTemporalBufferFixture() : buffer_(kBufferLengthS)
  {
  }

protected:
  virtual void SetUp()
  {
  }
  virtual void TearDown()
  {
  }  //
  void addValue(const TestData& data)
  {
    buffer_.addValue(data.timestamp, data);
  }

  static constexpr Timestamp kBufferLengthS = 100;
  ThreadsafeTemporalBuffer<TestData> buffer_;
};

TEST_F(ThreadsafeTemporalBufferFixture, SizeEmptyClearWork)
{
  EXPECT_TRUE(buffer_.empty());
  EXPECT_EQ(buffer_.size(), 0u);

  addValue(TestData(10));
  addValue(TestData(20));
  EXPECT_TRUE(!buffer_.empty());
  EXPECT_EQ(buffer_.size(), 2u);

  buffer_.clear();
  EXPECT_TRUE(buffer_.empty());
  EXPECT_EQ(buffer_.size(), 0u);
}

TEST_F(ThreadsafeTemporalBufferFixture, GetValueAtTimeWorks)
{
  addValue(TestData(3.1));
  addValue(TestData(10));
  addValue(TestData(0.004));
  addValue(TestData(40.234));

  TestData retrieved_item;
  EXPECT_TRUE(buffer_.getValueAtTime(3.1, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 3.1);

  EXPECT_TRUE(buffer_.getValueAtTime(0.004, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 0.004);

  EXPECT_TRUE(!buffer_.getValueAtTime(40, &retrieved_item));

  EXPECT_TRUE(buffer_.getValueAtTime(3.1, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 3.1);

  EXPECT_TRUE(buffer_.getValueAtTime(40.234, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 40.234);
}

TEST_F(ThreadsafeTemporalBufferFixture, GetNearestValueToTimeWorks)
{
  addValue(TestData(3.00));
  addValue(TestData(1.004));
  addValue(TestData(6.32));
  addValue(TestData(34));

  TestData retrieved_item;
  EXPECT_TRUE(buffer_.getNearestValueToTime(3, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 3);

  EXPECT_TRUE(buffer_.getNearestValueToTime(0, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 1.004);

  EXPECT_TRUE(buffer_.getNearestValueToTime(16, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 6.32);

  EXPECT_TRUE(buffer_.getNearestValueToTime(34.432421, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 34);

  EXPECT_TRUE(buffer_.getNearestValueToTime(32, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 34);

  EXPECT_TRUE(buffer_.getNearestValueToTime(1232, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 34);
}

TEST_F(ThreadsafeTemporalBufferFixture, GetNearestValueToTimeMaxDeltaWorks)
{
  addValue(TestData(30));
  addValue(TestData(10));
  addValue(TestData(20));

  const double kMaxDelta = 5;

  TestData retrieved_item;
  EXPECT_TRUE(buffer_.getNearestValueToTime(10, kMaxDelta, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 10);

  EXPECT_TRUE(!buffer_.getNearestValueToTime(0, kMaxDelta, &retrieved_item));

  EXPECT_TRUE(buffer_.getNearestValueToTime(9, kMaxDelta, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 10);

  EXPECT_TRUE(buffer_.getNearestValueToTime(16, kMaxDelta, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 20);

  EXPECT_TRUE(buffer_.getNearestValueToTime(26, kMaxDelta, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 30);

  EXPECT_TRUE(buffer_.getNearestValueToTime(32, kMaxDelta, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 30);

  EXPECT_TRUE(!buffer_.getNearestValueToTime(36, kMaxDelta, &retrieved_item));

  buffer_.clear();
  addValue(TestData(10));
  addValue(TestData(20));

  EXPECT_TRUE(buffer_.getNearestValueToTime(9, kMaxDelta, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 10);

  EXPECT_TRUE(buffer_.getNearestValueToTime(12, kMaxDelta, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 10);

  EXPECT_TRUE(buffer_.getNearestValueToTime(16, kMaxDelta, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 20);

  EXPECT_TRUE(buffer_.getNearestValueToTime(22, kMaxDelta, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 20);

  buffer_.clear();
  addValue(TestData(10));

  EXPECT_TRUE(buffer_.getNearestValueToTime(6, kMaxDelta, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 10);

  EXPECT_TRUE(buffer_.getNearestValueToTime(14, kMaxDelta, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 10);

  EXPECT_TRUE(!buffer_.getNearestValueToTime(16, kMaxDelta, &retrieved_item));
}

TEST_F(ThreadsafeTemporalBufferFixture, GetNearestValueToTimeMaxDeltaSmallWorks)
{
  addValue(TestData(0.05));
  addValue(TestData(0.06));
  addValue(TestData(0.057));

  const Timestamp kMaxDelta = 0.005;
  TestData retrieved_item;

  EXPECT_TRUE(buffer_.getNearestValueToTime(0.05, kMaxDelta, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 0.05);

  EXPECT_FALSE(buffer_.getNearestValueToTime(0.04, kMaxDelta, &retrieved_item));  //!

  EXPECT_TRUE(buffer_.getNearestValueToTime(0.058, kMaxDelta, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 0.057);

  EXPECT_FALSE(buffer_.getNearestValueToTime(0.066, kMaxDelta, &retrieved_item));  //!

  EXPECT_TRUE(buffer_.getNearestValueToTime(0.063, kMaxDelta, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 0.06);

  Timestamp retrieved_timestamp;
  EXPECT_TRUE(buffer_.getNearestValueToTime(0.063, kMaxDelta, &retrieved_item, &retrieved_timestamp));
  EXPECT_EQ(retrieved_timestamp, 0.06);

  buffer_.clear();
}

TEST_F(ThreadsafeTemporalBufferFixture, GetValueAtOrBeforeTimeWorks)
{
  addValue(TestData(30));
  addValue(TestData(10));
  addValue(TestData(20));
  addValue(TestData(40));

  TestData retrieved_item;
  Timestamp timestamp;

  EXPECT_TRUE(buffer_.getValueAtOrBeforeTime(40, &timestamp, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 40);
  EXPECT_EQ(timestamp, 40);

  EXPECT_TRUE(buffer_.getValueAtOrBeforeTime(50, &timestamp, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 40);
  EXPECT_EQ(timestamp, 40);

  EXPECT_TRUE(buffer_.getValueAtOrBeforeTime(15, &timestamp, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 10);
  EXPECT_EQ(timestamp, 10);

  EXPECT_TRUE(buffer_.getValueAtOrBeforeTime(10, &timestamp, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 10);
  EXPECT_EQ(timestamp, 10);

  EXPECT_TRUE(!buffer_.getValueAtOrBeforeTime(5, &timestamp, &retrieved_item));
}

TEST_F(ThreadsafeTemporalBufferFixture, GetValueAtOrAfterTimeWorks)
{
  addValue(TestData(30));
  addValue(TestData(10));
  addValue(TestData(20));
  addValue(TestData(40));

  TestData retrieved_item;
  Timestamp timestamp;

  EXPECT_TRUE(buffer_.getValueAtOrAfterTime(10, &timestamp, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 10);
  EXPECT_EQ(timestamp, 10);

  EXPECT_TRUE(buffer_.getValueAtOrAfterTime(5, &timestamp, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 10);
  EXPECT_EQ(timestamp, 10);

  EXPECT_TRUE(buffer_.getValueAtOrAfterTime(35, &timestamp, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 40);
  EXPECT_EQ(timestamp, 40);

  EXPECT_TRUE(buffer_.getValueAtOrAfterTime(40, &timestamp, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 40);
  EXPECT_EQ(timestamp, 40);

  EXPECT_TRUE(!buffer_.getValueAtOrAfterTime(45, &timestamp, &retrieved_item));
}

TEST_F(ThreadsafeTemporalBufferFixture, GetOldestNewestValueWork)
{
  TestData retrieved_item;
  EXPECT_TRUE(!buffer_.getOldestValue(&retrieved_item));
  EXPECT_TRUE(!buffer_.getNewestValue(&retrieved_item));

  addValue(TestData(30.4));
  addValue(TestData(10.122));
  addValue(TestData(20.6));
  addValue(TestData(40.8));

  Timestamp timestamp = 0;
  EXPECT_TRUE(buffer_.getOldestValue(&retrieved_item, &timestamp));
  EXPECT_DOUBLE_EQ(retrieved_item.timestamp, 10.122);
  EXPECT_DOUBLE_EQ(timestamp, 10.122);

  EXPECT_TRUE(buffer_.getNewestValue(&retrieved_item, &timestamp));
  EXPECT_DOUBLE_EQ(timestamp, 40.8);
  EXPECT_DOUBLE_EQ(retrieved_item.timestamp, 40.8);
}

TEST_F(ThreadsafeTemporalBufferFixture, GetValuesBetweenTimesWorks)
{
  addValue(TestData(10));
  addValue(TestData(20));
  addValue(TestData(30));
  addValue(TestData(40));
  addValue(TestData(50));

  // Test aligned borders.
  /// When the user does not ask for the lower bound.
  /// Implicitly also checks that it is default behaviour.
  std::vector<TestData> values;
  EXPECT_TRUE(buffer_.getValuesBetweenTimes(10, 50, &values));
  EXPECT_EQ(values.size(), 3u);
  EXPECT_EQ(values[0].timestamp, 20);
  EXPECT_EQ(values[1].timestamp, 30);
  EXPECT_EQ(values[2].timestamp, 40);

  // Test aligned borders.
  /// When the user does ask for the lower bound.
  EXPECT_TRUE(buffer_.getValuesBetweenTimes(10, 50, &values, true));
  EXPECT_EQ(values.size(), 4u);
  EXPECT_EQ(values[0].timestamp, 10);
  EXPECT_EQ(values[1].timestamp, 20);
  EXPECT_EQ(values[2].timestamp, 30);
  EXPECT_EQ(values[3].timestamp, 40);

  // Test unaligned borders.
  /// When the user does not ask for the lower bound.
  /// Implicitly also checks that it is default behaviour.
  EXPECT_TRUE(buffer_.getValuesBetweenTimes(15, 45, &values));
  EXPECT_EQ(values.size(), 3u);
  EXPECT_EQ(values[0].timestamp, 20);
  EXPECT_EQ(values[1].timestamp, 30);
  EXPECT_EQ(values[2].timestamp, 40);

  // Test unaligned borders.
  /// When the user does ask for the lower bound.
  EXPECT_TRUE(buffer_.getValuesBetweenTimes(15, 45, &values));
  EXPECT_EQ(values.size(), 3u);
  EXPECT_EQ(values[0].timestamp, 20);
  EXPECT_EQ(values[1].timestamp, 30);
  EXPECT_EQ(values[2].timestamp, 40);

  // Test unsuccessful queries.
  // Lower border oob.
  EXPECT_TRUE(!buffer_.getValuesBetweenTimes(5, 45, &values));
  EXPECT_TRUE(!buffer_.getValuesBetweenTimes(5, 45, &values, true));
  // Higher border oob.
  EXPECT_TRUE(!buffer_.getValuesBetweenTimes(30, 55, &values));
  EXPECT_TRUE(!buffer_.getValuesBetweenTimes(30, 55, &values, true));
  EXPECT_TRUE(values.empty());

  // The method should check-fail when the buffer is empty.
  buffer_.clear();
  EXPECT_TRUE(!buffer_.getValuesBetweenTimes(10, 50, &values));
  EXPECT_TRUE(!buffer_.getValuesBetweenTimes(10, 50, &values, true));
  // EXPECT_DEATH(buffer_.getValuesBetweenTimes(40, 30, &values), "^");
}

TEST_F(ThreadsafeTemporalBufferFixture, MaintaingBufferLengthWorks)
{
  addValue(TestData(0));
  addValue(TestData(50));
  addValue(TestData(100));
  EXPECT_EQ(buffer_.size(), 3u);

  addValue(TestData(150));
  EXPECT_EQ(buffer_.size(), 3u);

  TestData retrieved_item;
  EXPECT_TRUE(buffer_.getOldestValue(&retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 50);

  EXPECT_TRUE(buffer_.getNewestValue(&retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 150);
}

TEST_F(ThreadsafeTemporalBufferFixture, DeltetingValuesAtTimestampsWork)
{
  addValue(TestData(12.4));
  addValue(TestData(0.001));
  addValue(TestData(56));
  addValue(TestData(21));

  TestData retrieved_item;
  Timestamp timestamp;

  EXPECT_TRUE(buffer_.getValueAtOrAfterTime(12.4, &timestamp, &retrieved_item));
  EXPECT_EQ(buffer_.size(), 4u);
  EXPECT_TRUE(buffer_.deleteValueAtTime(timestamp));
  EXPECT_TRUE(buffer_.getValueAtOrBeforeTime(12.4, &timestamp, &retrieved_item));
  EXPECT_EQ(retrieved_item.timestamp, 0.001);
  EXPECT_FALSE(buffer_.getNearestValueToTime(12.2, 1, &retrieved_item));
  EXPECT_EQ(buffer_.size(), 3u);

  // delete value using non-exact timestamp
  Timestamp stored_timestamp;
  EXPECT_TRUE(buffer_.getNearestValueToTime(59, 4, &retrieved_item, &stored_timestamp));
  EXPECT_EQ(retrieved_item.timestamp, 56);
  EXPECT_EQ(stored_timestamp, 56);
  EXPECT_TRUE(buffer_.deleteValueAtTime(stored_timestamp));
  EXPECT_EQ(buffer_.size(), 2u);
}

}  // namespace dyno
```

## File: vision_map/test_map.cc
```
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include <glog/logging.h>
#include <gtest/gtest.h>

#include <exception>

#include "dynosam/common/Map.hpp"
#include "dynosam/common/Types.hpp"
#include "dynosam/frontend/vision/Frame.hpp"
#include "internal/helpers.hpp"

using namespace dyno;

TEST(Map, basicAddOnlyStatic) {
  //   GenericTrackedStatusVector<VisualMeasurementStatus<Keypoint>>
  //   measurements;

  StatusKeypointVector measurements;

  TrackletIds expected_tracklets;
  // 10 measurements with unique tracklets at frame 0
  for (size_t i = 0; i < 10; i++) {
    measurements.push_back(
        dyno_testing::makeStatusKeypointMeasurement(i, background_label, 0));
    expected_tracklets.push_back(i);
  }

  Map2d::Ptr map = Map2d::create();

  map->updateObservations(measurements);

  EXPECT_TRUE(map->frameExists(0));
  EXPECT_FALSE(map->frameExists(1));

  EXPECT_TRUE(map->landmarkExists(0));
  EXPECT_TRUE(map->landmarkExists(9));
  EXPECT_FALSE(map->landmarkExists(10));

  EXPECT_EQ(map->getStaticTrackletsByFrame(0), expected_tracklets);

  // expected tracklets in frame 0
  TrackletIds expected_tracklets_f0 = expected_tracklets;

  TrackletIds expected_tracklets_f1;
  // add another 5 points at frame 1
  measurements.clear();
  for (size_t i = 0; i < 5; i++) {
    measurements.push_back(
        dyno_testing::makeStatusKeypointMeasurement(i, background_label, 1));

    expected_tracklets.push_back(i);
    expected_tracklets_f1.push_back(i);
  }

  // apply update
  map->updateObservations(measurements);

  EXPECT_EQ(map->getStaticTrackletsByFrame(0), expected_tracklets_f0);
  EXPECT_EQ(map->getStaticTrackletsByFrame(1), expected_tracklets_f1);

  // check for frames in some landmarks
  // should be seen in frames 0 and 1
  LandmarkNode<Keypoint>::Ptr lmk1 = map->getLandmark(0);
  std::vector<FrameId> lmk_1_seen_frames =
      lmk1->getSeenFrames().collectIds<FrameId>();
  std::vector<FrameId> lmk_1_seen_frames_expected = {0, 1};
  EXPECT_EQ(lmk_1_seen_frames, lmk_1_seen_frames_expected);

  // should be seen in frames 0
  LandmarkNode<Keypoint>::Ptr lmk6 = map->getLandmark(6);
  std::vector<FrameId> lmk_6_seen_frames =
      lmk6->getSeenFrames().collectIds<FrameId>();
  std::vector<FrameId> lmk_6_seen_frames_expected = {0};
  EXPECT_EQ(lmk_6_seen_frames, lmk_6_seen_frames_expected);

  // check that the frames here are the ones in the map
  EXPECT_EQ(map->getFrame(lmk_1_seen_frames.at(0)),
            map->getFrame(lmk_6_seen_frames.at(0)));

  // finally check that there are no objects
  EXPECT_EQ(map->getFrame(lmk_1_seen_frames.at(0))->objects_seen.size(), 0);
  EXPECT_EQ(map->numObjectsSeen(), 0u);
}

TEST(Map, setStaticOrdering) {
  // add frames out of order
  Map2d::Ptr map = Map2d::create();

  StatusKeypointVector measurements;
  // frame 0
  measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(1, 0, 0));
  // frame 2
  measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(1, 0, 2));
  // frame 1
  measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(1, 0, 1));
  // frame 3
  measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(1, 0, 3));
  map->updateObservations(measurements);

  LandmarkNode<Keypoint>::Ptr lmk = map->getLandmark(1);
  EXPECT_TRUE(lmk != nullptr);

  FrameIds expected_frame_ids = {0, 1, 2, 3};
  EXPECT_EQ(lmk->getSeenFrames().collectIds<FrameId>(), expected_frame_ids);

  EXPECT_EQ(lmk->getSeenFrames().getFirstIndex(), 0u);
  EXPECT_EQ(lmk->getSeenFrames().getLastIndex(), 3u);
}

TEST(Map, basicObjectAdd) {
  Map2d::Ptr map = Map2d::create();
  StatusKeypointVector measurements;
  // add two dynamic points on object 1 and frames 0 and 1
  // tracklet 0, object 1 frame 0
  measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(0, 1, 0));
  // tracklet 0, object 1 frame 1
  measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(0, 1, 1));

  map->updateObservations(measurements);
  EXPECT_EQ(map->numObjectsSeen(), 1u);
  EXPECT_TRUE(map->objectExists(1));

  ObjectNode2d::Ptr object1 = map->getObject(1);
  // object 1 should have 1 point seen at frames 0 and 1
  EXPECT_EQ(object1->dynamic_landmarks.collectIds<TrackletId>(),
            TrackletIds{0});
  EXPECT_EQ(object1->dynamic_landmarks.size(), 1u);

  // now check that the frames also have these measurements
  FrameNode2d::Ptr frame_0 = map->getFrame(0);
  FrameNode2d::Ptr frame_1 = map->getFrame(1);
  EXPECT_TRUE(frame_0 != nullptr);
  EXPECT_TRUE(frame_1 != nullptr);

  EXPECT_EQ(frame_0->dynamic_landmarks.collectIds<TrackletId>(),
            TrackletIds{0});
  EXPECT_EQ(frame_1->dynamic_landmarks.collectIds<TrackletId>(),
            TrackletIds{0});

  // sanity check that there are no static points
  EXPECT_EQ(frame_0->static_landmarks.size(), 0u);
  EXPECT_EQ(frame_1->static_landmarks.size(), 0u);

  // check object id and seen frames
  LandmarkNode2d::Ptr lmk_0 = map->getLandmark(0);
  EXPECT_EQ(lmk_0->object_id, 1);  // object Id 1;
  EXPECT_EQ(lmk_0->getSeenFrames().collectIds<FrameId>(),
            FrameIds({0, 1}));  // seen frames

  // finally check that the landmark referred to by the frames are the same one
  // as getLandmark(0) this also implicitly tests FastMapNodeSet::find(index)
  auto frame_0_dynamic_lmks = frame_0->dynamic_landmarks;
  auto frame_1_dynamic_lmks = frame_1->dynamic_landmarks;

  // look from the lmk with id 0
  auto lmk_itr_frame_0 = frame_0_dynamic_lmks.find(0);
  auto lmk_itr_frame_1 = frame_1_dynamic_lmks.find(0);
  // should not be at the end as we have this landmark
  EXPECT_FALSE(lmk_itr_frame_0 == frame_0_dynamic_lmks.end());
  EXPECT_FALSE(lmk_itr_frame_1 == frame_1_dynamic_lmks.end());

  // check the lmk is the one we got from the map
  EXPECT_EQ(lmk_0, *lmk_itr_frame_0);
  EXPECT_EQ(lmk_0, *lmk_itr_frame_1);
}

TEST(Map, framesSeenDuplicates) {
  Map2d::Ptr map = Map2d::create();
  LandmarkNode2d::Ptr landmark_node =
      std::make_shared<LandmarkNode2d>(map->getptr());
  landmark_node->tracklet_id = 0;
  landmark_node->object_id = 0;

  EXPECT_EQ(landmark_node->numObservations(), 0);

  FrameNode2d::Ptr frame_node = std::make_shared<FrameNode2d>(map->getptr());
  frame_node->frame_id = 0;

  landmark_node->add(frame_node, Keypoint());

  EXPECT_EQ(landmark_node->numObservations(), 1);
  EXPECT_EQ(*landmark_node->getSeenFrames().begin(), frame_node);
  EXPECT_EQ(landmark_node->getMeasurements().size(), 1);

  // now add the same frame again
  EXPECT_THROW({ landmark_node->add(frame_node, Keypoint()); },
               DynosamException);
}

TEST(Map, objectSeenFrames) {
  Map2d::Ptr map = Map2d::create();
  StatusKeypointVector measurements;

  // add 2 objects
  // object 1 seen at frames 0 and 1
  // tracklet 0, object 1 frame 0
  measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(0, 1, 0));
  // tracklet 0, object 1 frame 1
  measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(0, 1, 1));
  // tracklet 0 has two observations

  // object 2 seen at frames 1 and 2
  // tracklet 1, object 2 frame 1
  // tracklet 1 has 1 observations
  measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(1, 2, 1));
  // tracklet 2, object 2 frame 2
  // tracklet 2 has 1 observations
  measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(2, 2, 2));

  // object 3 seen at frames 0, 1, 2
  // tracklet 3, object 3 frame 0
  measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(3, 3, 0));
  // tracklet 3, object 3 frame 1
  measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(3, 3, 1));
  // tracklet 3, object 3 frame 2
  measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(3, 3, 2));
  // tracklet 3 has 3 observations

  map->updateObservations(measurements);
  EXPECT_EQ(map->numObjectsSeen(), 3u);

  ObjectNode2d::Ptr object1 = map->getObject(1);
  ObjectNode2d::Ptr object2 = map->getObject(2);
  ObjectNode2d::Ptr object3 = map->getObject(3);

  FrameNodePtrSet<Keypoint> expected_frame_set_object1;
  expected_frame_set_object1.insert(CHECK_NOTNULL(map->getFrame(0)));
  expected_frame_set_object1.insert(CHECK_NOTNULL(map->getFrame(1)));

  FrameNodePtrSet<Keypoint> expected_frame_set_object2;
  expected_frame_set_object2.insert(CHECK_NOTNULL(map->getFrame(1)));
  expected_frame_set_object2.insert(CHECK_NOTNULL(map->getFrame(2)));

  FrameNodePtrSet<Keypoint> expected_frame_set_object3;
  expected_frame_set_object3.insert(CHECK_NOTNULL(map->getFrame(0)));
  expected_frame_set_object3.insert(CHECK_NOTNULL(map->getFrame(1)));
  expected_frame_set_object3.insert(CHECK_NOTNULL(map->getFrame(2)));

  EXPECT_EQ(object1->getSeenFrames(), expected_frame_set_object1);
  EXPECT_EQ(object2->getSeenFrames(), expected_frame_set_object2);
  EXPECT_EQ(object3->getSeenFrames(), expected_frame_set_object3);

  // frame 0 has seen object 1 and 3 -the reverse of the above testt
  EXPECT_EQ(map->getFrame(0)->objects_seen,
            ObjectNodePtrSet<Keypoint>({object1, object3}));
  // frame 1 has seen object 1, 2 and 3
  EXPECT_EQ(map->getFrame(1)->objects_seen,
            ObjectNodePtrSet<Keypoint>({object1, object3, object2}));
  // frame 2 has seen object 2 and 3
  EXPECT_EQ(map->getFrame(2)->objects_seen,
            ObjectNodePtrSet<Keypoint>({object2, object3}));

  // check object observation functions
  auto frame0 = map->getFrame(0);
  auto frame1 = map->getFrame(1);
  auto frame2 = map->getFrame(2);
  // check object observed frame 0
  EXPECT_TRUE(frame0->objectObserved(1));
  EXPECT_TRUE(frame0->objectObserved(3));
  EXPECT_FALSE(frame0->objectObserved(2));

  // check object observed frame 1 (all)
  EXPECT_TRUE(frame1->objectObserved(1));
  EXPECT_TRUE(frame1->objectObserved(3));
  EXPECT_TRUE(frame1->objectObserved(2));
  // check object observed frame 2
  EXPECT_TRUE(frame2->objectObserved(2));
  EXPECT_TRUE(frame2->objectObserved(3));
  EXPECT_FALSE(frame2->objectObserved(1));

  // check observed in previous (in frame 0, there is no previous so all
  // false!!)
  EXPECT_FALSE(frame0->objectObservedInPrevious(1));
  EXPECT_FALSE(frame0->objectObservedInPrevious(3));

  // both object1 and 3 appear in frame 0 but not object 2
  EXPECT_TRUE(frame1->objectObservedInPrevious(1));
  EXPECT_TRUE(frame1->objectObservedInPrevious(3));
  EXPECT_FALSE(frame1->objectObservedInPrevious(2));

  // all objects are observed at frame 1
  EXPECT_TRUE(frame2->objectObservedInPrevious(1));
  EXPECT_TRUE(frame2->objectObservedInPrevious(3));
  EXPECT_TRUE(frame2->objectObservedInPrevious(2));

  // check objectMotionExpected (i.e objects are observed at both frames)
  EXPECT_FALSE(frame0->objectMotionExpected(1));
  EXPECT_FALSE(frame0->objectMotionExpected(3));

  // object 1 and 3 seen at frames 0 and 1, but not object 2
  EXPECT_TRUE(frame1->objectMotionExpected(1));
  EXPECT_TRUE(frame1->objectMotionExpected(3));
  EXPECT_FALSE(frame1->objectMotionExpected(2));
  // object 2 and 3 seen at frames 1 and 2, but not object 1
  EXPECT_FALSE(frame2->objectMotionExpected(1));
  EXPECT_TRUE(frame2->objectMotionExpected(3));
  EXPECT_TRUE(frame2->objectMotionExpected(2));
}

TEST(Map, getLandmarksSeenAtFrame) {
  Map2d::Ptr map = Map2d::create();
  StatusKeypointVector measurements;

  // add 2 objects
  // object 1 seen at frames 0 and 1
  // tracklet 0, object 1 frame 0
  measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(0, 1, 0));
  // tracklet 0, object 1 frame 1
  measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(0, 1, 1));

  // object 2 seen at frames 1 and 2
  // tracklet 1, object 2 frame 1
  measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(1, 2, 1));
  // tracklet 2, object 2 frame 1
  measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(2, 2, 1));

  // object 3 seen at frames 0, 1, 2
  // tracklet 3, object 3 frame 0
  measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(3, 3, 0));
  // tracklet 3, object 3 frame 1
  measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(3, 3, 1));
  // tracklet 3, object 3 frame 1
  measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(4, 3, 1));
  // tracklet 3 has 3 observations

  map->updateObservations(measurements);
  EXPECT_EQ(map->numObjectsSeen(), 3u);

  ObjectNode2d::Ptr object1 = map->getObject(1);
  ObjectNode2d::Ptr object2 = map->getObject(2);
  ObjectNode2d::Ptr object3 = map->getObject(3);

  LandmarkNodePtrSet<Keypoint> expected_lmk_set_object1_frame0;
  expected_lmk_set_object1_frame0.insert(CHECK_NOTNULL(map->getLandmark(0)));

  LandmarkNodePtrSet<Keypoint> expected_lmk_set_object1_frame1;
  expected_lmk_set_object1_frame1.insert(CHECK_NOTNULL(map->getLandmark(0)));

  LandmarkNodePtrSet<Keypoint> expected_lmk_set_object2_frame1;
  expected_lmk_set_object2_frame1.insert(CHECK_NOTNULL(map->getLandmark(1)));
  expected_lmk_set_object2_frame1.insert(CHECK_NOTNULL(map->getLandmark(2)));

  LandmarkNodePtrSet<Keypoint> expected_lmk_set_object3_frame0;
  expected_lmk_set_object3_frame0.insert(CHECK_NOTNULL(map->getLandmark(3)));

  LandmarkNodePtrSet<Keypoint> expected_lmk_set_object3_frame1;
  expected_lmk_set_object3_frame1.insert(CHECK_NOTNULL(map->getLandmark(3)));
  expected_lmk_set_object3_frame1.insert(CHECK_NOTNULL(map->getLandmark(4)));

  EXPECT_EQ(object1->getLandmarksSeenAtFrame(0),
            expected_lmk_set_object1_frame0);
  EXPECT_EQ(object1->getLandmarksSeenAtFrame(1),
            expected_lmk_set_object1_frame1);
  EXPECT_EQ(object1->getLandmarksSeenAtFrame(2),
            LandmarkNodePtrSet<Keypoint>{});
  EXPECT_EQ(object2->getLandmarksSeenAtFrame(1),
            expected_lmk_set_object2_frame1);
  EXPECT_EQ(object3->getLandmarksSeenAtFrame(0),
            expected_lmk_set_object3_frame0);
  EXPECT_EQ(object3->getLandmarksSeenAtFrame(1),
            expected_lmk_set_object3_frame1);
}

// TODO: bring back!!
//  TEST(Map, testSimpleEstimateAccessWithPose) {
//      Map2d::Ptr map = Map2d::create();
//      StatusKeypointMeasurements measurements;

//     //TODO:frame cannot be 0? what is invalid frame then?
//     EXPECT_EQ(map->lastEstimateUpdate(), 0u);

//     //tracklet 0, static, frame
//     measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(0, 0,
//     0)); map->updateObservations(measurements);

//     auto frame_0 = map->getFrame(0);
//     auto pose_0_query = frame_0->getPoseEstimate();
//     EXPECT_FALSE(pose_0_query);
//     EXPECT_FALSE(pose_0_query.isValid());

//     gtsam::Values estimate;

//     //some random pose
//     gtsam::Rot3 R = gtsam::Rot3::Rodrigues(0.3,0.4,-0.5);
//     gtsam::Point3 t(3.5,-8.2,4.2);
//     gtsam::Pose3 pose_0_actual(R,t);
//     gtsam::Key pose_0_key = CameraPoseSymbol(0);
//     estimate.insert(pose_0_key, pose_0_actual);

//     map->updateEstimates(estimate, gtsam::NonlinearFactorGraph{}, 0);
//     pose_0_query = frame_0->getPoseEstimate();

//     EXPECT_TRUE(pose_0_query);
//     EXPECT_TRUE(pose_0_query.isValid());
//     EXPECT_TRUE(gtsam::assert_equal(pose_0_query.get(), pose_0_actual));
//     EXPECT_EQ(pose_0_query.key_, pose_0_key);

// }

// TEST(Map, testEstimateWithStaticAndDynamicViaLmk) {
//     Map2d::Ptr map = Map2d::create();
//     StatusKeypointMeasurements measurements;

//     //tracklet 0, static, frame 0
//     measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(0, 0,
//     0));
//     //tracklet 0, static, frame 1
//     measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(0, 0,
//     1));
//     //static points should return the same estimate

//     //tracklet 1, dynamic, frame 0
//     measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(1, 1,
//     0));
//     //tracklet 1, dynamic, frame 1
//     measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(1, 1,
//     1));
//     //dynamic points should return different estimates

//     gtsam::Point3 static_lmk(0, 0, 0);
//     gtsam::Point3 dynamic_lmk_1(0, 0, 1);
//     gtsam::Point3 dynamic_lmk_2(0, 0, 2);

//     gtsam::Values estimate;
//     estimate.insert(StaticLandmarkSymbol(0), static_lmk);
//     estimate.insert(DynamicLandmarkSymbol(0, 1), dynamic_lmk_1);
//     estimate.insert(DynamicLandmarkSymbol(1, 1), dynamic_lmk_2);

//     map->updateObservations(measurements);
//     map->updateEstimates(estimate,gtsam::NonlinearFactorGraph{}, 0 );

//     LandmarkNode2d::Ptr static_lmk_node = map->getLandmark(0);
//     EXPECT_TRUE(static_lmk_node->isStatic());
//     auto estimate_query = static_lmk_node->getStaticLandmarkEstimate();
//     EXPECT_TRUE(estimate_query);
//     EXPECT_TRUE(estimate_query.isValid());
//     EXPECT_TRUE(gtsam::assert_equal(estimate_query.get(), static_lmk));

//     LandmarkNode2d::Ptr dynamic_lmk_1_node = map->getLandmark(1);
//     EXPECT_FALSE(dynamic_lmk_1_node->isStatic());
//     estimate_query = dynamic_lmk_1_node->getDynamicLandmarkEstimate(0);
//     EXPECT_TRUE(estimate_query);
//     EXPECT_TRUE(estimate_query.isValid());
//     EXPECT_TRUE(gtsam::assert_equal(estimate_query.get(), dynamic_lmk_1));

//     LandmarkNode2d::Ptr dynamic_lmk_2_node = map->getLandmark(1);
//     EXPECT_FALSE(dynamic_lmk_2_node->isStatic());
//     estimate_query = dynamic_lmk_2_node->getDynamicLandmarkEstimate(1);
//     EXPECT_TRUE(estimate_query);
//     EXPECT_TRUE(estimate_query.isValid());
//     EXPECT_TRUE(gtsam::assert_equal(estimate_query.get(), dynamic_lmk_2));

// }

// TEST(Map, testEstimateWithStaticAndDynamicViaFrame) {

// }

// TEST(Map, testObjectNodeWithOnlyMotion) {
//     Map2d::Ptr map = Map2d::create();

//     //create dynamic observations for object 1 at frames 0 and 1
//     StatusKeypointMeasurements measurements;
//      //tracklet 1, dynamic, frame 0
//     measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(1, 1,
//     0));
//     //tracklet 1, dynamic, frame 1
//     measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(1, 1,
//     1));

//     gtsam::Values estimate;
//     //add motion only at frame 1
//     const gtsam::Pose3 pose_estimate =
//     utils::createRandomAroundIdentity<gtsam::Pose3>(0.3);
//     estimate.insert(ObjectMotionSymbol(1, 1), pose_estimate);

//     map->updateObservations(measurements);
//     map->updateEstimates(estimate,gtsam::NonlinearFactorGraph{}, 1 );

//     ObjectNode2d::Ptr object1 = map->getObject(1);
//     EXPECT_TRUE(object1->getMotionEstimate(1));

//     gtsam::Pose3 recovered_pose_estimate;
//     EXPECT_TRUE(object1->hasMotionEstimate(1, &recovered_pose_estimate));
//     //check nullptr version
//     EXPECT_TRUE(object1->hasMotionEstimate(1));
//     EXPECT_TRUE(gtsam::assert_equal(recovered_pose_estimate, pose_estimate));

//     //should not throw exception as we have seen this obejct at frames 0 and
//     1 (based on the measurements)
//     //we just dont have an estimate of the pose
//     EXPECT_FALSE(object1->getPoseEstimate(0));
//     EXPECT_FALSE(object1->getPoseEstimate(1));
// }

// TEST(Map, testObjectNodeWithOnlyPose) {
//     Map2d::Ptr map = Map2d::create();

//     //create dynamic observations for object 1 at frames 0 and 1
//     StatusKeypointMeasurements measurements;
//      //tracklet 1, dynamic, frame 0
//     measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(1, 1,
//     0));
//     //tracklet 1, dynamic, frame 1
//     measurements.push_back(dyno_testing::makeStatusKeypointMeasurement(1, 1,
//     1));

//     gtsam::Values estimate;
//     //add motion only at frame 1
//     const gtsam::Pose3 pose_estimate =
//     utils::createRandomAroundIdentity<gtsam::Pose3>(0.3);
//     estimate.insert(ObjectPoseSymbol(1, 1), pose_estimate);

//     map->updateObservations(measurements);
//     map->updateEstimates(estimate,gtsam::NonlinearFactorGraph{}, 1 );

//     ObjectNode2d::Ptr object1 = map->getObject(1);
//     EXPECT_TRUE(object1->getPoseEstimate(1));

//     gtsam::Pose3 recovered_pose_estimate;
//     EXPECT_TRUE(object1->hasPoseEstimate(1, &recovered_pose_estimate));
//     //check nullptr version
//     EXPECT_TRUE(object1->hasPoseEstimate(1));
//     EXPECT_TRUE(gtsam::assert_equal(recovered_pose_estimate, pose_estimate));

//     //should not throw exception as we have seen this obejct at frames 0 and
//     1 (based on the measurements)
//     //we just dont have an estimate of the motion
//     EXPECT_FALSE(object1->getMotionEstimate(1));
// }
```

## File: vision_map/test_tools.cc
```
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */
#include <gtest/gtest.h>
#include <gtsam/geometry/StereoCamera.h>

#include <cmath>

#include "dynosam/common/Types.hpp"
#include "dynosam/frontend/vision/VisionTools.hpp"
#include "internal/helpers.hpp"

using namespace dyno;

TEST(VisionTools, determineOutlierIdsBasic) {
  TrackletIds tracklets = {1, 2, 3, 4, 5};
  TrackletIds inliers = {1, 2};

  TrackletIds expected_outliers = {3, 4, 5};
  TrackletIds outliers;
  determineOutlierIds(inliers, tracklets, outliers);
  EXPECT_EQ(expected_outliers, outliers);
}

TEST(VisionTools, determineOutlierIdsUnorderd) {
  TrackletIds tracklets = {12, 45, 1, 85, 3, 100};
  TrackletIds inliers = {3, 1, 100};

  TrackletIds expected_outliers = {12, 45, 85};
  TrackletIds outliers;
  determineOutlierIds(inliers, tracklets, outliers);
  EXPECT_EQ(expected_outliers, outliers);
}

TEST(VisionTools, determineOutlierIdsNoSubset) {
  TrackletIds tracklets = {12, 45, 1, 85, 3, 100};
  TrackletIds inliers = {12, 45, 1, 85, 3, 100};

  TrackletIds outliers = {4, 5, 6};  // also add a test that outliers is cleared
  determineOutlierIds(inliers, tracklets, outliers);
  EXPECT_TRUE(outliers.empty());
}

TEST(VisionTools, testMacVOUncertaintyPropogation) {
  Camera camera = dyno_testing::makeDefaultCamera();
  CameraParams params = camera.getParams();
  auto camera_impl = camera.getImplCamera();

  // make stereo camera
  const double base_line = 0.5;
  gtsam::Cal3_S2Stereo::shared_ptr stereo_params =
      boost::make_shared<gtsam::Cal3_S2Stereo>(
          params.fx(), params.fy(), 0, params.cu(), params.cv(), base_line);
  gtsam::StereoCamera stereo_camera(gtsam::Pose3::Identity(), stereo_params);

  Feature feature;
  Keypoint kp(params.cu(), params.cv() + 20);
  feature.keypoint(kp);
  feature.depth(1.0);

  // sigmas squared
  double kp_sigma_2 = 0.1;
  double depth_sigma_2 = 0.000005;

  // first check diagonal components of proposed macv matrix
  gtsam::Matrix32 J_keypoint;
  gtsam::Matrix31 J_depth;
  gtsam::Point3 landmark =
      camera_impl->backproject(feature.keypoint(), feature.depth(), boost::none,
                               J_keypoint, J_depth, boost::none);

  gtsam::StereoPoint2 stereo_kp = stereo_camera.project(landmark);
  EXPECT_EQ(kp(0), stereo_kp.uL());
  EXPECT_EQ(kp(1), stereo_kp.v());

  gtsam::Matrix33 J_stereo_point;
  stereo_camera.backproject2(stereo_kp, boost::none, J_stereo_point);

  gtsam::Point3 calc_landmark(
      ((feature.keypoint()(0) - params.cu()) * feature.depth()) / params.fx(),
      ((feature.keypoint()(1) - params.cv()) * feature.depth()) / params.fy(),
      feature.depth());
  EXPECT_TRUE(gtsam::assert_equal(calc_landmark, landmark));

  // form measurement covariance matrices
  gtsam::Matrix22 pixel_covariance_matrix;
  pixel_covariance_matrix << kp_sigma_2, 0.0, 0.0, kp_sigma_2;

  gtsam::Matrix33 stereo_pixel_covariance_matrix;
  stereo_pixel_covariance_matrix << kp_sigma_2, 0.0, 0.0, 0, kp_sigma_2, 0, 0,
      0, kp_sigma_2;

  // for depth uncertainty, we model it as a quadratic increase with distnace
  // double depth_covariance = depth_sigma * std::pow(depth, 2);
  double depth_covariance = depth_sigma_2;
  LOG(INFO) << "J_keypoint " << J_keypoint;
  // calcualte 3x3 covairance matrix
  gtsam::Matrix33 covariance =
      J_keypoint * pixel_covariance_matrix * J_keypoint.transpose();
  // J_depth * depth_covariance * J_depth.transpose();

  gtsam::Matrix33 stereo_covariance = J_stereo_point *
                                      stereo_pixel_covariance_matrix *
                                      J_stereo_point.transpose();

  LOG(INFO) << "Jacobian cov " << covariance;
  LOG(INFO) << "Stereo Jacobian cov " << stereo_covariance;

  double d_2 = std::pow(feature.depth(), 2);
  double u_2 = std::pow(feature.keypoint()(0), 2);
  double v_2 = std::pow(feature.keypoint()(1), 2);
  double fx_2 = std::pow(params.fx(), 2);
  double fy_2 = std::pow(params.fy(), 2);
  double cx_2 = std::pow(params.cu(), 2);
  double cy_2 = std::pow(params.cv(), 2);

  double mac_v_sigma_x = ((kp_sigma_2 + d_2) * (depth_sigma_2 + u_2) -
                          u_2 * d_2 + cx_2 * depth_sigma_2) /
                         fx_2;
  double mac_v_sigma_y = ((kp_sigma_2 + d_2) * (depth_sigma_2 + v_2) -
                          v_2 * d_2 + cy_2 * depth_sigma_2) /
                         fy_2;
  double mac_v_sigma_z = depth_sigma_2;

  gtsam::Matrix33 macvo_covariance;
  macvo_covariance << mac_v_sigma_x, 0, 0, 0, mac_v_sigma_y, 0, 0, 0,
      mac_v_sigma_z;

  LOG(INFO) << "macvo cov " << macvo_covariance;
}
```

## File: visualization/test_viz.cc
```
/*
 *   Copyright (c) 2024 ACFR-RPG, University of Sydney, Jesse Morris
 (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a
 copy
 *   of this software and associated documentation files (the "Software"), to
 deal
 *   in the Software without restriction, including without limitation the
 rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in
 all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE
 *   SOFTWARE.
 */

#include <glog/logging.h>
#include <gtest/gtest.h>

#include "dynosam/visualizer/ColourMap.hpp"

using namespace dyno;

TEST(Color, testRGBDDownCasting) {
  RGBA<uint8_t> rgbd_int(255, 255, 0);
  EXPECT_EQ(rgbd_int.r, 255);
  EXPECT_EQ(rgbd_int.g, 255);
  EXPECT_EQ(rgbd_int.b, 0);
  EXPECT_EQ(rgbd_int.a, 255);

  RGBA<float> rgbd_float(rgbd_int);
  EXPECT_EQ(rgbd_float.r, 1.0);
  EXPECT_EQ(rgbd_float.g, 1.0);
  EXPECT_EQ(rgbd_float.b, 0);
  EXPECT_EQ(rgbd_float.a, 1.0);
}

TEST(Color, testRGBDDownCasting1) {
  RGBA<uint8_t> rgbd_int(128, 100, 10);
  EXPECT_EQ(rgbd_int.r, 128);
  EXPECT_EQ(rgbd_int.g, 100);
  EXPECT_EQ(rgbd_int.b, 10);
  EXPECT_EQ(rgbd_int.a, 255);

  RGBA<float> rgbd_float(rgbd_int);
  EXPECT_FLOAT_EQ(rgbd_float.r, 128.0 / 255.0);
  EXPECT_FLOAT_EQ(rgbd_float.g, 100.0 / 255.0);
  EXPECT_FLOAT_EQ(rgbd_float.b, 10.0 / 255.0);
  EXPECT_FLOAT_EQ(rgbd_float.a, 1.0);
}

TEST(Color, testRGBDUpCasting) {
  RGBA<float> rgbd_float(0.5, 0.2, 0.1);
  EXPECT_FLOAT_EQ(rgbd_float.r, 0.5);
  EXPECT_FLOAT_EQ(rgbd_float.g, 0.2);
  EXPECT_FLOAT_EQ(rgbd_float.b, 0.1);
  EXPECT_FLOAT_EQ(rgbd_float.a, 1.0);

  RGBA<uint8_t> rgbd_int(rgbd_float);
  EXPECT_EQ(rgbd_int.r, static_cast<uint8_t>(0.5 * 255.0));
  EXPECT_EQ(rgbd_int.g, static_cast<uint8_t>(0.2 * 255.0));
  EXPECT_EQ(rgbd_int.b, static_cast<uint8_t>(0.1 * 255.0));
  EXPECT_EQ(rgbd_int.a, static_cast<uint8_t>(255.0));
}
```

## File: test_main.cc
```
/*
 *   Copyright (c) 2023 ACFR-RPG, University of Sydney, Jesse Morris (jesse.morris@sydney.edu.au)
 *   All rights reserved.

 *   Permission is hereby granted, free of charge, to any person obtaining a copy
 *   of this software and associated documentation files (the "Software"), to deal
 *   in the Software without restriction, including without limitation the rights
 *   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 *   copies of the Software, and to permit persons to whom the Software is
 *   furnished to do so, subject to the following conditions:

 *   The above copyright notice and this permission notice shall be included in all
 *   copies or substantial portions of the Software.

 *   THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 *   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 *   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 *   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 *   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 *   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 *   SOFTWARE.
 */

#include "internal/helpers.hpp"

#include <gtest/gtest.h>
#include <glog/logging.h>

#include "rclcpp/rclcpp.hpp"


DEFINE_string(test_data_path, getTestDataPath(), "Path to data for unit tests.");

int main(int argc, char** argv)
{
  // rclcpp::init(argc, argv);
  ::testing::InitGoogleTest(&argc, argv);
  google::InitGoogleLogging(argv[0]);

  FLAGS_logtostderr = 1;
  FLAGS_colorlogtostderr = 1;
  FLAGS_log_prefix = 1;
  FLAGS_v = 1;
  google::ParseCommandLineFlags(&argc, &argv, true);
  return RUN_ALL_TESTS();
}
```
````
